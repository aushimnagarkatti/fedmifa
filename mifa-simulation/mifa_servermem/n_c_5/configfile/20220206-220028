import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare' #emnist'#cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.07],#[1/np.power(10,2.5)],
    1: [0.06],#[1/np.power(10,2)],
    2: [0.05]#[1/np.power(10,2)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1
    }

sch_freq = 300 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.4599088469902513, 2.5265407646935296, 2.336468764757499, 2.0998344530126687, 2.000354782704851, 1.9087279917555928, 1.975452873411011, 1.8746839639830726, 1.7546946080128798, 1.7603258138903228, 1.6699082186146392, 1.6525203953896164, 1.6812609733331079, 1.5821459682525831, 1.6409446308620659, 1.6244326414707353, 1.6839229735841017, 1.6604912514223908, 1.6095173863834802, 1.60883959761235, 1.5393478938480574, 1.5285301263367521, 1.5121484574106865, 1.6090866880748387, 1.5333246406565944, 1.467056107570913, 1.5972763992560068, 1.5278308943330248, 1.4648321317310642, 1.4316798991702853, 1.3916296128029864, 1.491889359544659, 1.38785443183285, 1.417119997037694, 1.6053035668077225, 1.4457258390047332, 1.3304994908012922, 1.4390519446304095, 1.3559905743181948, 1.4192564481621743, 1.4055246596769235, 1.3884378034475404, 1.4563558609616503, 1.2935693285573613, 1.3518542108601994, 1.436887622009549, 1.363762321428819, 1.4032962554665174, 1.3427362124113476, 1.354780127834503], [3.521275417940244, 2.0521599000030073, 1.8692190755024936, 1.7178763036368685, 1.66406823843668, 1.6027492367565777, 1.5456722631456081, 1.5277900822294437, 1.44252209598734, 1.4152394728575572, 1.332048947902899, 1.3165116008244477, 1.36569207936089, 1.2587534628638557, 1.3103314400235053, 1.3263536304086154, 1.3505871991177016, 1.32218826042678, 1.255528646469116, 1.3216694488801533, 1.158128814777616, 1.236523470682052, 1.209512334273366, 1.2382187685106483, 1.2203448271659767, 1.1476730965237807, 1.2837462498074808, 1.2415683090104264, 1.1351955691132118, 1.0688902516240164, 1.059412310617784, 1.1625289011387663, 1.0855262391562612, 1.109780447905889, 1.278552523317324, 1.1450105705222016, 1.0500795617712197, 1.1757991890958768, 1.0698438140113091, 1.1072870581397283, 1.1101727393849956, 1.0856855658687956, 1.1671358427934533, 0.9936185990716473, 1.0473736507561473, 1.169218987373789, 1.079341573760967, 1.100062203513908, 1.0421135470767817, 1.0989907130185195], [3.5935982200162813, 2.110175545037369, 1.9467842001487046, 1.796124247963763, 1.74719962233238, 1.6891265215576516, 1.625742353479793, 1.636827088682617, 1.51628989517832, 1.5009488458970535, 1.4205619598816315, 1.4446577883547218, 1.4407539729463514, 1.3803633987362889, 1.4039431938232896, 1.4050284229200878, 1.4562473734159767, 1.4376961118959488, 1.3366631880071427, 1.3996882800293409, 1.295306833613631, 1.3180762654841145, 1.298808862611991, 1.367505677838496, 1.3009286168537184, 1.2687607137876977, 1.3478394037690848, 1.314774380664828, 1.2232523698765672, 1.1681870675177801, 1.1651325117713878, 1.2414451129118387, 1.1839430183931499, 1.218266295040258, 1.3573721716431961, 1.241802597357927, 1.1522316627495734, 1.2293869956451913, 1.1304561324487825, 1.204150040240871, 1.1943698880428957, 1.163928758178645, 1.2188674591112698, 1.0866704221078842, 1.101194914592637, 1.2671684789364797, 1.1759438279517973, 1.2340144660693924, 1.1307740345281716, 1.1968330900964443]]
Acc algo: [[tensor(0.), tensor(0.2500), tensor(0.2656), tensor(0.2812), tensor(0.2656), tensor(0.4062), tensor(0.3281), tensor(0.3750), tensor(0.4062), tensor(0.4219), tensor(0.5312), tensor(0.4844), tensor(0.5000), tensor(0.4844), tensor(0.5156), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.6250), tensor(0.5469), tensor(0.5938), tensor(0.5938), tensor(0.5781), tensor(0.5781), tensor(0.6094), tensor(0.5469), tensor(0.5938), tensor(0.5938), tensor(0.6250), tensor(0.6094), tensor(0.5781), tensor(0.5938), tensor(0.6094), tensor(0.6094), tensor(0.5781), tensor(0.6094), tensor(0.6094), tensor(0.6094), tensor(0.5625), tensor(0.6250), tensor(0.5938), tensor(0.6094), tensor(0.5938)], [tensor(0.1875), tensor(0.3281), tensor(0.4375), tensor(0.4375), tensor(0.4844), tensor(0.5156), tensor(0.4375), tensor(0.4844), tensor(0.4844), tensor(0.5312), tensor(0.5156), tensor(0.5156), tensor(0.5156), tensor(0.5312), tensor(0.5156), tensor(0.5156), tensor(0.5312), tensor(0.5156), tensor(0.5469), tensor(0.5469), tensor(0.5156), tensor(0.5625), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5156), tensor(0.5469), tensor(0.5469), tensor(0.5156), tensor(0.5469), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5156), tensor(0.5000), tensor(0.5312), tensor(0.5156), tensor(0.5000), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5469)], [tensor(0.1875), tensor(0.2969), tensor(0.3750), tensor(0.4531), tensor(0.4531), tensor(0.5000), tensor(0.4531), tensor(0.4688), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5156), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5469), tensor(0.5156), tensor(0.5625), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5781), tensor(0.5312), tensor(0.5625), tensor(0.5156), tensor(0.5781), tensor(0.5312), tensor(0.5625), tensor(0.5469), tensor(0.5938), tensor(0.5781), tensor(0.5156), tensor(0.5469), tensor(0.5469), tensor(0.5781), tensor(0.5469), tensor(0.5625), tensor(0.5781), tensor(0.5156), tensor(0.5781), tensor(0.5625), tensor(0.5625)]]
test_loss_algo: [[4.85240364074707, 3.0358641147613525, 2.62858247756958, 2.2487120628356934, 2.330484390258789, 2.0873019695281982, 2.1139707565307617, 2.0374252796173096, 1.8330706357955933, 1.8734016418457031, 1.7597167491912842, 1.7495511770248413, 1.5585899353027344, 1.6847890615463257, 1.5642460584640503, 1.6076298952102661, 1.5404555797576904, 1.563447117805481, 1.5666279792785645, 1.4721068143844604, 1.5208184719085693, 1.447190761566162, 1.478083610534668, 1.5013542175292969, 1.4918217658996582, 1.4738593101501465, 1.5169579982757568, 1.4812623262405396, 1.4618791341781616, 1.4866995811462402, 1.4807794094085693, 1.4904890060424805, 1.458842396736145, 1.4914846420288086, 1.478283405303955, 1.487274408340454, 1.5050703287124634, 1.4853121042251587, 1.5046987533569336, 1.5016722679138184, 1.512465000152588, 1.4973223209381104, 1.4933984279632568, 1.5109891891479492, 1.5126063823699951, 1.5048450231552124, 1.4902222156524658, 1.4837243556976318, 1.4773845672607422, 1.5347137451171875], [3.0136587619781494, 2.1235902309417725, 1.9831326007843018, 1.900213599205017, 1.658127784729004, 1.6078588962554932, 1.7533928155899048, 1.6955068111419678, 1.7082855701446533, 1.6116689443588257, 1.623422384262085, 1.6194854974746704, 1.6096071004867554, 1.601530909538269, 1.5931715965270996, 1.6150556802749634, 1.5981683731079102, 1.6378428936004639, 1.575442910194397, 1.6189502477645874, 1.5761163234710693, 1.6168147325515747, 1.6649127006530762, 1.5391438007354736, 1.6104344129562378, 1.6072053909301758, 1.5999423265457153, 1.6056708097457886, 1.6147321462631226, 1.6040703058242798, 1.5691370964050293, 1.5437047481536865, 1.6023341417312622, 1.600920557975769, 1.622066617012024, 1.5659747123718262, 1.65227210521698, 1.679021954536438, 1.617601752281189, 1.6015080213546753, 1.6559619903564453, 1.6292074918746948, 1.6135956048965454, 1.6720956563949585, 1.6711022853851318, 1.6502044200897217, 1.6450976133346558, 1.6945439577102661, 1.6867966651916504, 1.664552927017212], [3.0574147701263428, 2.216844320297241, 2.067805767059326, 1.8502857685089111, 1.7471877336502075, 1.6946824789047241, 1.739188551902771, 1.64908766746521, 1.631213903427124, 1.6295866966247559, 1.6043659448623657, 1.6552358865737915, 1.575371503829956, 1.6606154441833496, 1.634613275527954, 1.6460407972335815, 1.6242526769638062, 1.6797912120819092, 1.7217999696731567, 1.7022510766983032, 1.595458745956421, 1.557057499885559, 1.6239960193634033, 1.6596717834472656, 1.5954248905181885, 1.619594693183899, 1.5887410640716553, 1.6956695318222046, 1.6366517543792725, 1.5835310220718384, 1.5963685512542725, 1.531416654586792, 1.572739601135254, 1.5992109775543213, 1.6040605306625366, 1.6183520555496216, 1.5959796905517578, 1.5366674661636353, 1.551326870918274, 1.614349603652954, 1.623339295387268, 1.5830512046813965, 1.5632166862487793, 1.5934990644454956, 1.5737299919128418, 1.5485191345214844, 1.5953835248947144, 1.588425874710083, 1.6421135663986206, 1.701470971107483]]
global_train_loss_algo: [[], [], []]
