import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist'#'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.4],#[1/np.power(10,2.5)],
    1: [0.1],#[1/np.power(10,2)],
    2: [0.1]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.8
    , #factor to reduce lr in scheduler
    1:0.8,
    2:0.8
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[4.399532779852549, 3.537435735066732, 3.274109863440196, 3.047903802871704, 2.5595969899495445, 2.158026237487793, 2.095046953678131, 1.8348999943733215, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [3.8038798880577085, 1.5976327204704286, 1.6093837297360103, 1.2542345535357793, 1.024412996912996, 0.9463346223831177, 0.9477518576582273, 0.7656924814383188, 0.6658919779459636, 0.8635002279231945, 0.6151040469408035, 0.844015356262525, 0.8016158815224965, 0.6177990733725685, 0.6746447949409484, 0.5639435249020657, 0.8736610170205434, 0.8451015271544456, 0.8290623291333518, 0.823652873535951, 0.9213499677975973, 0.8429066596428554, 0.6885662181973458, 0.8647642827033997, 0.8136563369418894, 0.6571858824888865, 0.8872931305567423, 0.554120777885119, 0.690890831982717, 0.744788801352183, 0.8025238979657491, 0.9634000724951426, 0.8256636118888855, 0.8433146355748175, 1.123764977256457, 0.7970549664894739, 1.379924477736155, 0.891584347685178, 0.9664391579727332, 0.6947924329042435, 0.917072568734487, 0.77169111986955, 1.0390442508061726, 0.9406307324767111, 1.0018718376755715, 0.776456096371015, 1.1069044262170793, 1.0284731580416362, 0.8680752004782359, 0.9847419434388479, 0.9887479797999065, 1.1609012614091236, 1.0231811915238698, 0.7660010220209758, 0.958334804534912, 1.020345400174459, 0.7925077470995132, 1.1476141607761385, 0.9572854379812877, 1.0503321141401927], [3.8410964727401735, 1.55399582862854, 1.628325591047605, 1.2740543807943663, 1.101526105761528, 0.9682406301498412, 0.9316281333665053, 0.830995034456253, 0.6357056464751563, 1.4152933943669002, 0.5844820833603541, 0.7759913685321809, 0.7953638396660485, 0.6178175894703184, 0.6338596628506978, 0.5273583197295666, 0.9246412259340285, 0.7858898579478264, 0.7879817044734955, 0.7582352553606034, 0.9123796766599019, 0.8076573552687962, 0.6830991683999698, 0.8127910331885019, 0.7408890634313936, 0.5838990403016407, 0.8541887022455533, 0.4864243681232134, 0.6225633219679196, 0.684407096862793, 0.7392347842852274, 0.8977843697468438, 0.7279827871719997, 0.7968816279172899, 1.1396961784362794, 0.7055242210626602, 1.3509647357463836, 0.8362774356206257, 0.9338015405337016, 0.6130776683489481, 0.8128356480598449, 0.628957884033521, 0.9905737249851226, 0.8196288836995761, 0.9173225046197574, 0.6665464037656783, 1.0206788784265517, 0.9308471604684987, 0.7230028404394786, 0.8479323041439055, 0.870334970633189, 1.0583284250895182, 0.904584624449412, 0.6256176108121871, 0.7628507594764233, 0.8938292715946833, 0.6436129964533306, 0.9456225742896398, 0.6935376326243082, 0.8801875738302865]]
Acc algo: [[tensor(0.), tensor(0.0312), tensor(0.0312), tensor(0.0625), tensor(0.1406), tensor(0.1875), tensor(0.1562), tensor(0.2031), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312)], [tensor(0.0312), tensor(0.2344), tensor(0.1875), tensor(0.4375), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.4531), tensor(0.4531), tensor(0.5312), tensor(0.5156), tensor(0.4375), tensor(0.4844), tensor(0.5781), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.4688), tensor(0.5156), tensor(0.5469), tensor(0.5000), tensor(0.0938), tensor(0.4844), tensor(0.5938), tensor(0.6094), tensor(0.6094), tensor(0.6094), tensor(0.5938), tensor(0.5781), tensor(0.5156), tensor(0.4219), tensor(0.5469), tensor(0.5625), tensor(0.4688), tensor(0.5312), tensor(0.5625), tensor(0.5469), tensor(0.5938), tensor(0.5625), tensor(0.5469), tensor(0.5156), tensor(0.5312), tensor(0.5469), tensor(0.5000), tensor(0.5312), tensor(0.4219), tensor(0.5156), tensor(0.5000), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5000), tensor(0.4844), tensor(0.4844), tensor(0.5312), tensor(0.5312), tensor(0.5625)], [tensor(0.0156), tensor(0.1719), tensor(0.2969), tensor(0.3125), tensor(0.4531), tensor(0.4062), tensor(0.4219), tensor(0.5000), tensor(0.4062), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.3906), tensor(0.5625), tensor(0.4844), tensor(0.5000), tensor(0.4375), tensor(0.5312), tensor(0.5156), tensor(0.6406), tensor(0.5781), tensor(0.4531), tensor(0.5625), tensor(0.5938), tensor(0.5781), tensor(0.6250), tensor(0.6094), tensor(0.5781), tensor(0.6094), tensor(0.5000), tensor(0.6250), tensor(0.6250), tensor(0.5625), tensor(0.6250), tensor(0.5625), tensor(0.5781), tensor(0.5938), tensor(0.6094), tensor(0.6094), tensor(0.5938), tensor(0.5312), tensor(0.5625), tensor(0.6250), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5469), tensor(0.6406), tensor(0.6250), tensor(0.5938), tensor(0.6250), tensor(0.6094), tensor(0.6250), tensor(0.6250), tensor(0.6406), tensor(0.6094), tensor(0.6094), tensor(0.6250), tensor(0.5781), tensor(0.6250)]]
test_loss_algo: [[4.122064590454102, 4.045067310333252, 4.496360778808594, 5.1608710289001465, 2.674837112426758, 2.8922860622406006, 2.77540922164917, 2.656373977661133, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [4.331657886505127, 2.6377267837524414, 2.595848798751831, 2.4068450927734375, 1.7002456188201904, 1.6971570253372192, 1.7200489044189453, 1.611785650253296, 1.5607296228408813, 1.656219244003296, 1.213608980178833, 1.7387876510620117, 1.4604281187057495, 1.2326372861862183, 1.6433346271514893, 1.446685552597046, 1.3393571376800537, 1.3476954698562622, 1.3586256504058838, 1.4363447427749634, 1.5820553302764893, 1.4549039602279663, 1.2611448764801025, 1.43360435962677, 5.57760763168335, 1.407418131828308, 1.3075895309448242, 1.1817851066589355, 1.2490841150283813, 1.0452287197113037, 1.2644535303115845, 1.3251432180404663, 1.42275869846344, 1.6262950897216797, 1.5156229734420776, 1.3049006462097168, 1.5061591863632202, 1.389312982559204, 1.2289667129516602, 1.4831490516662598, 1.481268048286438, 1.3688091039657593, 1.5225236415863037, 1.5657932758331299, 1.3731688261032104, 1.3595163822174072, 1.463870882987976, 1.3626912832260132, 1.7157012224197388, 1.3923335075378418, 1.462792992591858, 1.3917367458343506, 1.3309738636016846, 1.4232804775238037, 1.6083216667175293, 1.6056723594665527, 1.5495219230651855, 1.4501957893371582, 1.445116400718689, 1.3708096742630005], [4.158656597137451, 2.6294658184051514, 2.410069227218628, 2.5338075160980225, 2.032200574874878, 1.7428252696990967, 1.614670991897583, 1.4606108665466309, 1.825302243232727, 1.482275128364563, 1.2673041820526123, 1.27596914768219, 2.010399103164673, 1.310299038887024, 1.5103113651275635, 1.5396748781204224, 1.5604979991912842, 1.2315709590911865, 1.415342926979065, 0.9390534162521362, 1.1838676929473877, 1.3712520599365234, 1.1936930418014526, 1.3113421201705933, 1.210559606552124, 1.097490668296814, 1.064962387084961, 0.9737424254417419, 1.0913383960723877, 1.4367928504943848, 0.9336493015289307, 1.0024049282073975, 1.2568178176879883, 0.9858584403991699, 1.1052836179733276, 1.1663695573806763, 1.2288966178894043, 1.1417003870010376, 1.1119381189346313, 0.9547573924064636, 1.317001223564148, 1.1992368698120117, 0.9966980218887329, 1.1662697792053223, 1.1311336755752563, 1.179754614830017, 1.1891005039215088, 1.0389047861099243, 1.0048024654388428, 1.1231229305267334, 1.0131359100341797, 1.033639907836914, 0.9852872490882874, 1.0227094888687134, 0.960325300693512, 1.0545536279678345, 0.9774685502052307, 1.030645489692688, 1.1652628183364868, 1.0439122915267944]]
global_train_loss_algo: [[], [], []]
