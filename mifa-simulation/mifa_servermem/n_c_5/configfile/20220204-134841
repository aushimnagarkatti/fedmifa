import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare' #emnist'#cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.05],#[1/np.power(10,2.5)],
    1: [0.05],#[1/np.power(10,2)],
    2: [0.05]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.8
    , #factor to reduce lr in scheduler
    1:0.8,
    2:0.8
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[4.346593617870694, 3.0629485440710518, 2.455665530758496, 2.315351519214226, 2.1767736498912176, 2.056084086781456, 2.0574565123319624, 1.974384958384098, 1.9372638509120972, 1.9023782802915008, 1.8573361312585468, 1.84759771440044, 1.8341505143241494, 1.8208373864484841, 1.7898608053056635, 1.8238789395500858, 1.7433554413733883, 1.7696637522929604, 1.6989751711316305, 1.766919556047173, 1.7710975660083275, 1.7524002536697822, 1.721598822334895, 1.7766600548739386, 1.6880551692493377, 1.7812302719878457, 1.6781513607388656, 1.7543557837107098, 1.6903107690838943, 1.7750862519385653, 1.6713817074395756, 1.5920692867894075, 1.647971769044932, 1.6835370268593515, 1.6926963236192605, 1.6121385929259386, 1.6436540841067813, 1.771964428733586, 1.7104181208770448, 1.655080293080801, 1.7036019929663204, 1.6435189768941494, 1.7796748414352603, 1.7167596087909878, 1.6583287213073568, 1.6991986934373196, 1.5976188090058847, 1.8681362615995323, 1.7057542013002902, 1.6929137671390173], [4.342848358555445, 2.445957786423624, 2.0042278461789476, 1.917935867635744, 1.7901532561218552, 1.6580250184381164, 1.658723216176033, 1.6528493716227703, 1.5939292396356664, 1.6071909711756427, 1.5859319279923394, 1.5390822145784278, 1.5553415210890549, 1.5486126533607325, 1.5008362657691738, 1.5547556559393203, 1.5120088618414431, 1.5047972788404558, 1.4442495170695595, 1.5268543524845926, 1.518879534701573, 1.4797633071365177, 1.4562781384398746, 1.538927478834148, 1.4280759814163997, 1.5704988012836567, 1.4744003553677343, 1.4755281364574966, 1.5000886300598517, 1.5427985479081392, 1.4352513265712177, 1.3647715075774411, 1.4211251522995791, 1.4693186747058355, 1.4553257826073862, 1.3685123291521362, 1.4344687384086954, 1.548002754731624, 1.4839274614116524, 1.4475900034447489, 1.511008578404715, 1.452901434483811, 1.5104056676783455, 1.4996181571899898, 1.4654151697823385, 1.485420007255732, 1.4104833289640137, 1.638692076541614, 1.5055692435012384, 1.4917092084316979], [4.347884034255195, 2.436134399540898, 2.013886533708183, 1.9139299729959613, 1.7984021170398996, 1.6506724949867007, 1.668178768157959, 1.6565932082946486, 1.6032542156162006, 1.6214554995264194, 1.5852586061808756, 1.554746507302739, 1.567616921920021, 1.5283775657572591, 1.5131154561479652, 1.5243641348134083, 1.48802959110425, 1.5241829352518477, 1.4602728707861137, 1.5603906873398803, 1.5443277343690358, 1.5102808396414875, 1.5056039169280215, 1.5720065334796216, 1.46332589145123, 1.5936859282455618, 1.486285476732799, 1.5470372669744683, 1.5183251576794423, 1.5758153222510534, 1.4804240402524198, 1.3949455247652955, 1.4516397908290029, 1.4900650349819753, 1.4870098614656566, 1.4078905848120198, 1.4605113209018206, 1.5881276105490005, 1.5065267712144348, 1.4646658960301404, 1.5341820527318621, 1.4788684905224652, 1.525778571779268, 1.5187325318238092, 1.4896125928016632, 1.5173491792000977, 1.4276328517378576, 1.6208576206356688, 1.554011722228965, 1.5114918026879316]]
Acc algo: [[tensor(0.), tensor(0.1875), tensor(0.3281), tensor(0.2188), tensor(0.2969), tensor(0.3594), tensor(0.2969), tensor(0.3438), tensor(0.3750), tensor(0.3906), tensor(0.4219), tensor(0.4688), tensor(0.4219), tensor(0.4062), tensor(0.4375), tensor(0.4062), tensor(0.4531), tensor(0.4375), tensor(0.4531), tensor(0.4688), tensor(0.4844), tensor(0.4531), tensor(0.5156), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.5156), tensor(0.5000), tensor(0.5156), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5156), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5312)], [tensor(0.1875), tensor(0.2812), tensor(0.3125), tensor(0.3750), tensor(0.4531), tensor(0.4844), tensor(0.4531), tensor(0.5000), tensor(0.5156), tensor(0.5156), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5938), tensor(0.5781), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5469), tensor(0.5469), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5469), tensor(0.5625), tensor(0.5469), tensor(0.5625), tensor(0.5781), tensor(0.5625), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5469), tensor(0.5625), tensor(0.5469), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5625), tensor(0.5469)], [tensor(0.1875), tensor(0.3281), tensor(0.3281), tensor(0.4219), tensor(0.4375), tensor(0.4688), tensor(0.4844), tensor(0.5000), tensor(0.4844), tensor(0.5156), tensor(0.5000), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5000), tensor(0.5469), tensor(0.5625), tensor(0.5781), tensor(0.5469), tensor(0.5781), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5781), tensor(0.5469), tensor(0.5938), tensor(0.5938), tensor(0.5938), tensor(0.5781), tensor(0.5312), tensor(0.6094), tensor(0.5625), tensor(0.5625), tensor(0.6250), tensor(0.6250), tensor(0.5938), tensor(0.6406), tensor(0.6250), tensor(0.6406), tensor(0.6406), tensor(0.6250), tensor(0.6250), tensor(0.6406), tensor(0.6406), tensor(0.6406), tensor(0.5781), tensor(0.5781), tensor(0.6094), tensor(0.6719)]]
test_loss_algo: [[4.8271636962890625, 3.6021339893341064, 2.678462028503418, 2.664005756378174, 2.270063877105713, 2.246187925338745, 2.0983471870422363, 1.9501709938049316, 1.9920753240585327, 1.878302812576294, 1.8271945714950562, 1.8472344875335693, 1.8014212846755981, 1.7632032632827759, 1.7518452405929565, 1.7485493421554565, 1.6986812353134155, 1.7064917087554932, 1.7183917760849, 1.6513174772262573, 1.651580572128296, 1.6664413213729858, 1.6203458309173584, 1.618227243423462, 1.628942847251892, 1.6045188903808594, 1.6174405813217163, 1.596205711364746, 1.593477487564087, 1.6044286489486694, 1.5778286457061768, 1.5860846042633057, 1.5626304149627686, 1.572482705116272, 1.5707931518554688, 1.5613750219345093, 1.5772675275802612, 1.553686261177063, 1.5591241121292114, 1.5632039308547974, 1.5576913356781006, 1.5513687133789062, 1.5551533699035645, 1.546819806098938, 1.5459855794906616, 1.5521142482757568, 1.5510542392730713, 1.5413790941238403, 1.544190526008606, 1.5446796417236328], [4.080699920654297, 2.378657341003418, 2.00570011138916, 1.930198311805725, 1.7376699447631836, 1.6563029289245605, 1.66730535030365, 1.650861382484436, 1.6462700366973877, 1.6737205982208252, 1.623785138130188, 1.613054871559143, 1.5951510667800903, 1.589723825454712, 1.5484260320663452, 1.5702881813049316, 1.4957606792449951, 1.5046417713165283, 1.5142147541046143, 1.5189564228057861, 1.5125007629394531, 1.495780348777771, 1.48634934425354, 1.501356601715088, 1.4869091510772705, 1.4846243858337402, 1.4804749488830566, 1.476391315460205, 1.473190426826477, 1.4587570428848267, 1.4437289237976074, 1.4376775026321411, 1.438747525215149, 1.449385404586792, 1.4350837469100952, 1.440708875656128, 1.438814401626587, 1.451568841934204, 1.43435537815094, 1.426526427268982, 1.4292901754379272, 1.4332438707351685, 1.429452657699585, 1.4237240552902222, 1.435089111328125, 1.4190317392349243, 1.4335942268371582, 1.4329514503479004, 1.413623332977295, 1.4213539361953735], [4.090208053588867, 2.3221018314361572, 1.977891206741333, 1.8395109176635742, 1.7829670906066895, 1.6842859983444214, 1.6574416160583496, 1.595999836921692, 1.619398832321167, 1.5834778547286987, 1.5949862003326416, 1.632997989654541, 1.6204676628112793, 1.5836153030395508, 1.5726767778396606, 1.5726025104522705, 1.5557070970535278, 1.54649817943573, 1.5517910718917847, 1.5105814933776855, 1.510319471359253, 1.5127792358398438, 1.5357016324996948, 1.5223087072372437, 1.4688535928726196, 1.473762035369873, 1.4476702213287354, 1.4415528774261475, 1.4437229633331299, 1.4429270029067993, 1.4882901906967163, 1.4510315656661987, 1.4742385149002075, 1.4630157947540283, 1.4338393211364746, 1.4531103372573853, 1.4511016607284546, 1.4374287128448486, 1.438842535018921, 1.410225749015808, 1.406859278678894, 1.4324126243591309, 1.4010086059570312, 1.4454684257507324, 1.4276442527770996, 1.4386146068572998, 1.4128855466842651, 1.4101604223251343, 1.4111230373382568, 1.3781256675720215]]
global_train_loss_algo: [[], [], []]
