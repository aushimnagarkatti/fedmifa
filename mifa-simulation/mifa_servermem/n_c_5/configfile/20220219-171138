import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'shakespeare_lstm' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.05],
    1: [0.05],
    2: [0.05], #0.05
    3: [0.05],
    4: [0.05]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1,
    4:1
    }

sch_freq = 1 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold",
            4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[4.066511004456486, 2.628551954984091, 2.5266945487907746, 2.209736940736348, 2.265826809517606, 2.0280278066262913, 1.9239789971802648, 1.9518975581601854, 1.8285649010763574, 1.936700700231627, 1.811474856953121, 1.8350668394342222, 1.838917041951254, 1.7400822048114128, 1.7470634377300294, 1.7191708234636223, 1.7577130085349542, 1.6878885768293366, 1.6449313520406463, 1.6238504208077031, 1.6146770204210275, 1.5340150254518368, 1.6474016506147007, 1.633591796149584, 1.580219965491116, 1.5463191458459886, 1.6276803969026727, 1.5760657125149926, 1.5396045526683455, 1.4485065887597406, 1.5596762146932381, 1.6077901622896082, 1.625676306999398, 1.462230208819059, 1.5245669157467059, 1.5680202457855938, 1.5628829507949547, 1.5080780485734457, 1.5312116256678543, 1.550476210936235, 1.5215287698094848, 1.5008985999624096, 1.4498994402050849, 1.5306464772458082, 1.4678057442477428, 1.5323161547324238, 1.4538530264263658, 1.6521736017028537, 1.4814294580432263, 1.541362167846621], [4.070457433976718, 2.14537354542629, 2.000344582595534, 1.7865843399093824, 1.8697977721264103, 1.6276195279019106, 1.5520103322253949, 1.5108710420858917, 1.426746586275881, 1.5381423369279872, 1.4158178424554992, 1.4871519751034743, 1.3770477550280722, 1.3613572512497645, 1.339649386526278, 1.3244734316642575, 1.3482624874737907, 1.269499118400783, 1.2956189871119428, 1.2236127112870894, 1.2430185196569972, 1.1216938948338249, 1.2644545420815099, 1.2428722113122164, 1.1751733741929695, 1.1391863396224522, 1.204727457814393, 1.2383507997047223, 1.1469970621445587, 1.0348695108379364, 1.224391721561782, 1.200162939898503, 1.254114726386103, 1.101024572969353, 1.1709631214027365, 1.2257670817980368, 1.1934643065474173, 1.1511621768284261, 1.1865580835102807, 1.1719146410102925, 1.1600371719961067, 1.164530441018056, 1.0490884179752573, 1.1775044791082874, 1.0773819775437734, 1.1609011086523533, 1.100570711280319, 1.2663370785231773, 1.1247098205328534, 1.1690616404538041], [4.0665234269685815, 2.1433359455932792, 1.978950976349331, 1.786847319029708, 1.8671805869439608, 1.6269525869596493, 1.560788803323042, 1.529000958554161, 1.4537741323165057, 1.5165715518878176, 1.410350609540174, 1.4768522285505947, 1.4135908973351454, 1.3816343227935086, 1.3114188725965117, 1.3658594868566865, 1.4052337687206427, 1.2721043695618466, 1.3378577870717598, 1.270584437711598, 1.2423065213326583, 1.1519674399296442, 1.293028598672779, 1.2634943082356318, 1.2381497036671054, 1.2051449392996136, 1.2609140845980062, 1.244750862107082, 1.1287429029378, 1.0851613319296587, 1.253647162240268, 1.2693701466402, 1.2704934735580897, 1.1342145297797674, 1.2161299555286385, 1.2554710388806072, 1.20805566594082, 1.164304622191598, 1.2339362737285142, 1.2267877751286655, 1.197040781486567, 1.1857011459247482, 1.129481777554271, 1.2193124441142063, 1.129596139036508, 1.1819179924399243, 1.1454822067934973, 1.3358866453460294, 1.1675451192966722, 1.2076404547407464], [4.070434352607885, 2.134616883978147, 2.011390717379669, 1.8510400698323832, 1.8413436187287917, 1.7609315923057298, 1.6929838733931213, 1.6687673760793071, 1.5657205959331761, 1.666636782384386, 1.5252820357725043, 1.6750509531956461, 1.481202377334872, 1.597546435888582, 1.61736346548876, 1.5662229807043777, 1.484936314086319, 1.4611316727331984, 1.52824482556371, 1.4910284301403993, 1.4576722844756511, 1.3471427039337587, 1.5507236771383295, 1.5334161752614408, 1.467512915474598, 1.4211143648018913, 1.5123640212579648, 1.53151184641186, 1.4437372799432677, 1.3187885063503946, 1.523499530754927, 1.5198958162146465, 1.6093970218354532, 1.4101546124537285, 1.4935619481325548, 1.5774445053993709, 1.484366524104951, 1.4682038790691003, 1.47437306616, 1.4683354246530271, 1.4519152170018115, 1.5100459607507521, 1.4049622075039516, 1.5415561456182063, 1.404481422834607, 1.539189135516391, 1.422517718403483, 1.6595434494795043, 1.479103505020509, 1.5739280241283469], [4.06736608285492, 2.1121848840881845, 1.9777057352961207, 1.7670874860633146, 1.8344888458294257, 1.6410047492202124, 1.5706055109062764, 1.5470121903620715, 1.434680524135785, 1.527977543223138, 1.4435768871208523, 1.486972992418675, 1.4149039779375667, 1.390868754776982, 1.3584281468397688, 1.3998245855928246, 1.4078377274411378, 1.3829891822977765, 1.354803093450677, 1.2860988496463048, 1.2932414331468374, 1.1794691123377863, 1.2663586675555278, 1.3160269844367198, 1.287079625391169, 1.250528533019717, 1.302347247739042, 1.2891451414963704, 1.1964085814495193, 1.1485549265611525, 1.27207521019366, 1.302106817036661, 1.327409833847756, 1.1561675410925616, 1.2672889496614466, 1.2614423211516257, 1.24120845685473, 1.1751091434550849, 1.2789420612706028, 1.238947134327042, 1.224529359731087, 1.2458695543485352, 1.1651367126177594, 1.2398827907071515, 1.2125029924729742, 1.2239565895655578, 1.177909263962572, 1.368628847549778, 1.2076385149886861, 1.259182754990771]]
acc_algo = [[tensor(0.0104), tensor(0.2390), tensor(0.2256), tensor(0.3204), tensor(0.3483), tensor(0.3851), tensor(0.4025), tensor(0.4130), tensor(0.4270), tensor(0.4404), tensor(0.4399), tensor(0.4467), tensor(0.4588), tensor(0.4627), tensor(0.4709), tensor(0.4726), tensor(0.4777), tensor(0.4805), tensor(0.4779), tensor(0.4856), tensor(0.4794), tensor(0.4886), tensor(0.4924), tensor(0.4956), tensor(0.4974), tensor(0.4992), tensor(0.4990), tensor(0.5024), tensor(0.5039), tensor(0.5060), tensor(0.5057), tensor(0.5068), tensor(0.5068), tensor(0.5078), tensor(0.5094), tensor(0.5084), tensor(0.5105), tensor(0.5123), tensor(0.5148), tensor(0.5146), tensor(0.5154), tensor(0.5152), tensor(0.5167), tensor(0.5177), tensor(0.5181), tensor(0.5188), tensor(0.5186), tensor(0.5198), tensor(0.5202), tensor(0.5202)], [tensor(0.0104), tensor(0.3795), tensor(0.4148), tensor(0.4518), tensor(0.4636), tensor(0.4821), tensor(0.4924), tensor(0.4961), tensor(0.5080), tensor(0.5134), tensor(0.5160), tensor(0.5202), tensor(0.5234), tensor(0.5269), tensor(0.5278), tensor(0.5314), tensor(0.5324), tensor(0.5325), tensor(0.5355), tensor(0.5342), tensor(0.5371), tensor(0.5384), tensor(0.5381), tensor(0.5399), tensor(0.5386), tensor(0.5413), tensor(0.5417), tensor(0.5412), tensor(0.5394), tensor(0.5423), tensor(0.5417), tensor(0.5412), tensor(0.5422), tensor(0.5423), tensor(0.5427), tensor(0.5431), tensor(0.5428), tensor(0.5439), tensor(0.5426), tensor(0.5426), tensor(0.5440), tensor(0.5455), tensor(0.5455), tensor(0.5442), tensor(0.5437), tensor(0.5446), tensor(0.5461), tensor(0.5440), tensor(0.5438), tensor(0.5444)], [tensor(0.0104), tensor(0.3776), tensor(0.4195), tensor(0.4509), tensor(0.4567), tensor(0.4783), tensor(0.4873), tensor(0.4985), tensor(0.5016), tensor(0.5093), tensor(0.5133), tensor(0.5165), tensor(0.5164), tensor(0.5220), tensor(0.5176), tensor(0.5254), tensor(0.5264), tensor(0.5254), tensor(0.5313), tensor(0.5303), tensor(0.5301), tensor(0.5350), tensor(0.5291), tensor(0.5320), tensor(0.5319), tensor(0.5333), tensor(0.5328), tensor(0.5384), tensor(0.5332), tensor(0.5322), tensor(0.5378), tensor(0.5309), tensor(0.5328), tensor(0.5352), tensor(0.5384), tensor(0.5357), tensor(0.5395), tensor(0.5384), tensor(0.5407), tensor(0.5395), tensor(0.5407), tensor(0.5403), tensor(0.5392), tensor(0.5394), tensor(0.5390), tensor(0.5407), tensor(0.5395), tensor(0.5390), tensor(0.5432), tensor(0.5399)], [tensor(0.0104), tensor(0.3829), tensor(0.4181), tensor(0.4513), tensor(0.4572), tensor(0.4754), tensor(0.4820), tensor(0.4907), tensor(0.4962), tensor(0.5019), tensor(0.5051), tensor(0.5063), tensor(0.5097), tensor(0.5115), tensor(0.5101), tensor(0.5127), tensor(0.5147), tensor(0.5119), tensor(0.5154), tensor(0.5129), tensor(0.5147), tensor(0.5156), tensor(0.5170), tensor(0.5164), tensor(0.5138), tensor(0.5162), tensor(0.5186), tensor(0.5188), tensor(0.5141), tensor(0.5175), tensor(0.5170), tensor(0.5155), tensor(0.5178), tensor(0.5192), tensor(0.5227), tensor(0.5179), tensor(0.5203), tensor(0.5187), tensor(0.5193), tensor(0.5167), tensor(0.5188), tensor(0.5190), tensor(0.5175), tensor(0.5153), tensor(0.5161), tensor(0.5149), tensor(0.5177), tensor(0.5163), tensor(0.5192), tensor(0.5137)], [tensor(0.0104), tensor(0.3858), tensor(0.4221), tensor(0.4531), tensor(0.4595), tensor(0.4670), tensor(0.4801), tensor(0.4868), tensor(0.4986), tensor(0.5057), tensor(0.5047), tensor(0.5065), tensor(0.5093), tensor(0.5113), tensor(0.5136), tensor(0.5141), tensor(0.5094), tensor(0.5136), tensor(0.5217), tensor(0.5240), tensor(0.5250), tensor(0.5277), tensor(0.5245), tensor(0.5290), tensor(0.5144), tensor(0.5241), tensor(0.5305), tensor(0.5326), tensor(0.5253), tensor(0.5229), tensor(0.5275), tensor(0.5251), tensor(0.5239), tensor(0.5213), tensor(0.5294), tensor(0.5265), tensor(0.5314), tensor(0.5316), tensor(0.5332), tensor(0.5320), tensor(0.5346), tensor(0.5366), tensor(0.5317), tensor(0.5306), tensor(0.5295), tensor(0.5347), tensor(0.5370), tensor(0.5297), tensor(0.5313), tensor(0.5243)]]
test_loss_algo= [[4.848340158240872, 3.2173317097864502, 2.9091223859815405, 2.5270324645472937, 2.3357280884581897, 2.213299404132636, 2.1261012237319217, 2.0573320524307483, 2.008172657052155, 1.9621071464185635, 1.9713882153700588, 1.931062753120253, 1.8965141332760471, 1.8665941744083727, 1.8564315434185625, 1.8342978108732009, 1.8184150560393806, 1.8092560826204673, 1.8091402712219236, 1.7963790400519843, 1.795960404733651, 1.7793732119260157, 1.760017770319218, 1.7506425909795693, 1.7465747648825662, 1.738738212934268, 1.7380960712451332, 1.72695833863746, 1.7218522704223058, 1.7167441852577252, 1.7139849093197639, 1.707943754015364, 1.7107577332261066, 1.706080465958286, 1.7043476579300127, 1.6989600596698589, 1.6941081246136909, 1.6901108265473674, 1.6878215746068272, 1.6818745065662517, 1.683624966414168, 1.682687543279295, 1.6757965610141692, 1.6744133371224847, 1.671118337034966, 1.6721731007401082, 1.6691238323006783, 1.6663440747788183, 1.6645696569895716, 1.6658826669738034], [4.848340158240872, 2.2144378454594276, 2.072679236140524, 1.9134129563588493, 1.8702309613112755, 1.80305412447211, 1.7632542274386152, 1.7383719960114385, 1.7054144574753292, 1.6856888410711885, 1.670838588788768, 1.6627819119587417, 1.6470817455109452, 1.6350771209782298, 1.6302788318517807, 1.6197176613313222, 1.61918535073764, 1.6172420091669522, 1.608398045096835, 1.6085188415643001, 1.60093585960345, 1.5984292252164631, 1.6021448350439338, 1.597529012652134, 1.6000935519658055, 1.5971267883520444, 1.5933595707948642, 1.596587081489745, 1.608595684863387, 1.5976016091025202, 1.594647757096285, 1.5970475163057678, 1.6010623677090894, 1.5936223350030163, 1.5930011145086767, 1.6014607950814148, 1.5969794337705434, 1.5958836134500953, 1.5973629358449908, 1.5978782378998502, 1.5999436144394017, 1.5966959407595258, 1.5949636814722856, 1.601408846081369, 1.602202358525757, 1.599967863974611, 1.5988727159025558, 1.5998745981128764, 1.6008973598782155, 1.6032075807807413], [4.848340158240872, 2.2130201678097037, 2.0218113691005435, 1.9069630672202775, 1.8811468295886207, 1.8171127239981482, 1.7794992404416008, 1.745455408852916, 1.7238318950328555, 1.6988688637996459, 1.6867026613761047, 1.6749642798865794, 1.6772532437764844, 1.6663268388330013, 1.659933836333658, 1.6446027617914885, 1.6397901593537665, 1.650267246617748, 1.6210963002298386, 1.6312702134145032, 1.6321897266590268, 1.6171440248232205, 1.6359940860087454, 1.6169948146564173, 1.6309866601150567, 1.6178091514824542, 1.6214048458935961, 1.6072919410392552, 1.62811130855379, 1.6288316675125487, 1.6199251054758395, 1.6262460231994134, 1.6246748387778047, 1.623307927670368, 1.611400239782339, 1.6121257162065699, 1.608329663543792, 1.6026917078995164, 1.6018817874029083, 1.610748872604927, 1.6011651963954603, 1.5973690798654885, 1.6068764270080145, 1.6050287178751965, 1.6154662177357402, 1.606576915666088, 1.6097297224796714, 1.6133266448068817, 1.5927442770464628, 1.6098341155481992], [4.848340158240872, 2.2035944648664243, 2.0354286560320314, 1.9116313928678295, 1.9072713531171892, 1.825381457041927, 1.797184375830714, 1.7680552790076856, 1.749536072770944, 1.7338052819859284, 1.7207711663305831, 1.7203021415545914, 1.7093082103474206, 1.704931813082408, 1.7135849062073643, 1.69309625718676, 1.6897394911111894, 1.701262570020695, 1.6906107016733634, 1.7007005055200783, 1.6991720908233747, 1.6948541952152787, 1.6957401646405494, 1.6868151361133827, 1.7019368191477227, 1.69187121174972, 1.6869943719423854, 1.6865209958230782, 1.704189825301304, 1.6893576643606192, 1.689470190896749, 1.6935923497000525, 1.6885651688094787, 1.6858415410846153, 1.675446000247661, 1.6831108259483372, 1.6811455729482996, 1.6861702344769942, 1.6852958896269532, 1.7025317917172758, 1.6899224564308772, 1.6920542105392988, 1.6953883802027612, 1.701140924959629, 1.7031620732365405, 1.7060961541370072, 1.6933479338031845, 1.6975257799686494, 1.6885890111025241, 1.703940281797649], [4.848340158240872, 2.1892612275264542, 2.048993795222122, 1.9066899324344366, 1.8833284497225526, 1.8489798080260478, 1.8043214703274, 1.7855651637403842, 1.7395760241687226, 1.7150932093982332, 1.7236898022840361, 1.7082938710327513, 1.7065420023235576, 1.698388127598987, 1.6918318567528912, 1.6881631985839558, 1.7028865768564188, 1.6902475002908877, 1.6660096030393716, 1.6569110619013017, 1.651348859196056, 1.6440846061109786, 1.663822931331872, 1.6349010345425026, 1.6952760992693816, 1.652678569723298, 1.6321229220360198, 1.631216471906001, 1.6577870144161337, 1.664134680914012, 1.6392887626664716, 1.6844146950381305, 1.6572999292906903, 1.6614798244901436, 1.6424692210800174, 1.645996848851691, 1.635681299268489, 1.6273721212496775, 1.6193286866322605, 1.6356523625632577, 1.6200289096820908, 1.6179221557792947, 1.6315668900902423, 1.6287589685212158, 1.6440454857230755, 1.6212343037643506, 1.628130462975269, 1.6498814968922852, 1.6242024561876478, 1.6666051401302697]]
global_train_loss_algo = [[], [], [], [], []]
