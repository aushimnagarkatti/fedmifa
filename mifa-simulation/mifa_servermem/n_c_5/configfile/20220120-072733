import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.2],#[1/np.power(10,2.5)],
    1: [0.08],#[1/np.power(10,2)],
    2: [0.08]#[1/np.power(10,2)]
    }

lrfactor = {
    0:1
    , #factor to reduce lr in scheduler
    1:1,
    2:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.9079729302724195, 3.6453337326049797, 3.2231663235028583, 2.988984800974528, 2.397420849005381, 2.2810164713859558, 2.1038840425014493, 1.882599790096283, 1.7416813141504925, 1.7595694263776143, 1.5646323277836756, 1.7478679267565411, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [3.8068903811772663, 1.8126642058690385, 1.2903858989477157, 1.1712291624546052, 0.7679190983374914, 0.7002089078227678, 0.9091840431094169, 0.8943687975804011, 0.779273733774821, 0.7634453420837721, 0.715247029807596, 0.7925600893298784, 0.8389450815121332, 0.6833635424574216, 0.7430686662594478, 0.6627627104173104, 0.5123261945943037, 0.6922333449522654, 0.6600371562043825, 0.8891850689053534, 0.6954052739342054, 0.5747079428037007, 0.5241280640314023, 0.626064844330152, 0.6660607947905858, 0.5763663232525189, 0.7923781142632167, 0.5865075894196827, 0.7659440822203955, 0.5621803706884385, 0.6329611256718636, 0.7130342832158009, 0.6478289786974589, 0.47058757940928153, 0.8472331501046815, 0.7191595059335232, 0.530759463250637, 0.7177450834115346, 0.48423387817510716, 0.48682055393854784, 0.4043603686243296, 0.5667978077431519, 0.7947579859693845, 0.5721590307354927, 0.5927889907856783, 0.4543039583961169, 0.8599111999571324, 0.6429210128386815, 0.45482594937086096, 0.5455719468990962, 0.5498639686803023, 0.6296203722953797, 0.5469034260114034, 0.4222675940195719, 0.5552169645627341, 0.45677654256423317, 0.5900685179730256, 0.6904263589580854, 0.4346947096909086, 0.7539858044187226], [3.8271473097801207, 1.8598330685297646, 1.2941303501526513, 1.1175787994861603, 0.7552060374617577, 0.6820111158490181, 0.861677192290624, 0.8608610521554947, 0.7867729007005693, 0.7956603372097015, 0.6227453632014139, 0.798808997074763, 0.7917322926521302, 0.7127800769607227, 0.7497468042373656, 0.6536037384668985, 0.5380837752918401, 0.6695828401048979, 0.6444166531364124, 0.8860923970341682, 0.6881921270489693, 0.6168899644414584, 0.5419597855408986, 0.6073361933032672, 0.6793676308294139, 0.5790700738032658, 0.785863210260868, 0.5588682203491528, 0.7751666056315104, 0.582855010330677, 0.6486716167132061, 0.6115140078465144, 0.6283421008785565, 0.49095793912808106, 0.4599234034990271, 0.6007707573175429, 0.49715181231498723, 0.6794961335261663, 0.929839237873753, 0.5386260557572047, 0.4203159440805515, 0.5740407109828223, 0.8358044493198395, 0.5606008831659953, 0.5423399711648623, 0.4288167881667614, 0.5764681425193945, 0.6638559623559316, 0.49915628870328266, 0.550782137910525, 0.5307524690429369, 0.6070456322431564, 0.5261562313238779, 0.4210388068556785, 0.5245483681956927, 0.4480238620688518, 0.5801944193740686, 0.694954320748647, 0.4356459034482638, 0.6901756483316422]]
Acc algo: [[tensor(0.), tensor(0.0312), tensor(0.0312), tensor(0.0469), tensor(0.0469), tensor(0.2031), tensor(0.2031), tensor(0.2500), tensor(0.3125), tensor(0.2500), tensor(0.2969), tensor(0.2812), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.0312)], [tensor(0.0312), tensor(0.2188), tensor(0.4062), tensor(0.4219), tensor(0.3750), tensor(0.4375), tensor(0.4844), tensor(0.4844), tensor(0.5312), tensor(0.5312), tensor(0.4844), tensor(0.5781), tensor(0.5781), tensor(0.5469), tensor(0.4531), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5781), tensor(0.6250), tensor(0.6094), tensor(0.5625), tensor(0.5312), tensor(0.5781), tensor(0.5781), tensor(0.5938), tensor(0.5156), tensor(0.5469), tensor(0.5156), tensor(0.5625), tensor(0.6094), tensor(0.5312), tensor(0.5781), tensor(0.6250), tensor(0.5000), tensor(0.5469), tensor(0.5625), tensor(0.5938), tensor(0.6094), tensor(0.5625), tensor(0.6250), tensor(0.6406), tensor(0.5781), tensor(0.5469), tensor(0.6094), tensor(0.6094), tensor(0.5781), tensor(0.6562), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5781), tensor(0.5781), tensor(0.5938), tensor(0.5312), tensor(0.5938), tensor(0.6250), tensor(0.6250)], [tensor(0.0156), tensor(0.2500), tensor(0.3594), tensor(0.4062), tensor(0.3594), tensor(0.4688), tensor(0.5156), tensor(0.4375), tensor(0.5000), tensor(0.5156), tensor(0.5625), tensor(0.5469), tensor(0.5469), tensor(0.3594), tensor(0.4219), tensor(0.5781), tensor(0.5156), tensor(0.6094), tensor(0.5781), tensor(0.6250), tensor(0.3750), tensor(0.5469), tensor(0.6094), tensor(0.6250), tensor(0.5938), tensor(0.6094), tensor(0.5938), tensor(0.4688), tensor(0.5938), tensor(0.5781), tensor(0.4844), tensor(0.6094), tensor(0.5469), tensor(0.4844), tensor(0.6094), tensor(0.5156), tensor(0.6875), tensor(0.5625), tensor(0.5312), tensor(0.5000), tensor(0.4531), tensor(0.6406), tensor(0.5312), tensor(0.6094), tensor(0.5469), tensor(0.5781), tensor(0.5469), tensor(0.6094), tensor(0.5312), tensor(0.4375), tensor(0.6094), tensor(0.5156), tensor(0.6250), tensor(0.5625), tensor(0.5000), tensor(0.4844), tensor(0.4844), tensor(0.5938), tensor(0.4375), tensor(0.5781)]]
test_loss_algo: [[4.126654624938965, 4.053789138793945, 4.134313106536865, 3.5671627521514893, 3.084527015686035, 2.6982016563415527, 2.6546905040740967, 2.6454381942749023, 2.7286794185638428, 2.896487236022949, 2.356259346008301, 2.372807264328003, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan], [4.271768569946289, 2.7485036849975586, 1.9318219423294067, 1.7738492488861084, 1.9060547351837158, 1.5892961025238037, 1.652123212814331, 1.3668750524520874, 1.9018666744232178, 1.5403972864151, 1.746747374534607, 1.3678991794586182, 1.4701471328735352, 1.4024646282196045, 1.6762526035308838, 1.2494683265686035, 1.4709904193878174, 1.6091375350952148, 1.296684741973877, 1.1099616289138794, 1.3954614400863647, 1.406943917274475, 1.2548141479492188, 1.2965303659439087, 1.4329867362976074, 1.1682753562927246, 1.40308678150177, 1.4275048971176147, 1.2846064567565918, 1.2803475856781006, 1.1609934568405151, 1.3326913118362427, 1.161960482597351, 1.0619027614593506, 1.3697707653045654, 1.3778475522994995, 1.1938362121582031, 1.1671324968338013, 1.226448893547058, 1.204037070274353, 1.2266254425048828, 1.019035816192627, 1.2834464311599731, 1.3393619060516357, 1.3160558938980103, 1.2840136289596558, 1.4177777767181396, 1.0757639408111572, 1.2827297449111938, 1.431706190109253, 1.304262638092041, 1.3103963136672974, 1.2901920080184937, 1.2089500427246094, 1.216540813446045, 1.170743465423584, 1.3835017681121826, 1.3327152729034424, 1.180734395980835, 1.0483791828155518], [4.22380256652832, 2.6562795639038086, 2.1021387577056885, 1.7265478372573853, 1.7811557054519653, 1.77090585231781, 1.6236964464187622, 1.6698715686798096, 1.6041407585144043, 1.6442970037460327, 1.5571383237838745, 1.317903995513916, 1.2554904222488403, 1.692234754562378, 1.84327232837677, 1.1851041316986084, 1.5309360027313232, 1.0394142866134644, 1.1459013223648071, 1.0348321199417114, 1.881630778312683, 1.329277753829956, 1.1550575494766235, 1.16770601272583, 1.2077972888946533, 1.16286039352417, 1.214744210243225, 1.666167974472046, 1.1198101043701172, 1.130515456199646, 1.5851863622665405, 1.0274903774261475, 1.1631652116775513, 1.34731125831604, 1.0092610120773315, 1.3713557720184326, 0.9545491337776184, 1.2840741872787476, 1.1868714094161987, 1.2714478969573975, 1.5607622861862183, 0.9973441362380981, 1.4424629211425781, 1.1243847608566284, 1.2387118339538574, 1.1076176166534424, 1.5190664529800415, 1.1504794359207153, 1.3515113592147827, 1.5781049728393555, 0.9894319772720337, 1.2557138204574585, 0.974719762802124, 1.1206953525543213, 1.2565677165985107, 1.5148415565490723, 1.681290864944458, 1.0937613248825073, 1.518491268157959, 1.2275787591934204]]
global_train_loss_algo: [[], [], []]
