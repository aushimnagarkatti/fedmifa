import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = False


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'dynamic' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,2)],
    2: [1/np.power(10,2)],
    3: [0.01]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.250185441970825, 2.036592791080475, 1.5368201512098314, 1.0752816233038902, 0.7842651590704918, 0.6840139952301979, 0.8140361189842225, 0.7044646539934911, 0.6070434109866618, 0.5703882758133113, 0.7826706638932228, 0.6582856196910144, 0.5205039769411087, 0.4920514870760962, 0.5987740752939135, 0.6058711806684732, 0.5207625999487936, 0.5628604767471552, 0.377870995849371, 0.5140485446620733], [2.1602380096912386, 0.9685712599754334, 0.9748665523529054, 0.8419776287674903, 0.857250474691391, 0.7193256884859874, 0.9260128220915794, 0.7350820574164391, 0.7806821691989899, 0.7127527222037315, 0.9069251236319541, 0.7803925632312894, 0.7178798045497388, 0.6354119446175173, 0.8340539361536502, 0.7968315617740155, 0.7691144065815024, 0.6735719752497972, 0.6890141569077969, 0.6946493057918269], [2.1639275324344633, 0.7338351625204087, 0.6612519064918161, 0.48596099589020014, 0.5393322499096393, 0.47149625234073034, 0.6933356133103371, 0.5286193362518679, 0.40182576938532294, 0.3967350191157311, 0.64211928691715, 0.5111093471944332, 0.3647403317224234, 0.39364082293817776, 0.4753493546508253, 0.5146453827992082, 0.40517223940230906, 0.4392822988331318, 0.28858771288767454, 0.37212704318808393], [2.1625806832313534, 0.8691457785665989, 0.8850577956438064, 0.6797976893186568, 0.6718464035540819, 0.6492840166017413, 0.8678158843517302, 0.752399322744459, 0.5165763465315103, 0.5080568694695831, 0.8749758987128734, 0.7112185715138912, 0.6209213222004473, 0.5327145891077817, 0.697970783188939, 0.6983193353563546, 0.5579357156716287, 0.6746057773847134, 0.4309014151617885, 0.5806896959338337]]
Acc algo: [[tensor(0.0469), tensor(0.1719), tensor(0.1250), tensor(0.1250), tensor(0.1562), tensor(0.3125), tensor(0.4375), tensor(0.4844), tensor(0.3906), tensor(0.4062), tensor(0.4375), tensor(0.4375), tensor(0.4531), tensor(0.4688), tensor(0.4531), tensor(0.4375), tensor(0.4844), tensor(0.5312), tensor(0.5156), tensor(0.5000)], [tensor(0.0625), tensor(0.1406), tensor(0.2188), tensor(0.2031), tensor(0.2188), tensor(0.1719), tensor(0.1875), tensor(0.2031), tensor(0.3125), tensor(0.2812), tensor(0.2812), tensor(0.1719), tensor(0.2656), tensor(0.3125), tensor(0.3125), tensor(0.2969), tensor(0.2344), tensor(0.2969), tensor(0.2812), tensor(0.2656)], [tensor(0.0625), tensor(0.1875), tensor(0.2344), tensor(0.2188), tensor(0.4219), tensor(0.3594), tensor(0.2344), tensor(0.4219), tensor(0.3125), tensor(0.3906), tensor(0.5312), tensor(0.4062), tensor(0.3906), tensor(0.3438), tensor(0.2031), tensor(0.3438), tensor(0.3594), tensor(0.5000), tensor(0.3750), tensor(0.4688)], [tensor(0.0781), tensor(0.2188), tensor(0.2969), tensor(0.2812), tensor(0.3906), tensor(0.5312), tensor(0.2031), tensor(0.2969), tensor(0.3438), tensor(0.3438), tensor(0.4062), tensor(0.4062), tensor(0.3438), tensor(0.3438), tensor(0.3750), tensor(0.3281), tensor(0.3281), tensor(0.4688), tensor(0.3750), tensor(0.4844)]]
test_loss_algo: [[2.3178937435150146, 2.3011271953582764, 2.383192300796509, 2.483394145965576, 2.087473154067993, 1.9213268756866455, 1.8200699090957642, 1.739867925643921, 1.7523804903030396, 1.7265723943710327, 1.6964945793151855, 1.6477558612823486, 1.6228619813919067, 1.6085658073425293, 1.599439024925232, 1.6074035167694092, 1.5839345455169678, 1.5068066120147705, 1.5032685995101929, 1.478144645690918], [2.3192999362945557, 2.359328269958496, 2.1717522144317627, 2.120046854019165, 2.1835880279541016, 2.206292152404785, 2.1035261154174805, 2.1297240257263184, 2.005197763442993, 1.8819001913070679, 1.953786015510559, 2.0969836711883545, 1.9838725328445435, 2.0887911319732666, 2.004373550415039, 2.0542194843292236, 2.0481159687042236, 1.9630954265594482, 1.913114309310913, 1.9483309984207153], [2.3267364501953125, 2.626621723175049, 2.175957679748535, 2.53281307220459, 1.6897979974746704, 2.002354145050049, 2.2474470138549805, 1.6495888233184814, 2.262322425842285, 1.6388583183288574, 1.5051292181015015, 1.954935073852539, 1.549791693687439, 1.9852946996688843, 2.532158851623535, 2.3138527870178223, 1.6199053525924683, 1.464555263519287, 1.787739634513855, 1.45693838596344], [2.326995611190796, 2.2239766120910645, 1.9670782089233398, 2.0186479091644287, 1.697888731956482, 1.6669102907180786, 2.123966932296753, 1.63588285446167, 1.7584561109542847, 1.723338007926941, 1.5686535835266113, 1.8268868923187256, 1.6414577960968018, 1.988686442375183, 1.7742300033569336, 2.00832200050354, 1.8647375106811523, 1.5606911182403564, 1.4753143787384033, 1.387297511100769]]
global_train_loss_algo: [[2.293347096809036, 2.2933335703657107, 2.3085275568315744, 2.3452191703459797, 2.0580071558427933, 2.0180918906655765, 1.95158648414685, 1.9088693442551985, 1.8132124316052098, 1.737156463401092, 1.7174152516952865, 1.7059828073472318, 1.6653574839272463, 1.6110829680472079, 1.611404149428658, 1.6001207779740434, 1.5857456922531128, 1.5467907806186725, 1.5651969339536584, 1.5400915048311434], [2.293645557235269, 2.2784438974717083, 2.2569501003645875, 2.16984307400101, 2.134958452275952, 2.1097053954058596, 1.945259747755192, 1.9583117957310299, 2.0656449252077382, 1.9930397165400902, 2.04394963025437, 2.0467984124522687, 2.252350502916614, 2.164826867830418, 2.1066438502362925, 2.0974169359792527, 2.288426530025804, 2.1388923880999045, 2.1102485373197006, 2.0652016969897864], [2.2913489262466236, 2.6519908313556098, 2.3072498332508995, 2.5343520510227173, 1.6672381034592534, 2.111006155953078, 1.8678909392308092, 1.650637835950193, 2.3762128020796323, 1.6791748264256645, 1.7170624504308871, 2.0887823351813704, 1.5240917394838065, 1.9138728797893085, 2.1579139357637565, 2.2658232752319494, 1.4388781383519282, 1.7404741407050501, 1.7965706745376977, 1.4574487469995114], [2.2907763844560782, 2.2769417159087824, 2.0264209188768625, 2.15989402523431, 1.7254764117548227, 1.7770674943619067, 1.730892582012869, 1.670314792324515, 1.9548988383444375, 1.727005958557129, 1.668248122153075, 2.0899597962798975, 1.586072107410187, 1.976320815513201, 1.6001506659685802, 1.950697014703775, 1.6371903059732578, 1.8012154683127732, 1.5412327965812, 1.3955591388065796]]
