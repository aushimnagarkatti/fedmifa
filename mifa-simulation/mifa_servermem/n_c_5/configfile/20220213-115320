import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = False


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'dynamic' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,2)],
    2: [1/np.power(10,2)],
    3: [0.01]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.250185441970825, 2.036592791080475, 1.5368201512098314, 1.0752816233038902, 0.7842651590704918, 0.6840139952301979, 0.8140361189842225, 0.7044646539934911, 0.6070434109866618, 0.5703882758133113, 0.7826706638932228, 0.6582856196910144, 0.5205039769411087, 0.4920514870760962, 0.5987740752939135, 0.6058711806684732, 0.5207625999487936, 0.5628604767471552, 0.377870995849371, 0.5140485446620733], [2.1602380096912386, 0.9685712599754334, 0.9748665523529054, 0.8419776287674903, 0.857250474691391, 0.7193256884859874, 0.9260128220915794, 0.7350820574164391, 0.7806821691989899, 0.7127527222037315, 0.9069251236319541, 0.7803925632312894, 0.7178798045497388, 0.6354119446175173, 0.8340539361536502, 0.7968315617740155, 0.7691144065815024, 0.6735719752497972, 0.6890141569077969, 0.6946493057918269], [2.1639275324344633, 0.7338351625204087, 0.6612519064918161, 0.48596099589020014, 0.5393322499096393, 0.47149625234073034, 0.6933356133103371, 0.5286193362518679, 0.40182576938532294, 0.3967350191157311, 0.64211928691715, 0.5111093471944332, 0.3647403317224234, 0.39364082293817776, 0.4753493546508253, 0.5146453827992082, 0.40517223940230906, 0.4392822988331318, 0.28858771288767454, 0.37212704318808393], [2.1625806832313534, 0.8691457785665989, 0.8850577956438064, 0.6797976893186568, 0.6718464035540819, 0.6492840166017413, 0.8678158843517302, 0.752399322744459, 0.5165763465315103, 0.5080568694695831, 0.8749758987128734, 0.7112185715138912, 0.6209213222004473, 0.5327145891077817, 0.697970783188939, 0.6983193353563546, 0.5579357156716287, 0.6746057773847134, 0.4309014151617885, 0.5806896959338337],
            ]
Acc algo: [[(0.0469), (0.1719), (0.1250), (0.1250), (0.1562), (0.3125), (0.4375), (0.4844), (0.3906), (0.4062), (0.4375), (0.4375), (0.4531), (0.4688), (0.4531), (0.4375), (0.4844), (0.5312), (0.5156), (0.5000)],
            ]
test_loss_algo: [[2.3178937435150146, 2.3011271953582764, 2.383192300796509, 2.483394145965576, 2.087473154067993, 1.9213268756866455, 1.8200699090957642, 1.739867925643921, 1.7523804903030396, 1.7265723943710327, 1.6964945793151855, 1.6477558612823486, 1.6228619813919067, 1.6085658073425293, 1.599439024925232, 1.6074035167694092, 1.5839345455169678, 1.5068066120147705, 1.5032685995101929, 1.478144645690918],
] 
global_train_loss_algo: [[2.293347096809036, 2.2933335703657107, 2.3085275568315744, 2.3452191703459797, 2.0580071558427933, 2.0180918906655765, 1.95158648414685, 1.9088693442551985, 1.8132124316052098, 1.737156463401092, 1.7174152516952865, 1.7059828073472318, 1.6653574839272463, 1.6110829680472079, 1.611404149428658, 1.6001207779740434, 1.5857456922531128, 1.5467907806186725, 1.5651969339536584, 1.5400915048311434],
]