import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,3)],
    2: [1/np.power(10,3)],
    3: [0.01]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            #0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            #3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.3147422194480898, 2.295790891647339, 2.2598897314071658, 2.267341690063476, 2.316811573505402, 2.287222146987915, 2.297465045452118, 2.316575248241425, 2.291324076652527, 2.295640270709991, 2.3061397790908815, 2.312392661571503, 2.3090928483009336, 2.271814477443695, 2.2701652288436893, 2.2594298481941224, 2.2671699976921085, 2.295944187641144, 2.291255633831024, 2.261588320732117, 2.2880462598800664, 2.314966950416565, 2.297989900112152, 2.2983934569358824, 2.2726782178878784, 2.297646484375, 2.2735367751121522, 2.29840491771698, 2.3103420615196226, 2.3113727593421936, 2.286577622890472, 2.302248530387878, 2.272925989627838, 2.267227776050568, 2.244395143985748, 2.2657852101325986, 2.2936755895614622, 2.282669610977173, 2.2758903813362124, 2.2709021186828613, 2.2346634960174563, 2.2665209126472474, 2.2147312593460087, 2.2637878751754763, 2.247146518230438, 2.2885605335235595, 2.297546644210816, 2.250801734924316, 2.209014372825622, 2.1738790392875673], [2.312751295566559, 2.29908744096756, 2.255892796516419, 2.2679313707351683, 2.3004018139839175, 2.280833430290222, 2.2793993496894833, 2.2766117000579835, 2.3018335628509523, 2.2727143859863284, 2.272958073616028, 2.2095461249351502, 1.9954679715633392, 1.8026041543483733, 1.1224102598428725, 0.9529875349253416, 0.6567078599892556, 0.9369589385390281, 0.8827580761117859, 0.8722500520944594, 0.8949358548223971, 0.8115475758910179, 0.8948161411285401, 0.9161074775457383, 0.7017603851109743, 0.8793631130456925, 0.6610056684352457, 0.7147420164663345, 0.7788924452662468, 0.7235355648398399, 0.8769806945323945, 0.8326698762178422, 0.7375492604076863, 0.7342656561732293, 0.7447914023697375, 0.42195306795649234, 0.6717892894661054, 0.7171752314595505, 0.6398961988510564, 0.8591921281814574, 0.5306046270253136, 0.5433696470828726, 0.5301861562207341, 0.7598475638031961, 0.741949463635683, 0.7184896018356086, 0.7770469163358211, 0.5958325401693582, 0.5653037632908673, 0.6258758489997126]]
Acc algo: [[tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.0938), tensor(0.1094), tensor(0.0938), tensor(0.0938), tensor(0.0938), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.0938), tensor(0.0938), tensor(0.0938), tensor(0.0938), tensor(0.0938), tensor(0.0938), tensor(0.1094), tensor(0.0938), tensor(0.0781), tensor(0.0938), tensor(0.1406), tensor(0.1406), tensor(0.1562), tensor(0.1875), tensor(0.2188), tensor(0.1875), tensor(0.2031), tensor(0.1875), tensor(0.1875), tensor(0.1875), tensor(0.1875), tensor(0.2031), tensor(0.2031), tensor(0.1875), tensor(0.2031), tensor(0.2031), tensor(0.2031), tensor(0.2188), tensor(0.2188), tensor(0.2031)], [tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1406), tensor(0.1094), tensor(0.1250), tensor(0.1094), tensor(0.1094), tensor(0.1250), tensor(0.1719), tensor(0.2031), tensor(0.1094), tensor(0.1875), tensor(0.2031), tensor(0.1719), tensor(0.1719), tensor(0.2344), tensor(0.1719), tensor(0.1562), tensor(0.1406), tensor(0.2656), tensor(0.1719), tensor(0.0625), tensor(0.1875), tensor(0.0312), tensor(0.2188), tensor(0.1250), tensor(0.0469), tensor(0.2656), tensor(0.2188), tensor(0.2188), tensor(0.3438), tensor(0.2188), tensor(0.1875), tensor(0.2188), tensor(0.1719), tensor(0.1250), tensor(0.1719), tensor(0.0781), tensor(0.3438), tensor(0.2344), tensor(0.1719), tensor(0.2188), tensor(0.1875), tensor(0.2344), tensor(0.3125), tensor(0.3594), tensor(0.4062), tensor(0.3750)]]
test_loss_algo: [[2.293959856033325, 2.2921652793884277, 2.291465997695923, 2.2915971279144287, 2.291172742843628, 2.2911300659179688, 2.290958881378174, 2.290562629699707, 2.290327787399292, 2.290262460708618, 2.2899322509765625, 2.2895219326019287, 2.289273977279663, 2.289163589477539, 2.2887802124023438, 2.288259506225586, 2.287691354751587, 2.2872328758239746, 2.286912679672241, 2.286479949951172, 2.2860634326934814, 2.2852189540863037, 2.2846386432647705, 2.2840466499328613, 2.2833786010742188, 2.2831907272338867, 2.2826294898986816, 2.281867742538452, 2.281162738800049, 2.2802894115448, 2.279087543487549, 2.2781074047088623, 2.2769503593444824, 2.2760493755340576, 2.2748050689697266, 2.2738828659057617, 2.2724642753601074, 2.2706170082092285, 2.269071340560913, 2.267131805419922, 2.2650511264801025, 2.262782573699951, 2.261369228363037, 2.258920907974243, 2.256707191467285, 2.2538933753967285, 2.251157283782959, 2.24838924407959, 2.2452762126922607, 2.2406105995178223], [2.2936911582946777, 2.289529800415039, 2.2878007888793945, 2.288090944290161, 2.2840795516967773, 2.2853689193725586, 2.2812352180480957, 2.2744693756103516, 2.2711777687072754, 2.270721197128296, 2.2576513290405273, 2.226989269256592, 2.245082378387451, 2.2437314987182617, 2.2110583782196045, 2.9635260105133057, 2.38350510597229, 2.7576193809509277, 2.7532169818878174, 2.8844499588012695, 2.707838296890259, 2.541863203048706, 2.7462215423583984, 2.6507771015167236, 2.5265400409698486, 2.954806327819824, 2.385706901550293, 2.794973611831665, 3.0054028034210205, 2.4003260135650635, 2.566901922225952, 2.5753097534179688, 2.0448315143585205, 2.660738706588745, 2.156193494796753, 2.528646230697632, 2.506316900253296, 2.8686180114746094, 2.5056707859039307, 2.48130202293396, 2.01674747467041, 2.3444416522979736, 2.5759823322296143, 2.165823459625244, 2.3608853816986084, 2.3407986164093018, 2.0863778591156006, 2.0419931411743164, 1.840355634689331, 1.8751293420791626]]
global_train_loss_algo: [[], []]
