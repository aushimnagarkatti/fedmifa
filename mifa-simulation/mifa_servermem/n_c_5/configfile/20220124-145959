import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist'#'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.3],#[1/np.power(10,2.5)],
    1: [0.1],#[1/np.power(10,2)],
    2: [0.1]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.8
    , #factor to reduce lr in scheduler
    1:0.8,
    2:0.8
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.809146305720011, 3.4233592414855956, 3.1943315134048462, 2.838066239039103, 2.4904176520506542, 2.0188281893730164, 2.0282860215504965, 1.8199191319942476, 1.4532390701770783, 1.570682813167572, 1.5827038450241089, 1.5489265473683673, 1.3527493845621748, 1.5508094982206821, 1.320241077740987, 1.2063381290435793, 1.414579504887263, 1.4291199598312379, 1.5110504528681439, 1.5356249602635703, 1.4988924924532572, 1.5106858654816944, 1.3675883448521298, 1.5222395681142806, 1.5200819945335389, 1.4121154234409334, 1.4686323322455088, 1.274614349683126, 1.453045747836431, 1.372820618510246, 1.2576406395435333, 1.458728412389755, 1.5615482210318248, 1.4913204083442686, 1.3732593371073405, 1.126164095878601, 1.5736936434110007, 1.5568593535025916, 1.1757563751935958, 1.754583800236384, 1.371887926777204, 1.3499327584107719, 1.5973787411053977, 1.4014768159389497, 1.241584268808365, 1.347232453028361, 1.2818288299242657, 1.334102121512095, 1.4872548321882886, 1.143248347957929, 1.7959196681976315, 1.7626094761689504, 1.7608960327307384, 1.6332977180480956, 1.5232331428527832, 1.4016935243606572, 1.6034036928017934, 1.1107992961009343, 1.3252019852002461, 1.7288410949707032], [3.729994998296102, 1.47016037662824, 1.2082088464101155, 0.9761650444269181, 1.0517406397064526, 0.7130081841349601, 1.018710148135821, 0.7056753333409628, 0.5708963871002197, 0.6874835168123246, 0.902906314531962, 0.7399983488122622, 0.6938045362854999, 0.9956347268124424, 0.6618581587076188, 0.5959814419349034, 0.7579728033542634, 0.7244852682352066, 0.8770568908055623, 0.7481941318511962, 0.8324164709200461, 0.9029605807860692, 0.6887958691865206, 0.8514901147683462, 0.842736535469691, 0.8493692713181178, 0.847721264799436, 0.7154575738112132, 0.7660832312107086, 0.8083694049219291, 0.6904046737750371, 0.8763001153866451, 0.925388223449389, 0.9550185165007908, 0.8304858762820562, 0.6601424638430278, 0.9893697067101795, 0.8868045768241088, 0.675137541734037, 1.083466041723887, 0.8454303912421068, 0.8133379765351613, 1.0389052581787108, 0.9337873037656148, 0.8008802906672159, 0.8742539100845654, 0.8252848154703777, 0.8929317080577215, 0.9388125801086427, 0.7641150880455971, 1.1865198008219402, 1.2582545965512595, 1.1772101591428121, 1.0793103094696996, 1.0887229465643566, 0.8916293747425079, 1.1409277913570404, 0.7776553016901016, 0.9420075341065726, 1.2544695004622142], [3.727511584599813, 1.4551516874631247, 1.1679444822867713, 0.9677518389125666, 0.9149543186028799, 0.7096622496843338, 0.9524450031916298, 0.6821899493535359, 0.556903635263443, 0.6746184912125269, 0.8842317637602488, 0.7091286022464434, 0.6486040255725385, 0.8128954855923851, 0.638252316892147, 0.56870683213075, 0.7501747709115347, 0.708457441329956, 0.8139503308534624, 0.7062787424524625, 0.8622417074441909, 0.8646308853228888, 0.7284014618098736, 0.8680456243753433, 0.7875373634497326, 0.7879856413602829, 0.7975892371237279, 0.6378016682863236, 0.7315840382973353, 0.6873063270052273, 0.6227741687695186, 0.8683873915672302, 0.8650748985608419, 0.8067503183285396, 0.7597239351669948, 0.6033092144727707, 0.912023960351944, 0.8300426633358002, 0.6716586491465568, 1.0267091368436811, 0.8030883005857469, 0.7655902264515559, 0.9995217279593149, 0.9054364308714866, 0.6794974193970362, 0.7675517458717028, 0.6716341409285863, 0.8109433646599452, 0.8396374380588532, 0.6021462034980456, 1.0586838573614756, 1.152756872077783, 1.0473177067041397, 0.904540524959564, 0.9870569452047346, 0.7685137606859207, 1.0014995441039403, 0.6385616730153562, 0.7575568881034852, 1.0312878530422847]]
Acc algo: [[tensor(0.0469), tensor(0.0312), tensor(0.0469), tensor(0.0469), tensor(0.0781), tensor(0.2188), tensor(0.2031), tensor(0.1562), tensor(0.2031), tensor(0.2188), tensor(0.3281), tensor(0.3750), tensor(0.3125), tensor(0.3594), tensor(0.2656), tensor(0.3281), tensor(0.3594), tensor(0.4375), tensor(0.3125), tensor(0.3281), tensor(0.3125), tensor(0.3281), tensor(0.3594), tensor(0.3125), tensor(0.3438), tensor(0.3594), tensor(0.3906), tensor(0.3750), tensor(0.3594), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4375), tensor(0.4062), tensor(0.3594), tensor(0.3281), tensor(0.3125), tensor(0.2969), tensor(0.3594), tensor(0.2969), tensor(0.3438), tensor(0.3750), tensor(0.3750), tensor(0.3438), tensor(0.4219), tensor(0.4062), tensor(0.3281), tensor(0.4062), tensor(0.4375), tensor(0.3594), tensor(0.4062), tensor(0.4219), tensor(0.4219), tensor(0.3594), tensor(0.4844), tensor(0.3906), tensor(0.4219), tensor(0.4375), tensor(0.4531), tensor(0.4531)], [tensor(0.0312), tensor(0.2969), tensor(0.2969), tensor(0.4375), tensor(0.4219), tensor(0.4844), tensor(0.4219), tensor(0.4844), tensor(0.4688), tensor(0.5625), tensor(0.5156), tensor(0.5000), tensor(0.5625), tensor(0.4531), tensor(0.4844), tensor(0.5312), tensor(0.4844), tensor(0.5000), tensor(0.5625), tensor(0.6562), tensor(0.5781), tensor(0.5625), tensor(0.5781), tensor(0.5000), tensor(0.6094), tensor(0.5000), tensor(0.5312), tensor(0.5781), tensor(0.5938), tensor(0.5781), tensor(0.5625), tensor(0.5156), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5000), tensor(0.5156), tensor(0.5000), tensor(0.5156), tensor(0.4375), tensor(0.4844), tensor(0.5469), tensor(0.5000), tensor(0.5625), tensor(0.5000), tensor(0.5625), tensor(0.5469), tensor(0.5156), tensor(0.5625), tensor(0.5781), tensor(0.5469), tensor(0.5312), tensor(0.5000), tensor(0.4844), tensor(0.5156), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5781), tensor(0.5000)], [tensor(0.), tensor(0.2656), tensor(0.3906), tensor(0.3750), tensor(0.3438), tensor(0.4375), tensor(0.4219), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.5156), tensor(0.5000), tensor(0.5000), tensor(0.6094), tensor(0.5312), tensor(0.5156), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.4219), tensor(0.4375), tensor(0.5312), tensor(0.5469), tensor(0.5781), tensor(0.5781), tensor(0.5312), tensor(0.5469), tensor(0.5938), tensor(0.5156), tensor(0.5625), tensor(0.6094), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.6250), tensor(0.5781), tensor(0.6094), tensor(0.6094), tensor(0.6250), tensor(0.6250), tensor(0.5938), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.6250), tensor(0.6250), tensor(0.6094), tensor(0.6250), tensor(0.6406), tensor(0.6094), tensor(0.6250), tensor(0.6250), tensor(0.6250), tensor(0.6406), tensor(0.6250), tensor(0.6094), tensor(0.6250), tensor(0.6406)]]
test_loss_algo: [[4.118812084197998, 4.0181193351745605, 4.233185768127441, 4.308969020843506, 2.848339319229126, 2.9560887813568115, 2.8202760219573975, 2.7594339847564697, 2.5386428833007812, 2.7850632667541504, 2.1391475200653076, 2.1330654621124268, 2.1699814796447754, 2.1954994201660156, 2.3598341941833496, 2.2084357738494873, 1.9982858896255493, 1.881711721420288, 2.0284082889556885, 2.086113929748535, 1.9952467679977417, 2.013275623321533, 2.039924144744873, 1.9905685186386108, 1.9607819318771362, 1.9304605722427368, 1.8907545804977417, 1.905604600906372, 1.9254438877105713, 1.8378469944000244, 1.7448961734771729, 1.8902183771133423, 1.9038125276565552, 1.8368901014328003, 2.0133230686187744, 1.9580761194229126, 2.0255606174468994, 2.2455620765686035, 1.992529273033142, 2.067779779434204, 2.2296383380889893, 1.9148863554000854, 1.9080477952957153, 2.064988851547241, 1.8238821029663086, 1.8227919340133667, 2.0844781398773193, 1.6862214803695679, 1.615509271621704, 1.8990929126739502, 1.799959659576416, 1.6355417966842651, 1.6239904165267944, 1.842177391052246, 1.8104115724563599, 1.6750702857971191, 1.7985087633132935, 2.0548911094665527, 1.747244119644165, 1.6933151483535767], [4.216553211212158, 2.505547523498535, 2.3063580989837646, 1.663774847984314, 2.285731554031372, 1.6032960414886475, 1.6643486022949219, 1.6021780967712402, 1.6050386428833008, 1.3358889818191528, 1.4201607704162598, 1.5614416599273682, 1.42142915725708, 1.5640935897827148, 1.5930761098861694, 1.2434393167495728, 1.7420283555984497, 1.335288643836975, 1.223555088043213, 1.0120519399642944, 1.0726066827774048, 1.3714057207107544, 1.2929526567459106, 1.3180313110351562, 1.1317076683044434, 1.2486921548843384, 1.7148289680480957, 1.0719447135925293, 1.0890345573425293, 1.2965894937515259, 1.2359040975570679, 1.3386236429214478, 1.2042241096496582, 1.6222710609436035, 1.3815627098083496, 1.7284588813781738, 1.5326173305511475, 1.4005753993988037, 1.481103539466858, 1.5312875509262085, 1.4280928373336792, 1.321146845817566, 1.3508862257003784, 1.5779162645339966, 1.5175392627716064, 1.210401177406311, 1.4026294946670532, 1.5492948293685913, 1.3904205560684204, 1.2058240175247192, 1.293372392654419, 1.3381688594818115, 1.4662933349609375, 1.367835283279419, 1.3373020887374878, 1.3219202756881714, 1.3318119049072266, 1.2580705881118774, 1.3003339767456055, 1.3820042610168457], [4.260313034057617, 2.72001314163208, 2.116960048675537, 1.8447505235671997, 2.2567601203918457, 1.8093249797821045, 1.690667986869812, 1.4670385122299194, 1.9343080520629883, 1.3954733610153198, 1.643137812614441, 1.540920615196228, 1.614438533782959, 1.022610068321228, 1.4226014614105225, 1.3600616455078125, 1.3313177824020386, 1.3164031505584717, 1.3889304399490356, 1.3131928443908691, 1.664311408996582, 1.6530088186264038, 1.3534023761749268, 1.136457920074463, 0.9975379705429077, 1.146742820739746, 1.1435139179229736, 1.247852087020874, 1.2189037799835205, 1.3322340250015259, 1.308293342590332, 1.2572864294052124, 1.0707292556762695, 1.0979719161987305, 1.230353593826294, 1.1603103876113892, 1.2517443895339966, 0.9960610866546631, 1.0981568098068237, 1.0950591564178467, 1.150471806526184, 1.3276207447052002, 1.4060776233673096, 1.2901973724365234, 1.3126628398895264, 1.263537883758545, 1.0646895170211792, 1.145982027053833, 1.1663696765899658, 1.2128616571426392, 1.050531268119812, 1.0982913970947266, 1.2055484056472778, 1.032879114151001, 1.105607271194458, 1.0287978649139404, 1.0689036846160889, 1.1463533639907837, 1.106298565864563, 1.0289620161056519]]
global_train_loss_algo: [[], [], []]
