import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1500 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2)],
    1: [0.08],
    2: [0.05],
    3: [0.06],
    4: [0.08]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1,
    4:1
    }

sch_freq = 300 #scheduler every n rounds



#select algos to run
d_algo = {
            #0: "MIFA",
            #1: "UMIFA",
            2: "FedAvg",
            #3: "scaffold",
            #4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[1.9640714383125306, 0.680402730628848, 0.773999246880412, 0.6209636471793055, 0.7467368490993976, 0.7372172778844834, 0.4126820923341438, 0.32056419554850435, 0.6210009936243296, 0.4501538183167577, 0.40730335706379267, 0.483380590416491, 0.40932555094361306, 0.4201593517139554, 0.3836111899837852, 0.3363941938336939, 0.3978586847567931, 0.2761264215921983, 0.4138776188110933, 0.32696876957488713, 0.3300137927546166, 0.22454408339690418, 0.2101007093256339, 0.16309382491152064, 0.26973124354961325, 0.35462424459168684, 0.19591843832167796, 0.20808604153629856, 0.3228879444417544, 0.22860862137924415, 0.2712482006521896, 0.1359894345665816, 0.2193032951746136, 0.16930341603700072, 0.19728356628998883, 0.18171533377608284, 0.19581655096706072, 0.0883669761987403, 0.16098651808570139, 0.0880951607802126, 0.09897903693250555, 0.15570559893181782, 0.1828217451320961, 0.11327635761583224, 0.10603841991789523, 0.1431460829335265, 0.16226210748776793, 0.16010796002607094, 0.14398354885826845, 0.11589196808286943, 0.14007341392134548, 0.09016871097424883, 0.1451354359329707, 0.125029642554, 0.11238885688828304, 0.08699371327878908, 0.10525572639890016, 0.09621542120756203, 0.08742990470258519, 0.0978150577489032, 0.11512319356406805, 0.19006140990820938, 0.09759939499283063, 0.09288819551002234, 0.07405698655755259, 0.1680037118576001, 0.10214414415298963, 0.09214974115475343, 0.054613656845758675, 0.07522459980631538, 0.07599184969541965, 0.07915404297003989, 0.10543786773225292, 0.07286653911345639, 0.16563736407493707]]
acc_algo = [[tensor(0.1000), tensor(0.1457), tensor(0.2038), tensor(0.2457), tensor(0.3167), tensor(0.3032), tensor(0.2556), tensor(0.3235), tensor(0.3853), tensor(0.4205), tensor(0.3870), tensor(0.4303), tensor(0.4252), tensor(0.4169), tensor(0.4443), tensor(0.4601), tensor(0.4580), tensor(0.3888), tensor(0.4281), tensor(0.4547), tensor(0.4700), tensor(0.4878), tensor(0.4752), tensor(0.3797), tensor(0.4997), tensor(0.5115), tensor(0.4903), tensor(0.5231), tensor(0.4936), tensor(0.4985), tensor(0.5248), tensor(0.4406), tensor(0.5048), tensor(0.4533), tensor(0.4822), tensor(0.5321), tensor(0.4806), tensor(0.4584), tensor(0.4943), tensor(0.5173), tensor(0.4891), tensor(0.4814), tensor(0.4816), tensor(0.5488), tensor(0.5087), tensor(0.5355), tensor(0.5075), tensor(0.5300), tensor(0.5364), tensor(0.5129), tensor(0.5219), tensor(0.5155), tensor(0.5381), tensor(0.5360), tensor(0.5096), tensor(0.4750), tensor(0.5104), tensor(0.5332), tensor(0.5381), tensor(0.5606), tensor(0.4860), tensor(0.5462), tensor(0.5400), tensor(0.3990), tensor(0.5587), tensor(0.5335), tensor(0.5457), tensor(0.5313), tensor(0.4944), tensor(0.5194), tensor(0.5108), tensor(0.5168), tensor(0.4541), tensor(0.5441), tensor(0.5383)]]
test_loss_algo= [[2.3036994858152546, 2.462399154711681, 2.3358380513586057, 2.147290469734532, 1.7854013541701492, 1.994867112226547, 2.218043415409744, 2.052630863371928, 1.7840739951771536, 1.5940180849877132, 1.869278373232313, 1.6627851800554116, 1.6402579621904214, 1.6836508808621935, 1.6248473642738002, 1.5842802889028174, 1.578063862338947, 1.8945192409928437, 1.6897680197551752, 1.6727928029503791, 1.81057230985848, 1.6284955785532667, 1.6373049810433844, 2.421933168058942, 1.563151328427017, 1.5182768725285865, 1.590082213377497, 1.484544980298182, 1.6144371313653934, 1.6063373339403966, 1.4741871095007393, 2.1662610374438533, 1.5988888763318396, 1.8618570482654937, 1.726913982895529, 1.5125512598426478, 1.7381783822539505, 1.931856036945513, 1.8091962451388122, 1.7126356203844593, 1.790204161291669, 1.7982822163089824, 1.9002187608913252, 1.585805951030391, 1.8159617033733684, 1.6225818904342166, 1.685066157845175, 1.5764278241783192, 1.6284851207854643, 1.856692446265251, 1.6838727802228017, 1.7885579743962379, 1.5896861769591168, 1.6481672700043697, 1.7261474238839118, 2.0100339499248823, 1.9108975267713997, 1.5878448714116575, 1.655483833543814, 1.5195003812480126, 1.9061987422833777, 1.5687389085247259, 1.6279676567976642, 3.0523542264464556, 1.5645364910174326, 1.6447332262233565, 1.7001143944491246, 1.7310039427629702, 2.062696833519419, 1.8273291595422538, 1.909828889901471, 1.871209389844518, 2.2918972794417365, 1.7857542994675364, 1.7179405795540779]]
global_train_loss_algo = [[]]
