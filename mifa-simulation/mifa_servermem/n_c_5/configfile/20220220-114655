import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1500 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'r' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.1],
    1: [0.1],
    2: [0.1], #0.05
    3: [np.power(10,-1.5)],
    4: [np.power(10,-1.5)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1,
    4:1
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            # 0: "MIFA",
            # 1: "UMIFA",
            # 2: "FedAvg",
            #3: "scaffold",
            4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[3.0265829277038576, 0.8791366499662399, 0.6720482168346644, 0.5442058536782861, 0.29832576055079696, 0.36595897404506106, 0.2984969875193201, 0.1969567556626862, 0.31829107537167145, 0.20628259798279033, 0.30725293067749593, 0.15287039058981464, 0.252641812292859, 0.15521285438211635, 0.14159753331914543, 0.13152825622120873, 0.10349914711987367, 0.19939967829937816, 0.07905597411045165, 0.07671919683925807, 0.0672183654428227, 0.06028015511925332, 0.05976013641688042, 0.07145486797526246, 0.10159740448885715, 0.047642230654601006, 0.2920550917345099, 0.07855805480794516, 0.07815915300132473, 0.046766082845279014]]
acc_algo = [[tensor(0.0068), tensor(0.2678), tensor(0.3607), tensor(0.3887), tensor(0.4101), tensor(0.4431), tensor(0.4633), tensor(0.4351), tensor(0.4931), tensor(0.4713), tensor(0.5213), tensor(0.5225), tensor(0.5445), tensor(0.5253), tensor(0.5566), tensor(0.5540), tensor(0.5362), tensor(0.5646), tensor(0.5624), tensor(0.6063), tensor(0.5961), tensor(0.5806), tensor(0.5911), tensor(0.5989), tensor(0.6057), tensor(0.6083), tensor(0.5787), tensor(0.6146), tensor(0.6350), tensor(0.5996)]]
test_loss_algo= [[5.234741326350315, 1.9654691264887525, 1.752428192241936, 1.6987456842592568, 1.6044304135498728, 1.5284980953119363, 1.5043236252608572, 1.638447436557454, 1.4497048847234932, 1.4816618945188582, 1.3416567046171541, 1.3533310567497447, 1.2948516204858282, 1.4614316536362764, 1.2720414916421199, 1.300560352149283, 1.3469853534060678, 1.2807290466727725, 1.3251320727311882, 1.1916109965105726, 1.186504843508362, 1.210976446889768, 1.237047046612782, 1.1831408587230998, 1.1627708214103796, 1.2006616136830324, 1.2673488392192087, 1.1670275019232634, 1.1200894447648602, 1.2315903340175653]]
global_train_loss_algo = [[]]
