import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.01],#[1/np.power(10,2.5)],
    1: [0.01],#[1/np.power(10,2)],
    2: [0.01]#[1/np.power(10,2)]
    }

lrfactor = {
    0:1
    , #factor to reduce lr in scheduler
    1:1,
    2:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[4.098189023971558, 3.90087663269043, 3.401219733556112, 3.6729097746213277, 3.5884490044911708, 3.632737769285838, 3.3311968231201172, 3.596407061894735, 3.4906061437924705, 3.2092684001922605, 3.0951157579421995, 2.953391576925913, 2.879356326262156, 2.6846980516115826, 2.5873196585973104, 2.738514709472656, 2.492422429402669, 2.4895550219217935, 2.2214550919532776, 2.4890698154767352, 2.299916755358378, 2.26111146068573, 2.108628775278727, 2.1376466584205622, 1.8998480180899304, 1.7632270956039426, 1.7817699750264488, 1.9616285228729247, 2.0769852312405903, 1.6385240138371784, 1.771761919975281, 1.8968135671615598, 1.7673735419909158, 1.5270342760086062, 1.5634680024782814, 1.5675191208521526, 1.5287367796897888, 1.4846183423201242, 1.4123642500241598, 1.5635841880639396, 1.4423692456881205, 1.1905431983868282, 1.4000462369124096, 1.1329469684759776, 1.3669341786702474, 1.235710293451945, 1.073779729406039, 1.3020492629210154, 1.118672270178795, 1.152755182584127, 1.4493447049458823, 1.1130492041110993, 1.1198569309711455, 1.3562171396017075, 1.0969470822811125, 1.1392127283414204, 1.4274552917480468, 1.1920725836356483, 1.0940739303827285, 1.1494068385958673], [4.097101443926493, 2.597182343959808, 2.251047444343567, 1.982404509067535, 1.7316450683275857, 1.6632566622893017, 1.7934689251581826, 1.5850082502365113, 1.7551581825812659, 1.3013497082392376, 1.1562904645601908, 1.0665317124525706, 1.0352654800812404, 1.4204641405741374, 1.0625003169576326, 1.161911068201065, 0.981865839878718, 1.068262066602707, 0.8227656990687053, 1.2811475336551665, 1.008411966164907, 1.111676423648993, 0.835303908586502, 0.8729030750195184, 0.7046121937905749, 0.7762230771780014, 0.7673376711209615, 0.8687660111188888, 0.9870899496475856, 0.6510584956407547, 0.8047001556952793, 0.9957020318905514, 0.7894141791264216, 0.6323648893634478, 0.735488462527593, 0.7522244702577591, 0.705911985039711, 0.6905146573235591, 0.720757673184077, 0.7905435061256091, 0.6618340794841447, 0.5637924714634817, 0.7245182737211386, 0.5474331304679314, 0.7471196929613749, 0.7200018705924351, 0.5300301438570022, 0.6912573178907235, 0.6320558570822079, 0.6592979149023692, 0.8752879194418588, 0.5406362636784714, 0.5916939419507982, 0.7804826619625091, 0.5984802004198233, 0.6520445758104325, 0.9463910710811614, 0.7482569621006648, 0.6394700838128726, 0.5851533870932956], [4.097356193542481, 2.6085879831314083, 2.2915415684382117, 2.0014538048108417, 1.7684005053838097, 1.669540616671244, 1.827414371172587, 1.7003749790191651, 1.7357873737017315, 1.289793221473694, 1.1443465607166288, 1.1293586800893147, 1.0502979856729506, 1.3592690813541413, 1.1277395165761313, 1.1308507726987203, 1.0153208469549815, 1.0857717164357503, 0.7876518957813581, 1.307097502152125, 0.9952560482422511, 1.0380404074192047, 0.8852025135358176, 0.8922471894820531, 0.7095887721578281, 0.771122249563535, 0.7966675845781962, 0.8372718087037404, 1.053851157426834, 0.6608670569260915, 0.8095139249960581, 0.9937671896616619, 0.8042703354358673, 0.6829384827216467, 0.6727635395129521, 0.7209802499612172, 0.7770230142275492, 0.6686985755711794, 0.7254543650150299, 0.794220189611117, 0.6963009980519613, 0.5963757332662741, 0.7430628386139869, 0.5995444170633952, 0.7469835925102233, 0.7329712369044622, 0.5204416607817014, 0.6937862786948681, 0.645175057053566, 0.6509186447660129, 0.8749194162090619, 0.5624969324568907, 0.5871451306343077, 0.8151251483758291, 0.6300891035795212, 0.6693994789918264, 0.9861907880306242, 0.7461684035261472, 0.6648210956652959, 0.589039620360049]]
Acc algo: [[tensor(0.), tensor(0.0156), tensor(0.0156), tensor(0.0312), tensor(0.0469), tensor(0.0156), tensor(0.0781), tensor(0.0312), tensor(0.0625), tensor(0.0312), tensor(0.0312), tensor(0.0312), tensor(0.1250), tensor(0.0312), tensor(0.0312), tensor(0.2500), tensor(0.1094), tensor(0.0469), tensor(0.0938), tensor(0.2812), tensor(0.1250), tensor(0.1094), tensor(0.2188), tensor(0.2500), tensor(0.2031), tensor(0.2500), tensor(0.1875), tensor(0.2344), tensor(0.2812), tensor(0.2656), tensor(0.2812), tensor(0.2188), tensor(0.2812), tensor(0.3125), tensor(0.3438), tensor(0.3750), tensor(0.3281), tensor(0.3594), tensor(0.4219), tensor(0.4375), tensor(0.2969), tensor(0.3906), tensor(0.4375), tensor(0.4219), tensor(0.3750), tensor(0.3750), tensor(0.4688), tensor(0.4375), tensor(0.4219), tensor(0.3438), tensor(0.3438), tensor(0.4219), tensor(0.3750), tensor(0.4844), tensor(0.4062), tensor(0.4219), tensor(0.4375), tensor(0.4375), tensor(0.4844), tensor(0.4219)], [tensor(0.), tensor(0.0312), tensor(0.0312), tensor(0.1562), tensor(0.1719), tensor(0.2656), tensor(0.3594), tensor(0.3281), tensor(0.2812), tensor(0.3906), tensor(0.3906), tensor(0.3594), tensor(0.4844), tensor(0.3281), tensor(0.2969), tensor(0.4062), tensor(0.4219), tensor(0.4531), tensor(0.4531), tensor(0.4375), tensor(0.4688), tensor(0.4844), tensor(0.4688), tensor(0.4844), tensor(0.4844), tensor(0.4219), tensor(0.5156), tensor(0.5312), tensor(0.5156), tensor(0.5156), tensor(0.5000), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5781), tensor(0.5469), tensor(0.5625), tensor(0.5469), tensor(0.6250), tensor(0.5469), tensor(0.5781), tensor(0.5781), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5312), tensor(0.5156), tensor(0.5156), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5469)], [tensor(0.0156), tensor(0.0312), tensor(0.0312), tensor(0.1875), tensor(0.1875), tensor(0.2500), tensor(0.3281), tensor(0.2812), tensor(0.2969), tensor(0.3594), tensor(0.3438), tensor(0.3125), tensor(0.3594), tensor(0.4062), tensor(0.3750), tensor(0.4688), tensor(0.4844), tensor(0.4688), tensor(0.4375), tensor(0.3281), tensor(0.4062), tensor(0.3750), tensor(0.4219), tensor(0.3750), tensor(0.5000), tensor(0.4375), tensor(0.4531), tensor(0.5156), tensor(0.5469), tensor(0.4219), tensor(0.5312), tensor(0.5000), tensor(0.5625), tensor(0.5156), tensor(0.5156), tensor(0.5000), tensor(0.4531), tensor(0.5781), tensor(0.5000), tensor(0.5156), tensor(0.5312), tensor(0.4375), tensor(0.5625), tensor(0.5469), tensor(0.4219), tensor(0.4375), tensor(0.4375), tensor(0.5000), tensor(0.4219), tensor(0.5312), tensor(0.4844), tensor(0.5469), tensor(0.5156), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5156), tensor(0.4844), tensor(0.5625), tensor(0.5312)]]
test_loss_algo: [[4.148871898651123, 4.128729343414307, 4.355038642883301, 4.821181774139404, 3.571906328201294, 3.8966681957244873, 4.583806037902832, 4.576213359832764, 3.6944127082824707, 3.473723888397217, 4.577867031097412, 4.457468032836914, 3.03898549079895, 3.2834784984588623, 3.6087605953216553, 2.6582512855529785, 2.812011957168579, 3.162632465362549, 2.774010181427002, 2.4103808403015137, 2.846709728240967, 2.869671583175659, 2.589707612991333, 2.776428699493408, 2.7998201847076416, 2.5205681324005127, 2.6768624782562256, 2.7476344108581543, 2.3086886405944824, 2.219245195388794, 2.5299456119537354, 2.3408679962158203, 2.0937983989715576, 2.110731363296509, 2.0555241107940674, 1.8931336402893066, 2.035351276397705, 2.1669023036956787, 1.8850703239440918, 1.930416226387024, 2.152498960494995, 1.7891299724578857, 1.6917067766189575, 1.9320404529571533, 1.9170467853546143, 1.639695405960083, 1.7587162256240845, 1.951599359512329, 1.6264911890029907, 1.6438465118408203, 1.9779945611953735, 1.7805413007736206, 1.5824706554412842, 1.642331600189209, 1.8105738162994385, 1.5998821258544922, 1.5725373029708862, 1.731490135192871, 1.6286189556121826, 1.6037936210632324], [4.1269378662109375, 4.279294013977051, 3.8679068088531494, 2.5327258110046387, 2.651127576828003, 2.419877529144287, 2.3666915893554688, 2.4736311435699463, 2.2298738956451416, 1.936132788658142, 1.8838578462600708, 2.0998787879943848, 1.653038501739502, 2.2597031593322754, 2.055657386779785, 1.8092013597488403, 1.8542149066925049, 1.9099233150482178, 1.6884504556655884, 1.6567074060440063, 1.8349823951721191, 1.523802638053894, 1.5659980773925781, 1.6014429330825806, 1.4907655715942383, 1.7237331867218018, 1.3672832250595093, 1.5542277097702026, 1.5248223543167114, 1.5951725244522095, 1.3572248220443726, 1.3912591934204102, 1.3323091268539429, 1.3716228008270264, 1.5219695568084717, 1.342667818069458, 1.4184859991073608, 1.4516873359680176, 1.3947850465774536, 1.4237228631973267, 1.2365689277648926, 1.3930609226226807, 1.4030425548553467, 1.1729506254196167, 1.4062774181365967, 1.4583523273468018, 1.3179433345794678, 1.2521438598632812, 1.431050419807434, 1.4133155345916748, 1.2844305038452148, 1.3465391397476196, 1.1834943294525146, 1.3512762784957886, 1.421726107597351, 1.372846245765686, 1.4103370904922485, 1.2359821796417236, 1.179273247718811, 1.3288885354995728], [4.126661777496338, 4.220948219299316, 3.9798355102539062, 2.497767925262451, 2.799151659011841, 2.357403039932251, 2.328857898712158, 2.1738431453704834, 2.3187668323516846, 1.9421573877334595, 1.817355751991272, 2.3299479484558105, 1.7272497415542603, 1.974181056022644, 1.8630226850509644, 1.568798542022705, 1.7686454057693481, 1.7858498096466064, 1.7685248851776123, 2.0197906494140625, 1.9271433353424072, 1.7976562976837158, 1.8469159603118896, 2.0118191242218018, 1.6121726036071777, 1.8231146335601807, 1.5609008073806763, 1.526996374130249, 1.2391343116760254, 1.6687653064727783, 1.5137423276901245, 1.3870983123779297, 1.272764801979065, 1.4462676048278809, 1.4040437936782837, 1.459011435508728, 1.6717755794525146, 1.4300583600997925, 1.5552705526351929, 1.3684340715408325, 1.351913332939148, 1.5524424314498901, 1.163238763809204, 1.160416841506958, 1.879146695137024, 1.4659488201141357, 1.7756668329238892, 1.3832206726074219, 1.7451837062835693, 1.404914140701294, 1.4147850275039673, 1.3291668891906738, 1.3902720212936401, 1.3205842971801758, 1.4045500755310059, 1.5003547668457031, 1.7276134490966797, 1.5618005990982056, 1.301434874534607, 1.4920332431793213]]
global_train_loss_algo: [[], [], []]
