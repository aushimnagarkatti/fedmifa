import lenet
import numpy as np

#Generic Hyp
batch_size= 50 #Neural Network batch size
n_rnds= 1500 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2)],
    1: [1/np.power(10,1)],
    2: [1/np.power(10,2)],
    3: [1/np.power(10,1)],
    4: [0.03]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1,
    4:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            # 0: "MIFA",
            # 1: "UMIFA",
            # 2: "FedAvg",
            # 3: "scaffold",
            4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[2.0753591775894167, 0.7589899871731177, 1.0402681905031206, 0.9222459825873376, 0.9566872146725653, 0.9579251331090928, 0.6360367475543172, 0.5640916131739504, 0.913027805685997, 0.9119010007381438, 0.8727645051479339, 0.9350160065293313, 0.8442443001270293, 0.8179424807429314, 0.8589117333292962, 0.8393347454071044, 0.9254126536846161, 0.6562511708168313, 0.7444205516693183, 0.733052946056705, 0.7304196809232235, 0.7271648259460927, 0.7867840087413789, 0.5617235286021606, 0.7541499441862106, 0.8552021735906601, 0.6448496685747522, 0.6629483632731717, 0.7833553948998452, 0.5700084294669795, 0.8403632193803787, 0.7372545774281025, 0.8666575169563293, 0.8278163969516754, 0.7138845756405499, 0.7893504333496094, 0.6809683682641481, 0.5163891245040577, 0.7850220011174678, 0.6537717809790047, 0.5979285576927941, 0.5441319383913651, 0.6403424274176359, 0.5624765303730964, 0.6626671297138091, 0.5556109887408093, 0.6416007959842682, 0.7269676408846862, 0.7353459196304902, 0.6979898731410504, 0.6833351110597141, 0.45625288024544713, 0.6049694919737523, 0.7868827185034751, 0.6826620401442052, 0.7157721804082394, 0.7661556224524976, 0.4086071952950442, 0.4913274202495813, 0.4909080591052771, 0.580072315755533, 0.5633665110426955, 0.625683793759672, 0.6417860101163387, 0.6799218574166297, 0.8123082704842091, 0.6852239010483026, 0.4741632711864077, 0.6149626824259758, 0.4378469918877818, 0.7042879982292651, 0.6537494865059853, 0.6969463442265986, 0.6245769491046668, 0.6804056211560965]]
acc_algo = [[tensor(0.0600), tensor(0.0800), tensor(0.1800), tensor(0.1400), tensor(0.2200), tensor(0.2000), tensor(0.2400), tensor(0.2400), tensor(0.2000), tensor(0.3400), tensor(0.3600), tensor(0.3200), tensor(0.3800), tensor(0.3600), tensor(0.3400), tensor(0.3000), tensor(0.4200), tensor(0.4000), tensor(0.3600), tensor(0.4200), tensor(0.3400), tensor(0.4000), tensor(0.3800), tensor(0.3600), tensor(0.3800), tensor(0.3600), tensor(0.3800), tensor(0.3600), tensor(0.4000), tensor(0.3800), tensor(0.4000), tensor(0.3400), tensor(0.3600), tensor(0.4000), tensor(0.3400), tensor(0.4000), tensor(0.3600), tensor(0.3400), tensor(0.3800), tensor(0.4200), tensor(0.4200), tensor(0.3800), tensor(0.4400), tensor(0.3800), tensor(0.4200), tensor(0.4000), tensor(0.4800), tensor(0.5000), tensor(0.4200), tensor(0.4600), tensor(0.4600), tensor(0.3200), tensor(0.3600), tensor(0.4200), tensor(0.3400), tensor(0.2800), tensor(0.3200), tensor(0.3600), tensor(0.4400), tensor(0.4000), tensor(0.4800), tensor(0.4400), tensor(0.4000), tensor(0.4800), tensor(0.4600), tensor(0.4400), tensor(0.4000), tensor(0.4200), tensor(0.4000), tensor(0.4000), tensor(0.4400), tensor(0.3600), tensor(0.4400), tensor(0.4400), tensor(0.3400)]]
test_loss_algo= [[2.311579942703247, 2.262361764907837, 2.2682101726531982, 2.2542219161987305, 2.208927631378174, 2.1884281635284424, 2.1388628482818604, 2.0917224884033203, 2.093792200088501, 2.014418601989746, 2.0166704654693604, 2.026348829269409, 1.992551326751709, 2.0187935829162598, 1.948392629623413, 1.9411262273788452, 1.9128952026367188, 1.8729215860366821, 1.9278558492660522, 1.8618125915527344, 1.8871939182281494, 1.8644070625305176, 1.8725758790969849, 1.8594213724136353, 1.828841209411621, 1.8255107402801514, 1.8058221340179443, 1.7593660354614258, 1.8007197380065918, 1.8038935661315918, 1.7790378332138062, 1.8040293455123901, 1.7957239151000977, 1.7261855602264404, 1.7130954265594482, 1.7419363260269165, 1.7099602222442627, 1.739326000213623, 1.7139172554016113, 1.6867280006408691, 1.6669328212738037, 1.6461083889007568, 1.6609539985656738, 1.6788008213043213, 1.6698766946792603, 1.6451072692871094, 1.648871898651123, 1.675848364830017, 1.7006220817565918, 1.6614645719528198, 1.6974238157272339, 1.7108769416809082, 1.664739727973938, 1.7114214897155762, 1.666855812072754, 1.7043744325637817, 1.686099886894226, 1.6199250221252441, 1.6346008777618408, 1.6218119859695435, 1.6085549592971802, 1.614007830619812, 1.6180484294891357, 1.6416608095169067, 1.6615769863128662, 1.6417698860168457, 1.6380131244659424, 1.644738793373108, 1.6180366277694702, 1.6113693714141846, 1.5712300539016724, 1.6051124334335327, 1.5439066886901855, 1.5168665647506714, 1.630672812461853]]
global_train_loss_algo = [[]]
