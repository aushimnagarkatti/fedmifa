import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = False


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 0
clust_mech = 'dynamic' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,2.5)],
    2: [1/np.power(10,2.5)],
    3: [1/np.power(10,2.5)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.2776905488967896, 2.151037814617157, 1.5441799694299698, 1.108416228443384, 0.6391522199101746, 0.9460583856701851, 0.5339855707529931, 0.6452025102078915, 0.5193741677701473, 0.6661998126842082, 0.46771852992475027, 0.5063919355371036, 0.6220625530555844, 0.46607365871546796, 0.5667416952550411, 0.526697499603033, 0.5786104419268667, 0.454532706476748, 0.40383472946006804, 0.5732190573215485], [2.2773262333869937, 0.8825197345018386, 0.8115610840916634, 0.8219859050214291, 0.564514126908034, 0.8406421238183975, 0.4169624699000269, 0.5564682021923364, 0.4707557246275246, 0.5752720899693667, 0.4382766026630998, 0.4704567537736148, 0.5739186936616899, 0.4055718035623431, 0.48037158142775305, 0.4960470049455762, 0.5093263225082774, 0.3714398491382599, 0.4224215836892837, 0.5380937781557441], [2.279267888069153, 0.9348845300078393, 0.8004865631461143, 0.84177417203784, 0.584942652732134, 0.8340622213482858, 0.4492829124024137, 0.5757412784546613, 0.4923872242681682, 0.6511296932585537, 0.4053698295727372, 0.502447933270596, 0.5701790818385779, 0.43191952645313, 0.5163471577316522, 0.5004157337546349, 0.5121468972484581, 0.4085528441797942, 0.4495507388561964, 0.5690146264806389], [2.2772945737838746, 1.1361422324180606, 0.954124225974083, 1.1565609896183013, 0.7889773119986057, 1.1829323291778564, 0.702399925403297, 0.7803769324719905, 0.8100943545252084, 0.8411449525505305, 0.6499639235436916, 0.6779372723400592, 0.8321163429319858, 0.5763290085294284, 0.7092786528170107, 0.8200825890898704, 0.7890632331185043, 0.5561778124049306, 0.631216002041474, 0.7873527826368809]]
Acc algo: [[tensor(0.0156), tensor(0.1094), tensor(0.1406), tensor(0.1406), tensor(0.2188), tensor(0.3281), tensor(0.3438), tensor(0.3438), tensor(0.3438), tensor(0.4062), tensor(0.4062), tensor(0.4219), tensor(0.4375), tensor(0.4375), tensor(0.4531), tensor(0.5000), tensor(0.4688), tensor(0.5000), tensor(0.5000), tensor(0.5000)], [tensor(0.0312), tensor(0.1406), tensor(0.3125), tensor(0.3438), tensor(0.3125), tensor(0.2344), tensor(0.3125), tensor(0.3281), tensor(0.3906), tensor(0.4219), tensor(0.4688), tensor(0.5000), tensor(0.5000), tensor(0.5156), tensor(0.4844), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5938)], [tensor(0.0312), tensor(0.1406), tensor(0.1094), tensor(0.2500), tensor(0.2812), tensor(0.2969), tensor(0.2969), tensor(0.3438), tensor(0.3906), tensor(0.2656), tensor(0.3125), tensor(0.3281), tensor(0.2656), tensor(0.3125), tensor(0.3594), tensor(0.2656), tensor(0.4844), tensor(0.2188), tensor(0.5312), tensor(0.3750)], [tensor(0.0312), tensor(0.1719), tensor(0.2188), tensor(0.2031), tensor(0.3438), tensor(0.2031), tensor(0.2969), tensor(0.3750), tensor(0.4219), tensor(0.2656), tensor(0.3281), tensor(0.3594), tensor(0.4219), tensor(0.4062), tensor(0.3594), tensor(0.2969), tensor(0.4375), tensor(0.2188), tensor(0.4219), tensor(0.4688)]]
test_loss_algo: [[2.31398606300354, 2.2723660469055176, 2.5527775287628174, 2.467115640640259, 2.074566602706909, 1.9085272550582886, 1.8447703123092651, 1.789408802986145, 1.7631844282150269, 1.660874366760254, 1.653732419013977, 1.6575345993041992, 1.6527440547943115, 1.5917826890945435, 1.560367465019226, 1.5462605953216553, 1.5268199443817139, 1.4707316160202026, 1.4589736461639404, 1.4453835487365723], [2.3124732971191406, 2.450360059738159, 1.9984573125839233, 1.8608016967773438, 1.8782597780227661, 1.9451541900634766, 1.74588942527771, 1.724766492843628, 1.6799310445785522, 1.6206761598587036, 1.5334522724151611, 1.4851502180099487, 1.4579380750656128, 1.5099362134933472, 1.5376992225646973, 1.4442033767700195, 1.4235252141952515, 1.3842641115188599, 1.3942474126815796, 1.3197219371795654], [2.312506675720215, 2.9166817665100098, 2.6759414672851562, 2.176687002182007, 2.00750732421875, 2.193911075592041, 1.9619619846343994, 1.9897446632385254, 1.8103785514831543, 2.0945048332214355, 1.9095990657806396, 1.8612147569656372, 2.3317770957946777, 1.9519072771072388, 1.8116103410720825, 2.083439826965332, 1.6217364072799683, 2.2809221744537354, 1.4209721088409424, 1.885214924812317], [2.3123815059661865, 2.6091489791870117, 2.2582836151123047, 2.1206212043762207, 1.8828123807907104, 2.032865285873413, 1.7974598407745361, 1.7826786041259766, 1.69976806640625, 1.8887654542922974, 1.753565788269043, 1.7284939289093018, 1.7308889627456665, 1.6811617612838745, 1.7593214511871338, 2.1011106967926025, 1.5845515727996826, 2.2243993282318115, 1.6285624504089355, 1.5811783075332642]]
global_train_loss_algo: [[2.2994669255088356, 2.32390569756403, 2.6184100433993525, 2.5447193661614147, 2.091920197772248, 1.9537333254619023, 1.9504285642253163, 1.8853928408659328, 1.794285808835188, 1.807868296380543, 1.7256043089930053, 1.6698828677996955, 1.5954287300634262, 1.5606245509803753, 1.5826534224898003, 1.5421361804313367, 1.50892113434994, 1.4984575749358253, 1.5151501262889189, 1.4466461360911884], [2.301554388707251, 2.494028264909144, 2.3682957003488565, 2.0855890106971917, 1.8846180643267034, 1.8406994002859305, 1.8215840724118226, 1.7522174263244394, 1.6615664003145358, 1.6413325139933534, 1.6465270026870396, 1.6961322899364755, 1.5772958806408641, 1.68320556781481, 1.5766226419097626, 1.5084544372985431, 1.4865576432793952, 1.513579546338152, 1.5669697369151103, 1.4561134715519293], [2.301403294743784, 2.749148589570809, 2.9592521849190794, 2.1947578383833553, 2.2740620283214636, 2.113797090547469, 2.1667712759178923, 2.4550326910165263, 2.209862715752838, 1.8681572541556395, 2.2037034043875496, 1.728298297013773, 2.4395946578296552, 1.9436836297554738, 1.8911024242105996, 2.2912202372270474, 1.791345476799304, 2.508676290512085, 1.980847121657008, 1.5597440960157254], [2.301547956283745, 2.483200093974238, 2.5215908398713602, 2.0808377692766507, 2.119555706563203, 1.956312470728784, 1.993810300327018, 2.1094350805672843, 1.8952950001372706, 1.8182919386707608, 1.972395435471059, 1.7991172008197327, 1.9640152757734899, 1.768891033004312, 1.731424619932004, 2.062389458536797, 1.6511439159703072, 2.370127222727022, 1.9981605931925956, 1.616811102461022]]
