import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1500 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2)],
    1: [1/np.power(10,1.5)],
    2: [1/np.power(10,2)],
    3: [1/np.power(10,1.5)],
    4: [1/np.power(10,1)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1,
    4:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            # 0: "MIFA",
            # 1: "UMIFA",
            # 2: "FedAvg",
            # 3: "scaffold",
            4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[1.6597832781076431, 1.2070154338030261, 0.8551022926177485, 1.1705088257430178, 1.0359438905119895, 1.0331268182100029, 1.012584529405358, 1.0107057010212155, 1.0353652266965945, 1.1842552345991133, 1.0075306726305278, 0.9687375046988018, 0.9665487840026618, 0.9245616969862022, 0.7792240602715175, 0.5884527698886813, 0.9177243357896805, 0.9080672791600228, 0.9554370760917663, 0.787792885769693, 0.8435919058241416, 0.6037788206519326, 1.0296737304329873, 1.1107721769809724, 0.8791510085761548, 0.8396404120879015, 0.7316581743280404, 0.4965406002059171, 0.9703787830471992, 0.8150067417323591, 0.8173842122871429, 0.47875212080301993, 0.9484386098384858, 0.7893237054347992, 0.9612745034694672, 0.8169268965721128, 0.7891426280071028, 0.7902681675553322, 0.762486616075039, 0.6063100673956796, 0.8069865990430116, 0.8090527267928701, 0.9061690199375153, 0.6009628231188981, 0.7860053895413875, 0.5247431477803911, 0.9422483894228935, 0.8907668092846869, 0.7427782303176356, 0.7624483656795928, 0.9132213210128247, 0.7057562840264291, 0.5956895296438597, 0.6499820512591395, 0.8387898742407561, 0.6804559355974197, 0.6153138917637988, 0.6855812685285492, 0.7326091650640592, 0.6053057370241731, 0.7092067488143221, 1.021477554515004, 0.6467151247832226, 0.7443782655708491, 0.7523920806450769, 0.864720950424671, 0.6511289539234711, 0.6999169914843515, 0.6936539122462273, 0.7638727958733216, 0.7037225125420081, 0.6860926795750857, 0.8621501621603966, 0.7284557969868184, 0.7938013076782227]]
acc_algo = [[tensor(0.1094), tensor(0.0938), tensor(0.1094), tensor(0.1094), tensor(0.1406), tensor(0.2031), tensor(0.1875), tensor(0.0625), tensor(0.1406), tensor(0.0781), tensor(0.1250), tensor(0.1250), tensor(0.0312), tensor(0.1094), tensor(0.2188), tensor(0.0781), tensor(0.1094), tensor(0.1719), tensor(0.2031), tensor(0.2031), tensor(0.1250), tensor(0.2031), tensor(0.2969), tensor(0.1406), tensor(0.2031), tensor(0.1406), tensor(0.2812), tensor(0.2188), tensor(0.2656), tensor(0.0938), tensor(0.2344), tensor(0.1406), tensor(0.2031), tensor(0.1250), tensor(0.1406), tensor(0.1875), tensor(0.2031), tensor(0.2031), tensor(0.2344), tensor(0.0938), tensor(0.2500), tensor(0.1406), tensor(0.2031), tensor(0.2500), tensor(0.2812), tensor(0.3125), tensor(0.2031), tensor(0.3281), tensor(0.2344), tensor(0.3125), tensor(0.2656), tensor(0.2812), tensor(0.2188), tensor(0.2969), tensor(0.2969), tensor(0.2812), tensor(0.2344), tensor(0.2656), tensor(0.2500), tensor(0.1094), tensor(0.1562), tensor(0.2500), tensor(0.0938), tensor(0.1562), tensor(0.1719), tensor(0.2031), tensor(0.1875), tensor(0.1719), tensor(0.1562), tensor(0.1719), tensor(0.2188), tensor(0.1719), tensor(0.1875), tensor(0.1875), tensor(0.1562)]]
test_loss_algo= [[2.2994182109832764, 2.3355906009674072, 2.268319845199585, 2.313131809234619, 2.2766671180725098, 2.2524099349975586, 2.3588225841522217, 3.0405540466308594, 2.2920515537261963, 2.294884443283081, 2.297884702682495, 2.2738192081451416, 2.285992383956909, 2.2744102478027344, 2.2927188873291016, 2.331160068511963, 2.3032002449035645, 2.2590599060058594, 2.2229700088500977, 2.1973726749420166, 2.232835292816162, 2.162252426147461, 2.1491971015930176, 2.235374927520752, 2.2237586975097656, 2.2101519107818604, 2.1611011028289795, 2.2313082218170166, 2.1884987354278564, 2.747723340988159, 2.3298933506011963, 2.478564739227295, 2.190531015396118, 2.3669562339782715, 2.224191427230835, 2.3454811573028564, 2.1139304637908936, 2.1186580657958984, 2.111825942993164, 2.2800097465515137, 2.08565354347229, 2.162665843963623, 2.181239128112793, 2.182676315307617, 2.1048247814178467, 2.0780928134918213, 2.1439967155456543, 2.0470855236053467, 2.1165082454681396, 2.109623908996582, 2.1296496391296387, 2.108319044113159, 2.1389317512512207, 2.126453399658203, 2.078676223754883, 2.0562057495117188, 2.0859954357147217, 2.074308156967163, 2.0877974033355713, 2.177673578262329, 2.201709032058716, 2.0727603435516357, 2.1775929927825928, 2.0922787189483643, 2.159266710281372, 4.151809215545654, 2.182006597518921, 2.140040397644043, 2.114185333251953, 2.1526269912719727, 2.179723024368286, 2.1948859691619873, 2.1805496215820312, 2.1380767822265625, 2.178586483001709]]
global_train_loss_algo = [[]]
