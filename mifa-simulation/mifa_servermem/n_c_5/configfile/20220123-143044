import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist'#'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.3],#[1/np.power(10,2.5)],
    1: [0.1],#[1/np.power(10,2)],
    2: [0.1]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.8
    , #factor to reduce lr in scheduler
    1:0.8,
    2:0.8
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
           # 1: "UMIFA",
           # 2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.968330276920682, 3.7616438363393145, 3.101889042854309, 2.93736098752703, 2.444084745407104, 2.0921877597173055, 1.9922916340827943, 1.8541325395107269, 1.6299521903196972, 1.774064046939214, 1.3779410549004871, 1.5768967763582864, 1.661471334298452, 1.4028176124890646, 1.506925291697184, 1.1717444960276286, 1.2857484709421794, 1.4334634786844256, 1.3685259022712706, 1.3733375885089238, 1.2085887054602305, 1.6176088080406188, 1.3123341263135275, 1.4619246733188629, 1.3196969042221707, 1.0979437482357024, 1.4412102154890696, 1.608639584064484, 1.477023865063985, 1.4288836580912272, 1.2650414994160335, 1.3790850852330527, 1.3942605593204498, 1.5313041982650755, 1.5068333082199097, 1.5032134183247887, 1.1201812148094177, 1.3550086263815562, 1.2942426070372264, 1.2055806307991346, 1.495677745819092, 1.5448362055619558, 1.386631738980611, 1.1556151775519052, 1.5588584440350535, 1.4671502315430414, 1.2950720088084537, 1.4226915747324624, 1.8302334515253702, 1.5304578336079913, 1.4839613976875938, 1.2077819327513377, 1.5683776259422302, 1.2092451783021292, 1.523536167939504, 1.331419133345286, 1.521420001665751, 1.3837112447420754, 1.5217023366292317, 1.4587629635334014]]
Acc algo: [[tensor(0.), tensor(0.0469), tensor(0.0469), tensor(0.0312), tensor(0.1406), tensor(0.1250), tensor(0.2344), tensor(0.2031), tensor(0.2500), tensor(0.2031), tensor(0.2969), tensor(0.2812), tensor(0.3438), tensor(0.3281), tensor(0.2969), tensor(0.3594), tensor(0.3594), tensor(0.4062), tensor(0.3906), tensor(0.3750), tensor(0.3438), tensor(0.4219), tensor(0.2969), tensor(0.4219), tensor(0.3750), tensor(0.3125), tensor(0.3750), tensor(0.4062), tensor(0.3906), tensor(0.4219), tensor(0.4219), tensor(0.3750), tensor(0.4375), tensor(0.4531), tensor(0.3125), tensor(0.3438), tensor(0.3594), tensor(0.3125), tensor(0.3281), tensor(0.3750), tensor(0.3750), tensor(0.3594), tensor(0.3438), tensor(0.3594), tensor(0.3906), tensor(0.4062), tensor(0.3906), tensor(0.3906), tensor(0.3594), tensor(0.3438), tensor(0.4531), tensor(0.4844), tensor(0.4375), tensor(0.4062), tensor(0.4062), tensor(0.4375), tensor(0.4375), tensor(0.4375), tensor(0.4531), tensor(0.3906)]]
test_loss_algo: [[4.157712936401367, 4.0467143058776855, 4.055594444274902, 3.6186320781707764, 2.6579267978668213, 3.0481162071228027, 2.985476016998291, 2.9278361797332764, 2.2205188274383545, 2.698378562927246, 2.4232354164123535, 2.3959848880767822, 2.264967918395996, 2.0248541831970215, 2.1080570220947266, 2.0977725982666016, 1.9091227054595947, 1.914225697517395, 1.8384182453155518, 2.197495937347412, 2.3171305656433105, 2.0408105850219727, 2.048624038696289, 1.9866845607757568, 2.1078104972839355, 2.0081870555877686, 1.8928231000900269, 2.101907253265381, 1.9781228303909302, 1.8614298105239868, 1.9925200939178467, 2.0213205814361572, 1.9038431644439697, 1.9431393146514893, 2.1397414207458496, 1.9411355257034302, 1.9085304737091064, 2.4442920684814453, 2.232417106628418, 2.144327163696289, 2.206456184387207, 2.343259334564209, 2.2480430603027344, 2.1877033710479736, 2.1926820278167725, 2.0805089473724365, 1.9650166034698486, 1.933402419090271, 1.9679079055786133, 1.891224980354309, 1.8105552196502686, 1.742250919342041, 1.7548894882202148, 1.7778000831604004, 1.7247720956802368, 1.7210936546325684, 1.740073561668396, 1.7373119592666626, 1.6904363632202148, 1.7933282852172852]]
global_train_loss_algo: [[]]
