import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 700 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,2)],
    2: [1/np.power(10,2)],
    3: [0.01]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            # 0: "MIFA",
            #1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.2261164820194246, 0.5991522927302867, 0.6976785127818583, 0.5516220956295729, 0.38622813859954475, 0.4174519106931984, 0.6515537747740746, 0.29016840683238115, 0.388218637779355, 0.5111751067638397, 0.5142153092473746, 0.3065200301562436, 0.44044616912491624, 0.37303304879926147], [2.227912336587906, 0.6532201510109007, 0.8637016583979131, 0.7806158615648746, 0.453066044896841, 0.6283648748695849, 0.8881218926608563, 0.503761811684817, 0.514317720606923, 0.6767108727991581, 0.7730547231435775, 0.4540877076261677, 0.6765417553111912, 0.5969292104244233]]
Acc algo: [[tensor(0.1094), tensor(0.2500), tensor(0.2812), tensor(0.2812), tensor(0.2812), tensor(0.3438), tensor(0.2500), tensor(0.4062), tensor(0.5312), tensor(0.4844), tensor(0.3594), tensor(0.4531), tensor(0.5000), tensor(0.3438)], [tensor(0.1406), tensor(0.2031), tensor(0.2656), tensor(0.2656), tensor(0.3750), tensor(0.3750), tensor(0.3750), tensor(0.4062), tensor(0.5000), tensor(0.4844), tensor(0.4062), tensor(0.4062), tensor(0.3906), tensor(0.4219)]]
test_loss_algo: [[2.291804790496826, 2.2529003620147705, 2.1354076862335205, 1.8765603303909302, 2.1706719398498535, 1.9232633113861084, 1.9887255430221558, 1.7666220664978027, 1.3776410818099976, 1.5260183811187744, 1.654112696647644, 1.5216927528381348, 1.5846041440963745, 2.259045124053955], [2.291808605194092, 2.2316436767578125, 2.1349363327026367, 1.8895305395126343, 1.9383962154388428, 1.773598074913025, 1.7695637941360474, 1.563858151435852, 1.421948790550232, 1.6238799095153809, 1.7150349617004395, 1.631772756576538, 1.618300437927246, 1.6952786445617676]]
global_train_loss_algo: [[], []]
