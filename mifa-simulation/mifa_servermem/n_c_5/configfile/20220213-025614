import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'dynamic' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,2)],
    2: [1/np.power(10,3)],
    3: [0.01]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            #0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            #3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.21609200835228, 1.0371284025907517, 0.9554880452156068, 0.9136296972632408, 0.818906991481781, 0.6989676459878684, 0.6847348276479169, 0.8573085343837737, 0.6521165307314368, 0.6763711352087557, 0.7468768259603531, 0.6408421362773515, 0.7933813598752023, 0.9079165506362916, 0.8383537924289703, 0.6992476705869194, 0.639462069508154, 0.692496539808053, 0.784141475558281, 0.8310851261019707, 0.6367136524431407, 0.5498977291584015, 0.7103849384188652, 0.547929058658192, 0.519575285119936, 0.8553697526454925, 0.7680715078115463, 0.6233724347734825, 0.5323129919508938, 0.635433851960115, 0.805560440272093, 0.7956104391813278, 0.44927209438988946, 0.7639029563218356, 0.7376036781910807, 0.6552904471525107, 0.7815970554947854, 0.6625347064633388, 0.7203845555498265, 0.6770957921187801, 0.791025582626462, 0.6072258579797926, 0.8713664929568766, 0.4331527990149334, 0.6036113376775757, 0.5047920849872753, 0.8509266468882561, 0.6805533637106419, 0.5996992641454563, 0.45848822768952235], [2.3229269051551817, 2.314004232883453, 2.2373943996429446, 2.2850223517417905, 2.2743184232711795, 2.1848760747909544, 2.1471591341495513, 2.0397850477695467, 1.0696965225040913, 0.7803142537828535, 0.818748724347679, 0.7732874203042592, 0.8712051373720169, 1.0009305581450463, 0.8993607415258884, 0.6665093895606696, 0.7085457422863692, 0.7493351099599386, 0.9592650583386421, 0.8608522766828537, 0.6732093030586839, 0.6045785795617848, 0.7194200670719146, 0.6472334685362876, 0.5231621397752315, 0.9193505015969275, 0.7923728266358376, 0.6675071355700493, 0.5095086965523661, 0.6394781341962517, 0.7842173963785172, 0.798046290576458, 0.4528878172952682, 0.781367041170597, 0.7552760654781014, 0.5773801629943773, 0.7717891755700113, 0.6346046716347337, 0.7605918520968407, 0.6013588383747265, 0.7314086492359638, 0.5671403201855719, 0.8433869281411172, 0.39460355712100864, 0.5741638175211847, 0.4979065251140855, 0.7607225455343722, 0.6877646817266941, 0.5644247284578159, 0.3955616689845919]]
Acc algo: [[tensor(0.1094), tensor(0.0625), tensor(0.0312), tensor(0.0938), tensor(0.1719), tensor(0.2656), tensor(0.3594), tensor(0.2031), tensor(0.2344), tensor(0.3125), tensor(0.3594), tensor(0.2656), tensor(0.2656), tensor(0.2812), tensor(0.2812), tensor(0.3438), tensor(0.2188), tensor(0.3594), tensor(0.2812), tensor(0.2812), tensor(0.3281), tensor(0.2812), tensor(0.3125), tensor(0.3125), tensor(0.2344), tensor(0.3125), tensor(0.3438), tensor(0.2344), tensor(0.2188), tensor(0.3438), tensor(0.3594), tensor(0.3125), tensor(0.3281), tensor(0.0938), tensor(0.3438), tensor(0.2656), tensor(0.3594), tensor(0.2188), tensor(0.2969), tensor(0.2656), tensor(0.3438), tensor(0.3750), tensor(0.3438), tensor(0.3438), tensor(0.2500), tensor(0.2812), tensor(0.4375), tensor(0.3750), tensor(0.3125), tensor(0.3750)], [tensor(0.1094), tensor(0.1250), tensor(0.0625), tensor(0.0781), tensor(0.1406), tensor(0.2500), tensor(0.2969), tensor(0.0938), tensor(0.2031), tensor(0.2656), tensor(0.0312), tensor(0.1250), tensor(0.1875), tensor(0.1562), tensor(0.2344), tensor(0.1719), tensor(0.1094), tensor(0.1562), tensor(0.1875), tensor(0.1250), tensor(0.2969), tensor(0.1719), tensor(0.1875), tensor(0.2344), tensor(0.2812), tensor(0.2188), tensor(0.1250), tensor(0.1406), tensor(0.0469), tensor(0.1406), tensor(0.2656), tensor(0.2188), tensor(0.2188), tensor(0.1094), tensor(0.2188), tensor(0.1406), tensor(0.1719), tensor(0.1406), tensor(0.1562), tensor(0.3125), tensor(0.1406), tensor(0.0781), tensor(0.2031), tensor(0.2656), tensor(0.2812), tensor(0.1875), tensor(0.3281), tensor(0.3125), tensor(0.2969), tensor(0.2188)]]
test_loss_algo: [[2.2958076000213623, 2.589407205581665, 2.3843185901641846, 2.3127033710479736, 2.231323480606079, 2.198765277862549, 2.0542855262756348, 2.1418609619140625, 2.2169578075408936, 2.097393751144409, 1.9804413318634033, 2.1534085273742676, 2.0874319076538086, 2.0985803604125977, 1.9563803672790527, 1.956554651260376, 2.102510452270508, 1.9434438943862915, 1.925328254699707, 1.95518159866333, 2.0408194065093994, 2.0146210193634033, 1.9196655750274658, 1.8240864276885986, 2.0205633640289307, 1.9363689422607422, 1.8788543939590454, 2.040557622909546, 2.071345567703247, 1.9777390956878662, 1.9174432754516602, 2.0006532669067383, 1.8937392234802246, 2.3892085552215576, 1.9710557460784912, 2.224050283432007, 2.118561267852783, 2.112814426422119, 1.8988975286483765, 1.9605231285095215, 1.8996407985687256, 1.9644920825958252, 2.0130491256713867, 1.8961355686187744, 2.227275848388672, 2.3756260871887207, 1.8667213916778564, 1.9400532245635986, 2.086498498916626, 1.9708739519119263], [2.297032594680786, 2.29923152923584, 2.2933871746063232, 2.292306661605835, 2.2815186977386475, 2.262272596359253, 2.233901262283325, 2.283553123474121, 2.6248161792755127, 2.2455990314483643, 2.8630001544952393, 2.6281235218048096, 2.586287260055542, 2.52394437789917, 2.4108285903930664, 2.519603729248047, 2.9400253295898438, 2.4623324871063232, 2.4105374813079834, 2.2701449394226074, 2.1606860160827637, 2.269040107727051, 2.613098621368408, 2.397407293319702, 2.0886917114257812, 2.4144461154937744, 2.9358465671539307, 2.2961881160736084, 2.704695701599121, 2.460197925567627, 2.3013627529144287, 2.80637526512146, 2.570453643798828, 3.1865077018737793, 2.1881961822509766, 2.747222661972046, 2.5118794441223145, 3.6388235092163086, 2.367027521133423, 2.0430991649627686, 2.4693710803985596, 3.158318519592285, 2.9508228302001953, 2.091233015060425, 1.8758854866027832, 2.1656227111816406, 1.9819525480270386, 1.9962499141693115, 2.2806742191314697, 2.3868467807769775]]
global_train_loss_algo: [[], []]
