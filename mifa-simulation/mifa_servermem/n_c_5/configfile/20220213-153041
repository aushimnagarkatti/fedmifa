import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = False


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 0
clust_mech = 'dynamic' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,3)],
    2: [1/np.power(10,3)],
    3: [1/np.power(10,3)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.2853887462615967, 2.231179530620575, 0.9711624389886856, 0.6859481601789594, 0.721621298417449, 0.7397768506407738, 0.734302676320076, 0.6493627597484737, 0.4582338153175079, 0.5458964732987807, 0.6996901139616967, 0.550426095277071, 0.6828150403685868, 0.582911176867783, 0.42946100510656837, 0.4573509725555778, 0.630775820761919, 0.32750554986763747, 0.4305456040846184, 0.4476558173261583], [2.3083102416992185, 2.278532781600952, 2.278089330196381, 1.9630850791931151, 0.8506078162789343, 0.9030449533462525, 1.00729907438159, 0.6341204533074051, 0.5739167840825394, 0.6443726376909762, 0.7566039394587278, 0.6487700055353344, 0.8067537479382008, 0.7385145452618599, 0.5905770461261273, 0.6964972721040249, 0.8087944319844246, 0.4352576079685241, 0.5879830077243968, 0.5783502025157212], [2.3080807018280027, 2.2799857521057127, 2.275311229228973, 1.9778366696834564, 0.8557982036471368, 0.8298746094107627, 0.9382264402508737, 0.7022325999848544, 0.5977848910982722, 0.5710549973463639, 0.8051405969262124, 0.6098625757731497, 0.74288796948269, 0.7099662438035012, 0.6323214511573315, 0.65832265868783, 0.7869243711233139, 0.462431818805635, 0.5492399335559458, 0.6296887950599194], [2.309935486316681, 2.2858316254615785, 2.3007501363754272, 2.2154434752464294, 1.8103956019878389, 1.0995877832174301, 1.3444257539510727, 1.0764815816283224, 0.9423180813342332, 0.9391538136452438, 1.1605661350488663, 0.9899140104651452, 1.0508833372220399, 1.0342173939943315, 0.9179136082530022, 0.9350270017981529, 1.1933978456258774, 0.7333229882642627, 0.945491085499525, 1.0696757687628269]]
Acc algo: [[tensor(0.1562), tensor(0.0938), tensor(0.2344), tensor(0.2031), tensor(0.2969), tensor(0.3125), tensor(0.2500), tensor(0.2500), tensor(0.2344), tensor(0.2969), tensor(0.2656), tensor(0.3594), tensor(0.3125), tensor(0.3906), tensor(0.3125), tensor(0.4062), tensor(0.3750), tensor(0.3750), tensor(0.3906), tensor(0.4062)], [tensor(0.1562), tensor(0.1250), tensor(0.1094), tensor(0.1406), tensor(0.2656), tensor(0.2812), tensor(0.3906), tensor(0.2656), tensor(0.3438), tensor(0.2812), tensor(0.3281), tensor(0.3594), tensor(0.2969), tensor(0.2812), tensor(0.2812), tensor(0.3125), tensor(0.3281), tensor(0.3281), tensor(0.2969), tensor(0.3594)], [tensor(0.1562), tensor(0.0938), tensor(0.0938), tensor(0.1406), tensor(0.2812), tensor(0.2188), tensor(0.0938), tensor(0.2031), tensor(0.3125), tensor(0.1406), tensor(0.4062), tensor(0.1719), tensor(0.2500), tensor(0.2969), tensor(0.1719), tensor(0.1875), tensor(0.1406), tensor(0.1406), tensor(0.2344), tensor(0.1875)], [tensor(0.1562), tensor(0.1250), tensor(0.0938), tensor(0.1250), tensor(0.3438), tensor(0.2031), tensor(0.1875), tensor(0.2812), tensor(0.2656), tensor(0.1875), tensor(0.3594), tensor(0.2031), tensor(0.2969), tensor(0.3281), tensor(0.2812), tensor(0.2188), tensor(0.1562), tensor(0.2344), tensor(0.3750), tensor(0.3438)]]
test_loss_algo: [[2.2927491664886475, 2.284965991973877, 2.645843267440796, 2.0517423152923584, 2.001551389694214, 1.892293095588684, 1.9144964218139648, 1.93986177444458, 1.8624162673950195, 1.7946538925170898, 1.7427393198013306, 1.6965116262435913, 1.6624335050582886, 1.6723856925964355, 1.6455806493759155, 1.6023553609848022, 1.5977703332901, 1.5694524049758911, 1.5564844608306885, 1.5175042152404785], [2.292742967605591, 2.2884461879730225, 2.266801118850708, 2.174487829208374, 2.0819766521453857, 1.9828630685806274, 1.9114428758621216, 1.9105583429336548, 1.8140794038772583, 1.9286415576934814, 1.841912031173706, 1.7844284772872925, 1.8557251691818237, 1.8616117238998413, 1.818902611732483, 1.7722463607788086, 1.7853745222091675, 1.7736914157867432, 1.7973017692565918, 1.7495343685150146], [2.2927422523498535, 2.2855167388916016, 2.262686252593994, 2.1841752529144287, 2.14984130859375, 2.5890212059020996, 3.331404447555542, 2.2248079776763916, 2.0833029747009277, 2.286489248275757, 1.9789347648620605, 2.346435785293579, 2.1625823974609375, 1.968767762184143, 2.4912266731262207, 2.6178712844848633, 3.3896403312683105, 2.715149402618408, 2.345036268234253, 2.102600574493408], [2.292764663696289, 2.288945436477661, 2.2745044231414795, 2.2201642990112305, 2.0601084232330322, 2.3980679512023926, 2.0455286502838135, 1.9221583604812622, 1.9678280353546143, 2.055562734603882, 1.9470933675765991, 2.1603705883026123, 1.922445297241211, 1.866853952407837, 1.875116229057312, 2.081463575363159, 2.5114824771881104, 2.174394130706787, 1.8368940353393555, 1.7993764877319336]]
global_train_loss_algo: [[2.317725508109383, 2.3024871788366372, 2.6689176803354715, 2.1471665750074265, 1.9885262737188802, 2.020426508868137, 1.92718855636504, 1.9644762090100047, 1.9045020306811613, 1.8432285941165427, 1.7644999614152153, 1.7266807612555717, 1.7121948458044731, 1.6872965815427052, 1.5814820564616368, 1.5631329546804014, 1.5277744024001119, 1.5387609756511191, 1.4823427803985907, 1.4645118842954221], [2.31760452165628, 2.308963591485377, 2.295439986926515, 2.2777393428261017, 2.1536556165236647, 2.1316186625634312, 2.091705958251758, 2.01964139694448, 1.9629634390096835, 1.924988083217455, 1.9399287781447097, 1.8988315318246631, 1.938069904399345, 1.8644570273816432, 1.9024184087048406, 1.9970016447479462, 1.99627438957429, 1.8493850601603612, 1.88393410636336, 1.8671579976825763], [2.3175853152409234, 2.3104201488177796, 2.29637203679975, 2.3011793919536463, 2.497172347420012, 2.7170068135346903, 3.35627971860149, 2.3790047873011635, 2.4304201005364927, 2.5630397683824113, 2.5955339456763107, 2.0909294405251817, 2.005294398426095, 1.975629584106338, 2.4679422259635633, 2.590990802820991, 2.690290670870515, 2.757492455680047, 2.3988631878362594, 2.443271377507378], [2.317629856526699, 2.309302224222656, 2.299709110308791, 2.281004204774452, 2.221310444195252, 2.573985801633362, 2.1705607306926757, 2.0418598880548307, 2.003895906688612, 2.201201609638341, 2.19376145665298, 1.9547894851630911, 1.8821350642482337, 1.897970831150289, 1.998507245879649, 2.279879898061533, 2.1960239355521436, 2.3022335261640037, 1.9765821068793001, 2.0208014225410986]]
