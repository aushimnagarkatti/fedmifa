import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist'#'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.1],#[1/np.power(10,2.5)],
    1: [0.1],#[1/np.power(10,2)],
    2: [0.1]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.9
    , #factor to reduce lr in scheduler
    1:0.9,
    2:0.9
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.8305912931760147, 3.550978231747945, 3.2088461445172634, 2.8047314508756003, 2.787479351758957, 2.083385907173157, 2.1484877260923385, 1.600568112373352, 1.5104155007998148, 1.3514706139912207, 1.3229936151504518, 1.1654071573416391, 1.4334165257612863, 1.0518727533022563, 1.0361653939882913, 0.9864437738992274, 1.1294947187105815, 0.9878600508371989, 0.9416382120450338, 0.7718544467290244, 0.9503310399055481, 0.7995557872454325, 0.6479862894614538, 0.8432820721467337, 0.7520553252895674, 0.7531230815052986, 0.7987650736172994, 1.014471093416214, 0.631384844005108, 0.8946456476052603, 0.7162755089998245, 0.7257538576126098, 0.9777363348007203, 0.9324782118201256, 0.7912904632091522, 0.445082479317983, 0.595672781864802, 0.7278566257158914, 0.9468144841988881, 0.6341786710421244, 0.8392452508707843, 0.6038699582219124, 0.6744617066780725, 0.798905305981636, 1.2293652174870173, 0.6358946491877238, 0.6833342693646749, 0.6313289611538251, 0.7645858806371688, 0.7988424524168173], [3.8428853050867717, 2.5417813824017843, 2.054481590429942, 1.5907379472255705, 1.6617484289407731, 1.1719563392798107, 1.3518864697938162, 0.90284521373113, 0.9232836443185806, 0.9708010604381562, 0.8918049687544505, 0.8029793860514959, 1.1487083594004315, 0.7569018240769704, 0.7793608397841453, 0.9714512338240942, 0.8833691186904907, 0.8052220997810364, 0.7661963791449865, 0.6145496748586495, 0.804142839829127, 0.6460827275117238, 0.5065594859917959, 0.6967728274067243, 0.6833292226791382, 0.651241568962733, 0.6817308109601339, 0.8721462156375249, 0.5144122842028738, 0.784784059425195, 0.6470223951141039, 0.5944461085597673, 0.7799122315645217, 0.8166923723220826, 0.7284553173383077, 0.37846372663974764, 0.5316937779386839, 0.6490740356047948, 0.8058950107097627, 0.5263527525464694, 0.7690446535746256, 0.5316391468048096, 0.5583490501791238, 0.6961740886171659, 0.8420415813724199, 0.545785034875075, 0.6635005579007169, 0.5957605327169101, 0.664387426438431, 0.6356677624800553], [3.841498835881551, 2.543878671169281, 2.077329692363739, 1.8013769759734473, 1.5981524935364724, 1.1757126735051473, 1.4567915690541269, 0.8923248683214189, 0.9274288074175517, 0.893242949138085, 0.928759325782458, 0.7050021494428316, 1.130658946752548, 0.7736289379994075, 0.7374066860874494, 0.7421674039463202, 0.9237808290322622, 0.8142164467970531, 0.7263182673454285, 0.6561669652263322, 0.7931447061300279, 0.6385824582974116, 0.5379130499064921, 0.712449463903904, 0.6848599129120508, 0.6804938713709514, 0.6558534716765086, 0.8860963108142217, 0.45349303416411074, 0.7703285970290501, 0.6259241816401482, 0.5852205140193304, 0.7119353068073591, 0.8366329716245333, 0.7194175283908842, 0.3777623157699903, 0.5285216873387496, 0.6383672703504562, 0.7402241747379301, 0.5776685307423273, 0.6935485062934458, 0.5221996595462164, 0.5335651244272789, 0.6896290631095567, 0.9286106873552005, 0.5558592272400856, 0.6574050508737563, 0.583436910311381, 0.7366268326838811, 0.6804494420563181]]
Acc algo: [[tensor(0.), tensor(0.0312), tensor(0.0312), tensor(0.0469), tensor(0.1250), tensor(0.2656), tensor(0.2656), tensor(0.3281), tensor(0.3281), tensor(0.3906), tensor(0.3906), tensor(0.3281), tensor(0.3906), tensor(0.4375), tensor(0.4219), tensor(0.3906), tensor(0.4531), tensor(0.4531), tensor(0.4375), tensor(0.4844), tensor(0.4375), tensor(0.5156), tensor(0.5312), tensor(0.5000), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5781), tensor(0.5469), tensor(0.4844), tensor(0.5312), tensor(0.5312), tensor(0.5625), tensor(0.5781), tensor(0.5469), tensor(0.5156), tensor(0.5156), tensor(0.4844), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5156), tensor(0.4688), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5156), tensor(0.5156)], [tensor(0.0312), tensor(0.0625), tensor(0.1719), tensor(0.3594), tensor(0.3281), tensor(0.3906), tensor(0.3438), tensor(0.4531), tensor(0.4531), tensor(0.4688), tensor(0.4375), tensor(0.4844), tensor(0.5312), tensor(0.5156), tensor(0.4688), tensor(0.5000), tensor(0.5938), tensor(0.5625), tensor(0.5156), tensor(0.5156), tensor(0.5469), tensor(0.5000), tensor(0.5469), tensor(0.5000), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5156), tensor(0.5156), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5000), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.5469), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5625), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5625), tensor(0.5625), tensor(0.6094)], [tensor(0.0156), tensor(0.0625), tensor(0.2031), tensor(0.2969), tensor(0.2344), tensor(0.3594), tensor(0.4531), tensor(0.4375), tensor(0.4844), tensor(0.4219), tensor(0.4531), tensor(0.4219), tensor(0.4844), tensor(0.4531), tensor(0.5469), tensor(0.5000), tensor(0.5156), tensor(0.4219), tensor(0.4844), tensor(0.4844), tensor(0.5312), tensor(0.4375), tensor(0.5000), tensor(0.5938), tensor(0.5625), tensor(0.5781), tensor(0.4844), tensor(0.5000), tensor(0.5938), tensor(0.5781), tensor(0.4844), tensor(0.5312), tensor(0.5156), tensor(0.5000), tensor(0.5625), tensor(0.5156), tensor(0.4844), tensor(0.5938), tensor(0.5469), tensor(0.5312), tensor(0.5000), tensor(0.4062), tensor(0.5938), tensor(0.5938), tensor(0.6094), tensor(0.5469), tensor(0.5781), tensor(0.4688), tensor(0.5625), tensor(0.5625)]]
test_loss_algo: [[4.12188720703125, 6.3063201904296875, 3.6568315029144287, 3.5461337566375732, 2.907221794128418, 2.2741518020629883, 2.7534358501434326, 2.298539400100708, 2.2485928535461426, 2.1031270027160645, 2.030341625213623, 2.4424173831939697, 1.9685039520263672, 2.0742404460906982, 2.0391647815704346, 1.7553025484085083, 1.7915033102035522, 1.565961241722107, 1.8886831998825073, 1.6410077810287476, 1.897539496421814, 1.5714178085327148, 1.5256381034851074, 1.671101689338684, 1.390190839767456, 1.6388609409332275, 1.5035771131515503, 1.4095067977905273, 1.6765403747558594, 1.3909236192703247, 1.45658540725708, 1.727744221687317, 1.4401612281799316, 1.5430814027786255, 1.495797872543335, 1.4886475801467896, 1.657915711402893, 1.5677262544631958, 1.4478893280029297, 1.514082670211792, 1.4975558519363403, 1.4796534776687622, 1.5112485885620117, 1.471174955368042, 1.5796780586242676, 1.4498894214630127, 1.6099145412445068, 1.475461483001709, 1.5251327753067017, 1.5485090017318726], [4.0442352294921875, 2.8101022243499756, 2.536792278289795, 2.3794827461242676, 2.1016664505004883, 1.9171205759048462, 2.224092483520508, 2.044398546218872, 1.5794869661331177, 1.4642956256866455, 1.7300630807876587, 1.7649428844451904, 1.6356292963027954, 1.3419382572174072, 1.6850559711456299, 1.6500426530838013, 1.511784553527832, 1.2711559534072876, 1.5802416801452637, 1.2743486166000366, 1.3529311418533325, 1.4707441329956055, 1.4025028944015503, 1.4796258211135864, 1.2953922748565674, 1.4233511686325073, 1.3579643964767456, 1.3057279586791992, 1.3491053581237793, 1.210455060005188, 1.2354276180267334, 1.3190572261810303, 1.534434199333191, 1.4421000480651855, 1.3785285949707031, 1.3299778699874878, 1.4133591651916504, 1.3323378562927246, 1.3067495822906494, 1.3151808977127075, 1.4366493225097656, 1.2469788789749146, 1.3725887537002563, 1.2671515941619873, 1.4623661041259766, 1.3465497493743896, 1.4692249298095703, 1.11103355884552, 1.2669636011123657, 1.1862558126449585], [4.036402225494385, 3.2194578647613525, 2.574446201324463, 2.2596094608306885, 2.415674924850464, 2.063894271850586, 1.607642412185669, 1.8254255056381226, 1.588619589805603, 1.869486689567566, 1.4898772239685059, 1.7241370677947998, 1.5441926717758179, 1.5068886280059814, 1.466788649559021, 1.626924991607666, 1.505663275718689, 1.564382553100586, 1.4122350215911865, 1.8486616611480713, 1.344280481338501, 1.8637210130691528, 1.5876057147979736, 1.6076899766921997, 1.4363667964935303, 1.3855115175247192, 1.6733932495117188, 1.600339412689209, 1.3143526315689087, 1.5118387937545776, 1.6917898654937744, 1.5124856233596802, 1.6791073083877563, 1.442523717880249, 1.3913987874984741, 1.3914847373962402, 1.5780656337738037, 1.4658979177474976, 1.4819567203521729, 1.5510224103927612, 1.9727650880813599, 1.768285870552063, 1.3316011428833008, 1.241258978843689, 1.1399751901626587, 1.2603931427001953, 1.3422044515609741, 1.8172917366027832, 1.1340008974075317, 1.3416316509246826]]
global_train_loss_algo: [[], [], []]
