import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare' #emnist'#cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.07],#[1/np.power(10,2.5)],
    1: [0.03],#[1/np.power(10,2)],
    2: [0.05]#[1/np.power(10,2)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:0.8
    }

sch_freq = 300 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.996466481789331, 2.7092147324487534, 2.407838637756727, 2.41981182530286, 2.1714372285082453, 1.9716908861657558, 1.854482951628908, 1.8806429956500192, 1.7601358737094945, 1.73211808033169, 1.7413625389682177, 1.6653161699387617, 1.6192199186402205, 1.6221159529785318, 1.6287672519511769, 1.601986660122379, 1.5612249802458333, 1.6163336736328333, 1.6356876298876557, 1.4782693364572903, 1.5842637370546957, 1.5461694285011447, 1.5448450212531806, 1.5101575345867375, 1.4647863107225503, 1.478715450969871, 1.4869996718976186, 1.3748835401462787, 1.3590915428464858, 1.4313303068076377, 1.5072180053467161, 1.485993800745734, 1.4753562947945695, 1.4676819465364506, 1.4561188678831514, 1.4663990622216123, 1.4195168281239503, 1.3563108793503424, 1.4378673467203127, 1.4551636490758713, 1.4058293113133469, 1.3640131327809197, 1.369023360236516, 1.3117467436604102, 1.451951562867946, 1.4777902949486947, 1.4177930896687623, 1.4150081932760326, 1.3276296085198986, 1.3490148737004166], [4.3510749613791395, 2.3565704129475242, 2.184280871493219, 2.20601755666512, 2.035376440562454, 1.8759669151920026, 1.7855458666996569, 1.8348688576200485, 1.7162476010869405, 1.6954768516371748, 1.7175694091243898, 1.6649153568281794, 1.5842273253490724, 1.6512579534550507, 1.6140377510318924, 1.5581985564211733, 1.5413388895851443, 1.5635867898984832, 1.592578812458473, 1.4137138290130902, 1.560296337449909, 1.495997017141641, 1.4812266785329409, 1.4556804116074744, 1.429987027288281, 1.4136373385236478, 1.417118279143875, 1.361569261330145, 1.3058676259478212, 1.3739468094514735, 1.445112060037306, 1.3990926281021896, 1.3969612276881722, 1.3807934756718312, 1.404697566042373, 1.4232764764055001, 1.3518354674808672, 1.321330352404598, 1.3560411532968077, 1.381474320355567, 1.359695280165163, 1.282689836835631, 1.2836228082925183, 1.2556846764670333, 1.4029063456469353, 1.42431218548657, 1.3703073618430381, 1.341331751831944, 1.273398291245729, 1.289948479675351], [4.251011376526103, 2.2979612372133795, 2.08318307619365, 2.116324752498388, 1.9231646749948126, 1.7764241674214643, 1.6825408722538548, 1.7154415149422462, 1.5994646643690444, 1.559933929612826, 1.595411898650269, 1.4969658965374524, 1.4581111789554164, 1.4925175342460473, 1.498446151990845, 1.4803113304290718, 1.412627369162566, 1.4908246305881416, 1.5194144855519127, 1.3514407195349534, 1.4901250001021795, 1.4332816585572132, 1.4207227426235387, 1.4026879293019492, 1.3680772373762475, 1.3658359636447133, 1.388057581929309, 1.2825460985212618, 1.2508011821421043, 1.3398121405435675, 1.3990126351037708, 1.357341411811111, 1.3743004128675458, 1.3831126923107948, 1.3992342784165834, 1.396086346238609, 1.3522856893330302, 1.3100782954416523, 1.3372203222181394, 1.3681092073405439, 1.352822256190669, 1.307369387685275, 1.276596092334762, 1.2642480090318522, 1.4251883401779712, 1.4169742640509633, 1.369729998507497, 1.3667225501839415, 1.2833645475433315, 1.3095021060165668]]
Acc algo: [[tensor(0.), tensor(0.1719), tensor(0.3750), tensor(0.2656), tensor(0.2969), tensor(0.3438), tensor(0.4375), tensor(0.4062), tensor(0.4219), tensor(0.4219), tensor(0.4062), tensor(0.4844), tensor(0.5000), tensor(0.4688), tensor(0.5156), tensor(0.5312), tensor(0.5156), tensor(0.5000), tensor(0.5781), tensor(0.5000), tensor(0.5156), tensor(0.5000), tensor(0.5469), tensor(0.5156), tensor(0.5312), tensor(0.4688), tensor(0.4844), tensor(0.5000), tensor(0.5156), tensor(0.4844), tensor(0.5156), tensor(0.4688), tensor(0.5469), tensor(0.5000), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5781), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.6094), tensor(0.5781)], [tensor(0.1875), tensor(0.3125), tensor(0.3750), tensor(0.4062), tensor(0.4062), tensor(0.4531), tensor(0.4688), tensor(0.4531), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.4844), tensor(0.5156), tensor(0.4844), tensor(0.5156), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5469), tensor(0.5625), tensor(0.5938), tensor(0.5938), tensor(0.5938), tensor(0.5781), tensor(0.6250), tensor(0.6250), tensor(0.5938), tensor(0.5938), tensor(0.5625), tensor(0.6094), tensor(0.6094), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.6094), tensor(0.5469), tensor(0.5781), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.6250), tensor(0.6250), tensor(0.5625), tensor(0.5781)], [tensor(0.1875), tensor(0.3125), tensor(0.3594), tensor(0.4531), tensor(0.4688), tensor(0.4688), tensor(0.5000), tensor(0.4531), tensor(0.4531), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.4688), tensor(0.5156), tensor(0.5000), tensor(0.4688), tensor(0.5156), tensor(0.5156), tensor(0.5000), tensor(0.5156), tensor(0.5312), tensor(0.5000), tensor(0.5156), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5156), tensor(0.5781), tensor(0.5625), tensor(0.5469), tensor(0.5781), tensor(0.5312), tensor(0.5625), tensor(0.5938), tensor(0.5625), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5781), tensor(0.5781), tensor(0.5469), tensor(0.5625), tensor(0.5469), tensor(0.5469), tensor(0.5781), tensor(0.5625), tensor(0.5781)]]
test_loss_algo: [[4.868533134460449, 3.317293643951416, 2.43338942527771, 2.4655022621154785, 2.0701303482055664, 2.074467182159424, 1.9087060689926147, 1.9350873231887817, 1.8784457445144653, 1.8408135175704956, 1.7768198251724243, 1.7428961992263794, 1.713441252708435, 1.6453871726989746, 1.6109758615493774, 1.6342023611068726, 1.6524790525436401, 1.638248085975647, 1.5473161935806274, 1.6063958406448364, 1.6025357246398926, 1.5624885559082031, 1.5905530452728271, 1.5549348592758179, 1.5948474407196045, 1.5615317821502686, 1.5792806148529053, 1.5432558059692383, 1.5619070529937744, 1.5257230997085571, 1.575056791305542, 1.5311110019683838, 1.5131388902664185, 1.5451102256774902, 1.546661615371704, 1.5042393207550049, 1.5171749591827393, 1.523671269416809, 1.5076487064361572, 1.5306307077407837, 1.5442454814910889, 1.5329463481903076, 1.494628667831421, 1.493074893951416, 1.473116159439087, 1.4986814260482788, 1.4666070938110352, 1.4820626974105835, 1.4103527069091797, 1.4606611728668213], [4.104996681213379, 2.290168285369873, 2.1189920902252197, 2.045114517211914, 1.918972373008728, 1.833335280418396, 1.7389839887619019, 1.720127820968628, 1.7348905801773071, 1.678169846534729, 1.685784935951233, 1.6130038499832153, 1.5982773303985596, 1.5665910243988037, 1.5711358785629272, 1.5447546243667603, 1.5286108255386353, 1.5514791011810303, 1.5377538204193115, 1.5639533996582031, 1.560580849647522, 1.5297749042510986, 1.4980401992797852, 1.5100691318511963, 1.513664722442627, 1.470268964767456, 1.500840663909912, 1.495571494102478, 1.4858847856521606, 1.4728329181671143, 1.4958579540252686, 1.475839376449585, 1.458124041557312, 1.4526087045669556, 1.4497992992401123, 1.4741909503936768, 1.4533100128173828, 1.4250303506851196, 1.402836799621582, 1.4151225090026855, 1.442389726638794, 1.4168446063995361, 1.436862826347351, 1.4429123401641846, 1.4202144145965576, 1.4057843685150146, 1.3840028047561646, 1.3922245502471924, 1.4159855842590332, 1.4150758981704712], [3.8818483352661133, 2.2158150672912598, 2.059959650039673, 1.9069199562072754, 1.824077844619751, 1.7445000410079956, 1.7092591524124146, 1.6918543577194214, 1.740708351135254, 1.7157776355743408, 1.7174663543701172, 1.721688985824585, 1.7008451223373413, 1.7512589693069458, 1.701654314994812, 1.7118231058120728, 1.713931918144226, 1.6398111581802368, 1.6504403352737427, 1.606522798538208, 1.646826982498169, 1.65841543674469, 1.7036290168762207, 1.6680704355239868, 1.6875782012939453, 1.6497713327407837, 1.633293867111206, 1.6778749227523804, 1.6364322900772095, 1.6060429811477661, 1.5664218664169312, 1.5554450750350952, 1.5994073152542114, 1.5604352951049805, 1.6287600994110107, 1.5923329591751099, 1.5448904037475586, 1.6199363470077515, 1.6274460554122925, 1.5773751735687256, 1.6441473960876465, 1.6034146547317505, 1.5697077512741089, 1.6344702243804932, 1.609847903251648, 1.6781377792358398, 1.5460379123687744, 1.6110352277755737, 1.6822892427444458, 1.6490057706832886]]
global_train_loss_algo: [[], [], []]
