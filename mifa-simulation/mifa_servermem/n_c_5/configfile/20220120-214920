import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist'#'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.1],#[1/np.power(10,2.5)],
    1: [0.08],#[1/np.power(10,2)],
    2: [0.08]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.9
    , #factor to reduce lr in scheduler
    1:1,
    2:1
    }

sch_freq = 40 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
           # 1: "UMIFA",
           # 2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.7989780044555665, 3.514582206408183, 3.3011563301086424, 2.907125228246053, 2.5872155075073247, 2.454998989105225, 2.2300484013557433, 2.11961369228363, 2.2443721361955005, 2.1168018456300097, 1.8466479526360833, 1.7804661854108175, 1.8850139064788816, 1.6118493541081746, 1.9433478673299152, 1.4942960899670918, 1.5473156225681306, 1.450854842821757, 1.460079377492269, 1.6809977865219117, 1.6387372610966366, 1.49854163368543, 1.8707570031483969, 1.6230022692680357, 1.5896672296524046, 1.479692900975545, 1.5425955088933307, 1.7994303623835246, 1.6254541796843214, 1.5305564506848652, 1.6308276631037395, 1.8975581669807435, 1.8056112866600356, 1.4337874182065327, 2.063665496667226, 1.7255110466480255, 1.760501592795054, 1.7480564165115358, 1.8021902190844217, 1.6388141059875487, 1.5716437611933798, 1.699712121407191, 1.5354992580413822, 1.334158421754837, 1.7850668382644652, 1.8939753199418388, 1.6744334950447084, 1.693240130197434, 1.8104586124420166, 1.7394408496220908, 1.729887698173523, 1.7204758190313973, 1.8786526600519817, 1.7195735100110372, 1.8074855995178223, 1.6451032710075377, 1.9308917146921156, 2.1040851411819457, 1.6356140732765199, 1.8093430066108702]]
Acc algo: [[tensor(0.0625), tensor(0.0625), tensor(0.0312), tensor(0.0469), tensor(0.0312), tensor(0.1406), tensor(0.1875), tensor(0.1406), tensor(0.2188), tensor(0.2188), tensor(0.2031), tensor(0.2656), tensor(0.3594), tensor(0.3281), tensor(0.3125), tensor(0.3438), tensor(0.3750), tensor(0.3281), tensor(0.2969), tensor(0.3906), tensor(0.3281), tensor(0.3125), tensor(0.2500), tensor(0.2969), tensor(0.2969), tensor(0.3125), tensor(0.3125), tensor(0.3438), tensor(0.3281), tensor(0.2812), tensor(0.3125), tensor(0.3125), tensor(0.3438), tensor(0.3125), tensor(0.3281), tensor(0.2812), tensor(0.3438), tensor(0.2812), tensor(0.3438), tensor(0.2969), tensor(0.2969), tensor(0.3750), tensor(0.3750), tensor(0.3438), tensor(0.2656), tensor(0.3125), tensor(0.3125), tensor(0.3438), tensor(0.3750), tensor(0.3125), tensor(0.2969), tensor(0.3594), tensor(0.2500), tensor(0.3438), tensor(0.2812), tensor(0.2969), tensor(0.2500), tensor(0.2969), tensor(0.3281), tensor(0.3281)]]
test_loss_algo: [[4.116152763366699, 4.302548408508301, 5.48736572265625, 3.4952149391174316, 3.1840832233428955, 2.9222333431243896, 2.657320737838745, 2.5754234790802, 2.7249250411987305, 2.672579765319824, 2.862766981124878, 2.498643636703491, 2.0692615509033203, 2.2963223457336426, 2.3099284172058105, 2.093505620956421, 2.002053737640381, 2.199927568435669, 2.2035069465637207, 1.8217209577560425, 2.0019893646240234, 2.3116261959075928, 2.313734531402588, 2.115910530090332, 2.0935769081115723, 2.013808250427246, 1.9065349102020264, 1.9978677034378052, 1.9251525402069092, 1.989018440246582, 2.016962766647339, 1.9816009998321533, 1.9651051759719849, 2.0476207733154297, 1.9376062154769897, 1.7951375246047974, 1.7851382493972778, 2.1145918369293213, 1.8905932903289795, 1.945210576057434, 1.978693962097168, 1.791406512260437, 1.6539673805236816, 1.763207197189331, 1.954799771308899, 1.8785772323608398, 1.9126720428466797, 1.8794689178466797, 1.8833884000778198, 1.706187129020691, 1.7052631378173828, 1.9150400161743164, 2.1735901832580566, 1.7824598550796509, 1.91808021068573, 1.8967723846435547, 2.2912521362304688, 2.009958505630493, 1.818346381187439, 1.817158818244934]]
global_train_loss_algo: [[]]
