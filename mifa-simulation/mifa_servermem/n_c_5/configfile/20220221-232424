import lenet
import numpy as np

#Generic Hyp
batch_size= 64
 #Neural Network batch size
n_rnds= 1500 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250#250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'r' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.003],
    1: [0.003],
    2: [0.003], #0.05
    3: [0.003],
    4: [0.003]
    }

lrfactor = {
    0:0.5, #factor to reduce lr in scheduler
    1:0.5,
    2:0.5,
    3:0.5,
    4:0.5
    }

sch_freq = [1000] #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            #3: "scaffold",
            4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[1.4323235511779786, 1.0193147477507591, 0.8870093649625778, 0.7610920365154744, 0.6707073079049587, 0.6410533542092891, 0.6426650713384151, 0.44077692828141163, 0.5152809267863632, 0.5840876819193364, 0.5836311116069556, 0.40425392080098393, 0.5324672330357135, 0.48177644282579424, 0.40112196926027543, 0.42534910291433337, 0.2926447826030198, 0.42606053546071043, 0.20845223104930483, 0.31614823613315823, 0.3114536259230226, 0.2346081865392625, 0.3186159344948828, 0.23836153064330579, 0.27463148947281296, 0.13340345306089146, 0.40695538993924857, 0.31812668688595297, 0.25282684247009457, 0.27409828450297935], [1.441729494333267, 0.9023054793477059, 0.7734378063678742, 0.6686540473252536, 0.48196303591132167, 0.4892143716325517, 0.4149981657043099, 0.323335906630382, 0.4052242753002793, 0.3900489072408527, 0.504252516888082, 0.2599142006598413, 0.4257843962684274, 0.3549666083045304, 0.3002143020229414, 0.32736595744267105, 0.24729579413804456, 0.3526051107048988, 0.1823416567465756, 0.27023250070400534, 0.26159259519597977, 0.16953477818518875, 0.295371787622571, 0.2195598469133256, 0.21283544062811416, 0.11114877640269696, 0.3338667978346348, 0.2665175911132246, 0.21220428363420069, 0.2120738046849146], [1.4571068584918976, 0.877361164689064, 0.752941752076149, 0.6780663537979127, 0.4765587389841676, 0.4669453539885581, 0.4581440212577582, 0.3872263790539, 0.44022491998970514, 0.48218024060130116, 0.4809228485636413, 0.3487508703954518, 0.47200436394661666, 0.3806757835112512, 0.3371613517496735, 0.38563427571207287, 0.21774794280296192, 0.42942948020063343, 0.1868776663811877, 0.2956193067319691, 0.2685051471611951, 0.2313168272841722, 0.25737485791556536, 0.2406198860699078, 0.2008120886934921, 0.11825315320398659, 0.3737515675229952, 0.2640598673839122, 0.18586774736410006, 0.2744403407629579], [1.4010459131002426, 0.8954004153609276, 0.7889626947045325, 0.63959547534585, 0.46056748133152725, 0.49811901429900896, 0.4328096711076796, 0.348173520732671, 0.39674973381683226, 0.41712321782484646, 0.46620185426436367, 0.27903032019734386, 0.4624330481141806, 0.36343121461570266, 0.3212842750735581, 0.32453829453326766, 0.22933503381762424, 0.38198669349774717, 0.1619586546981009, 0.23705825260840357, 0.30095541328541, 0.14646289853379132, 0.2751215163106099, 0.26370921585126783, 0.2335482042613876, 0.14061003907583655, 0.3453812936949544, 0.3023939717654139, 0.24636956830043344, 0.23544363277731462]]
acc_algo = [[tensor(0.0008), tensor(0.1875), tensor(0.2330), tensor(0.2837), tensor(0.3033), tensor(0.3399), tensor(0.3454), tensor(0.3814), tensor(0.4135), tensor(0.4126), tensor(0.4270), tensor(0.4430), tensor(0.4608), tensor(0.4713), tensor(0.4949), tensor(0.5014), tensor(0.4983), tensor(0.5072), tensor(0.5194), tensor(0.5348), tensor(0.5411), tensor(0.5443), tensor(0.5640), tensor(0.5585), tensor(0.5844), tensor(0.5861), tensor(0.5966), tensor(0.6056), tensor(0.6081), tensor(0.5949)], [tensor(0.0008), tensor(0.1618), tensor(0.3276), tensor(0.4006), tensor(0.4148), tensor(0.4228), tensor(0.4194), tensor(0.4627), tensor(0.4860), tensor(0.4797), tensor(0.4914), tensor(0.4965), tensor(0.5175), tensor(0.5406), tensor(0.5486), tensor(0.5475), tensor(0.5675), tensor(0.5798), tensor(0.5750), tensor(0.5794), tensor(0.5811), tensor(0.5774), tensor(0.6317), tensor(0.6327), tensor(0.6216), tensor(0.6254), tensor(0.6592), tensor(0.6228), tensor(0.6376), tensor(0.6637)], [tensor(0.0008), tensor(0.1279), tensor(0.2828), tensor(0.2863), tensor(0.3336), tensor(0.3988), tensor(0.3595), tensor(0.3811), tensor(0.4067), tensor(0.3570), tensor(0.4641), tensor(0.4364), tensor(0.4914), tensor(0.4498), tensor(0.4923), tensor(0.3536), tensor(0.4576), tensor(0.4554), tensor(0.5139), tensor(0.4772), tensor(0.5413), tensor(0.4800), tensor(0.4631), tensor(0.5803), tensor(0.5349), tensor(0.5418), tensor(0.5865), tensor(0.5068), tensor(0.5480), tensor(0.6472)], [tensor(0.0008), tensor(0.2868), tensor(0.3566), tensor(0.3881), tensor(0.3884), tensor(0.4354), tensor(0.4314), tensor(0.4540), tensor(0.4624), tensor(0.4944), tensor(0.4808), tensor(0.5124), tensor(0.5325), tensor(0.5340), tensor(0.5340), tensor(0.5161), tensor(0.5239), tensor(0.5356), tensor(0.5962), tensor(0.5803), tensor(0.5935), tensor(0.5615), tensor(0.6124), tensor(0.6131), tensor(0.6133), tensor(0.6104), tensor(0.6294), tensor(0.6365), tensor(0.6138), tensor(0.6437)]]
test_loss_algo= [[4.712584960232874, 2.2577119906237173, 2.066732847007217, 1.9868467727284522, 1.8829973875337345, 1.7871983362610933, 1.765380803187182, 1.6675095785954954, 1.6008017237778682, 1.6248343886843153, 1.6014622001890924, 1.5427323192547842, 1.5055756523351, 1.439717079423795, 1.4008278087445885, 1.400401403949519, 1.4023323074267928, 1.3621433479770733, 1.3579678801214619, 1.3042088454696024, 1.2797753769121352, 1.2882493977334089, 1.2343484607471782, 1.2749407181314603, 1.205548046121172, 1.1786283277402259, 1.15552059935916, 1.128683248902582, 1.12576556281679, 1.1722627210009628], [4.712584960232874, 2.2287592432301517, 1.8332267961684305, 1.6540480996393094, 1.6106420520004954, 1.6190113558131418, 1.7278904239083552, 1.480092744159091, 1.4183338645157542, 1.4349977863822014, 1.447537458626328, 1.465616460818394, 1.4150706734626917, 1.311214051428874, 1.2918752257231694, 1.314674132568821, 1.2432496323706999, 1.201734263046532, 1.2284522493174121, 1.1625094330234893, 1.2131463581589377, 1.1935912484575988, 1.067363638407106, 1.0540859581558568, 1.0892413330685562, 1.0920475280968247, 0.9898348067216812, 1.0923190781265308, 1.0448943486638889, 0.988098732605102], [4.712584960232874, 2.6941026320123367, 2.120507409618159, 2.2012550367671215, 2.0445232155976023, 1.6370799647774665, 1.9548641253428853, 1.7622779607772827, 2.054668383233866, 1.9043697308582865, 1.5611974327427567, 1.7295258789305474, 1.4517227153109897, 1.6073569689586664, 1.5206825118155995, 2.6132528022596033, 1.7014718777055193, 1.6776440940844786, 1.523049408463156, 1.5789326224357458, 1.3823848381923263, 1.6446323523855513, 1.678253309741901, 1.2406466565314371, 1.4149219374747792, 1.4215017538161794, 1.2550730272463173, 1.5215306004900842, 1.3830306237670267, 1.0730692516466616], [4.712584960232874, 1.9445894083399682, 1.78426135649347, 1.7280338250907363, 1.6609182881701523, 1.5477027422303606, 1.5896990063843455, 1.4949120624809509, 1.494162457763769, 1.4225403563991474, 1.5171846325989742, 1.38257911023061, 1.3205922553493719, 1.3134193355870094, 1.3257369080166908, 1.4818135014005527, 1.4508263282715135, 1.3474304900047884, 1.15385123233127, 1.2185051019783992, 1.1919595868724167, 1.2748725061204023, 1.1183683895001746, 1.107577633705868, 1.1474147898376368, 1.117143232731303, 1.0792953565621832, 1.057762189655547, 1.1260909616567527, 1.0097566824050466]]
global_train_loss_algo = [[], [], [], []]
