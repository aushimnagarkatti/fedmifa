import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist'#'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.1],#[1/np.power(10,2.5)],
    1: [0.08],#[1/np.power(10,2)],
    2: [0.08]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.9
    , #factor to reduce lr in scheduler
    1:1,
    2:1
    }

sch_freq = 20 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
           # 1: "UMIFA",
           # 2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.700996068318685, 3.4915138332049045, 3.341530593554179, 2.9644019783337905, 2.690746192932129, 2.3726725079218545, 2.1111994660695395, 2.2936641759872436, 2.4053073445955913, 2.2358655862808225, 2.0664964140256243, 2.11030841588974, 2.35890989335378, 2.1376135285695392, 2.198884033362071, 1.9350972871780396, 2.3538231031099954, 2.2745639212926227, 2.411888215144475, 2.3212660438219705, 2.290184475580851, 2.1436750798225406, 2.0975116939544676, 2.288098802566528, 2.5559248224894207, 2.2295950012207033, 2.372287243843078, 2.385378025372823, 2.552516057014465, 2.3333356030782064, 2.629732931772868, 2.59209370195298, 2.484139572620392, 2.7209245936075845, 2.7182438948949175, 2.6671290736198423, 2.6890013519922893, 2.58361580212911, 2.727166272799174, 2.6203677900632223, 2.7860236538251244, 2.5037577374776205, 2.80191517384847, 2.8679578828811643, 2.869684491157532, 2.7543595606486, 3.609496647834778, 3.18845906384786, 3.1293419100443525, 3.061683706760406, 3.1493811098734534, 3.044664303143819, 3.277135596275329, 3.0087726259231564, 3.1997879783312477, 3.3253477032979326, 3.245579473972321, 3.1426090097427366, 3.341617358525594, 3.1155098209381102]]
Acc algo: [[tensor(0.), tensor(0.0156), tensor(0.0312), tensor(0.0312), tensor(0.0469), tensor(0.0625), tensor(0.1250), tensor(0.0938), tensor(0.2656), tensor(0.2031), tensor(0.2031), tensor(0.1719), tensor(0.2969), tensor(0.2188), tensor(0.2188), tensor(0.2344), tensor(0.3125), tensor(0.2969), tensor(0.2344), tensor(0.2656), tensor(0.2500), tensor(0.2500), tensor(0.2969), tensor(0.2656), tensor(0.2500), tensor(0.2812), tensor(0.2656), tensor(0.2812), tensor(0.2344), tensor(0.2188), tensor(0.1719), tensor(0.1562), tensor(0.1250), tensor(0.1094), tensor(0.1094), tensor(0.0938), tensor(0.0938), tensor(0.0938), tensor(0.1094), tensor(0.0938), tensor(0.0781), tensor(0.0781), tensor(0.0938), tensor(0.0938), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1094), tensor(0.1094), tensor(0.1094), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1250), tensor(0.1094)]]
test_loss_algo: [[4.132786750793457, 4.298520088195801, 5.337371349334717, 4.022491455078125, 3.896617889404297, 3.0734267234802246, 2.8535373210906982, 2.839292526245117, 2.482288122177124, 2.5844969749450684, 2.602783203125, 2.9104561805725098, 2.389273166656494, 2.3471083641052246, 2.7050302028656006, 2.7951107025146484, 2.4935739040374756, 2.3914196491241455, 2.532402992248535, 2.7403948307037354, 2.7721340656280518, 2.4981095790863037, 2.3240628242492676, 2.2767696380615234, 2.376518487930298, 2.403477430343628, 2.375396966934204, 2.3861048221588135, 2.4389679431915283, 2.5309433937072754, 2.6374430656433105, 2.7498538494110107, 2.8628740310668945, 2.9687623977661133, 3.0225508213043213, 3.054971218109131, 3.076021194458008, 3.1136598587036133, 3.153502941131592, 3.168147087097168, 3.167179822921753, 3.155681610107422, 3.141573667526245, 3.121924877166748, 3.100454092025757, 3.091414451599121, 3.0842504501342773, 3.0818262100219727, 3.083204984664917, 3.0890140533447266, 3.096724510192871, 3.1057472229003906, 3.11428165435791, 3.127135753631592, 3.144012451171875, 3.1639442443847656, 3.1872076988220215, 3.211315393447876, 3.2442126274108887, 3.279160737991333]]
global_train_loss_algo: [[]]
