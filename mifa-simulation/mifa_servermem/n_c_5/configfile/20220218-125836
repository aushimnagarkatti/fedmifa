import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1500 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2)],
    1: [1/np.power(10,1.5)],
    2: [1/np.power(10,2)],
    3: [1/np.power(10,1.5)],
    4: [1/np.power(10,1)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1,
    4:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            # 0: "MIFA",
            # 1: "UMIFA",
            # 2: "FedAvg",
            # 3: "scaffold",
            4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[1.5452108830213547, 1.162261090669781, 0.8020630457495281, 0.8188170604780316, 0.8712645125389098, 0.6561672464670847, 0.8318985007464654, 0.6740482747065835, 0.7810125270293793, 0.8039674410969019, 0.5958920560299885, 0.5518855786963831, 0.5782201064564287, 0.6087470295581443, 0.40732335982156653, 0.3464647023030557, 0.5729267211258412, 0.44921018903085497, 0.5572686021402478, 0.34331155018880966, 0.2756906873875414, 0.33284558244515206, 0.5483645052462818, 0.4841971552465111, 0.6271279146149753, 0.4584418168915727, 0.3482373321382329, 0.2523914546419019, 0.5173668651841581, 0.49853041641414164, 0.4246253905029153, 0.2774423737468896, 0.6637646891176701, 0.3260775468684733, 0.3699312356021255, 0.46906343239359555, 0.30362293640384447, 0.36964123580604796, 0.43262245901394636, 0.2821978287125239, 0.38149746078066527, 0.5595478524360805, 0.4920837745629251, 0.39727308343572076, 0.39049598903395233, 0.25152079814346506, 0.5565199057385326, 0.47771914139389987, 0.3822872542595724, 0.30159433688735593, 0.48350672421976926, 0.2814575556269847, 0.40982252062414776, 0.3293246088444721, 0.4498758247448132, 0.33979218465741723, 0.36863162748719336, 0.4722845852971659, 0.4362310701781825, 0.30599089886411096, 0.31704042344470507, 0.4219086029846221, 0.40532245993177635, 0.40557376627461056, 0.3122707856044872, 0.4386129446257837, 0.20063506470294673, 0.4456479549255164, 0.3045026663457975, 0.46570944580802465, 0.46567986356618346, 0.41198415221471807, 0.33767544136382643, 0.4295041299797594, 0.5018735102564096]]
acc_algo = [[tensor(0.1719), tensor(0.0938), tensor(0.1562), tensor(0.2188), tensor(0.3594), tensor(0.1875), tensor(0.2344), tensor(0.1719), tensor(0.3281), tensor(0.3125), tensor(0.2656), tensor(0.2969), tensor(0.3125), tensor(0.2969), tensor(0.2812), tensor(0.3281), tensor(0.4219), tensor(0.4531), tensor(0.3906), tensor(0.4062), tensor(0.4375), tensor(0.4375), tensor(0.4688), tensor(0.4375), tensor(0.4688), tensor(0.3906), tensor(0.4531), tensor(0.4531), tensor(0.5312), tensor(0.3906), tensor(0.4219), tensor(0.3594), tensor(0.3125), tensor(0.2969), tensor(0.3750), tensor(0.3906), tensor(0.4219), tensor(0.3438), tensor(0.4062), tensor(0.4688), tensor(0.4531), tensor(0.3906), tensor(0.4219), tensor(0.4375), tensor(0.4219), tensor(0.4375), tensor(0.3594), tensor(0.4062), tensor(0.3594), tensor(0.4062), tensor(0.4375), tensor(0.3438), tensor(0.3906), tensor(0.4375), tensor(0.4688), tensor(0.4375), tensor(0.3750), tensor(0.3750), tensor(0.3281), tensor(0.4531), tensor(0.5000), tensor(0.4375), tensor(0.3594), tensor(0.4531), tensor(0.4062), tensor(0.4531), tensor(0.4375), tensor(0.4375), tensor(0.4062), tensor(0.3906), tensor(0.3594), tensor(0.4844), tensor(0.4844), tensor(0.3750), tensor(0.4844)]]
test_loss_algo= [[2.2990236282348633, 2.361543655395508, 2.2464540004730225, 2.123115062713623, 2.0555670261383057, 2.0550806522369385, 2.1154065132141113, 2.0960469245910645, 1.9076581001281738, 1.9758884906768799, 1.9780972003936768, 1.8995198011398315, 1.9170794486999512, 1.879275918006897, 1.8656270503997803, 1.8429228067398071, 1.6840341091156006, 1.6685051918029785, 1.7164109945297241, 1.6196810007095337, 1.68763267993927, 1.657907247543335, 1.627002477645874, 1.6932002305984497, 1.5413001775741577, 1.6682615280151367, 1.5766628980636597, 1.678391933441162, 1.5320091247558594, 1.6885371208190918, 1.7876379489898682, 1.7377266883850098, 1.8882406949996948, 1.9562554359436035, 1.7822662591934204, 1.7565289735794067, 1.672635555267334, 1.8573102951049805, 1.7865568399429321, 1.6440790891647339, 1.613835096359253, 1.7572890520095825, 1.812286376953125, 1.8794597387313843, 1.5829288959503174, 1.780845284461975, 1.7488441467285156, 1.8372291326522827, 2.080749750137329, 1.9831562042236328, 1.5860302448272705, 1.7620207071304321, 1.8300387859344482, 1.7484729290008545, 1.556064486503601, 1.596817970275879, 1.7268306016921997, 1.8276063203811646, 2.0237836837768555, 1.7375775575637817, 1.5981860160827637, 1.8234000205993652, 1.8472601175308228, 1.664025068283081, 1.7268462181091309, 1.702790379524231, 1.81753408908844, 1.8815597295761108, 1.8374311923980713, 1.7104369401931763, 1.7015577554702759, 1.6828806400299072, 1.5709062814712524, 1.6568968296051025, 1.555593729019165]]
global_train_loss_algo = [[]]
