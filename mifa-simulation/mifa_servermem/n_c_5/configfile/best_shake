import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare' #emnist'#cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.02],#[1/np.power(10,2.5)],
    1: [0.02],#[1/np.power(10,2)],
    2: [0.02]#[1/np.power(10,2)]
    }

lrfactor = {
    0:1
    , #factor to reduce lr in scheduler
    1:1,
    2:1
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[4.675437043324298, 2.9169588703067824, 2.815089005661231, 2.4802452349755133, 2.401510807528802, 2.5128662911744604, 2.1853028071163005, 2.2143075669680496, 2.1641380609544556, 2.076255402133248, 2.002071281318851, 1.9227259856589296, 2.0002082259517406, 1.9595653725922169, 1.9071826765902338, 1.849115042844862, 1.8721918187684878, 1.849252776172738, 1.832857019531021, 1.8503353791507589, 1.8494769869598577, 1.8271771109085748, 1.7786292857883912, 1.7604092909059825, 1.7838056080081166, 1.733771927074864, 1.7322498348099207, 1.6865683913097602, 1.7341353244844846, 1.70711397535134, 1.768897732382112, 1.6819331186802888, 1.7793309062111242, 1.6794880723973684, 1.6834597829334874, 1.689553850818172, 1.7272693064541413, 1.7715274091037227, 1.6643768565705859, 1.7270201521063178, 1.778690623903152, 1.6405027241222967, 1.6964596458183177, 1.6647423921108246, 1.7363288380727173, 1.7548188157346511, 1.6313696920890088, 1.6168246203859646, 1.6890461326360597, 1.5934432466560966], [4.675160933509867, 2.417057699746099, 2.3610972316713434, 2.0865242625309235, 2.0719559871133724, 2.1191507704141896, 1.8882807490943112, 1.944748519552677, 1.8836404335923, 1.8039420568372737, 1.7515019148229474, 1.694434519771803, 1.7810994618227867, 1.7688438553734849, 1.698129589524104, 1.6993922861198651, 1.6833159694293016, 1.638335319737756, 1.6240526439533784, 1.6559283335593205, 1.6500369481735107, 1.6361763956671056, 1.5841997166378448, 1.5755014243996333, 1.5617499108805293, 1.5290715271005757, 1.5327720668800757, 1.4926143235642222, 1.4949272408762038, 1.5031963713227954, 1.5877973746343137, 1.4862985623045608, 1.6099803658738547, 1.4924703502720171, 1.4857902112909662, 1.471583059231558, 1.522893651174854, 1.5811327630205885, 1.4438464399076032, 1.5141033083303574, 1.620359039913081, 1.4342253990192788, 1.486659781233071, 1.4110442921744455, 1.483439678736031, 1.5128987623807713, 1.4029215786004392, 1.3978135698863439, 1.473330323639662, 1.3896514107387492], [4.675950061351007, 2.444189250770657, 2.3589555593089147, 2.082440680010527, 2.0487669996378153, 2.0933537958343744, 1.881913500164877, 1.9459593500372538, 1.8714588712062294, 1.7998759620717208, 1.7508020977951673, 1.6750946138924205, 1.7786885344296035, 1.7463098055070119, 1.6915894169182333, 1.7024379685277773, 1.6768199824660712, 1.6411388064322927, 1.6304189988388298, 1.660034996978413, 1.6456458003317906, 1.6252653646177613, 1.5989856421658704, 1.5661789554225076, 1.5693606535414761, 1.529931059877343, 1.5464072323391567, 1.4935070364428928, 1.5267301897489949, 1.5000289188365517, 1.5540495103897074, 1.4774304077731935, 1.6108936481777092, 1.4858596995817284, 1.4384180762856558, 1.4793664653998122, 1.5398082383531202, 1.5855312323472175, 1.4299551127482062, 1.509139457318478, 1.6550899122537406, 1.4427638277269001, 1.4691480632256018, 1.4252629971080355, 1.5074378859798112, 1.5349522626314407, 1.3880230937626514, 1.4231390346771196, 1.4747431130595985, 1.390696912745152]]
Acc algo: [[tensor(0.), tensor(0.1875), tensor(0.2656), tensor(0.2969), tensor(0.3438), tensor(0.2812), tensor(0.2656), tensor(0.3281), tensor(0.3906), tensor(0.3906), tensor(0.3906), tensor(0.4062), tensor(0.4219), tensor(0.4531), tensor(0.4375), tensor(0.4531), tensor(0.4375), tensor(0.4219), tensor(0.4375), tensor(0.4375), tensor(0.4688), tensor(0.4062), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4688), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.5000), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.4844), tensor(0.5156), tensor(0.4844), tensor(0.5000), tensor(0.5469), tensor(0.5000), tensor(0.5625)], [tensor(0.1875), tensor(0.3281), tensor(0.2812), tensor(0.3750), tensor(0.3906), tensor(0.4375), tensor(0.4531), tensor(0.4688), tensor(0.4688), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.4375), tensor(0.4688), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.5625), tensor(0.5312), tensor(0.5156), tensor(0.5312), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5000), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.5781), tensor(0.5625), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5938), tensor(0.5938), tensor(0.5938), tensor(0.6094), tensor(0.5781), tensor(0.5938), tensor(0.5938), tensor(0.6094), tensor(0.6250)], [tensor(0.1875), tensor(0.3438), tensor(0.3281), tensor(0.3594), tensor(0.4062), tensor(0.4531), tensor(0.4219), tensor(0.4375), tensor(0.4531), tensor(0.5312), tensor(0.4844), tensor(0.5000), tensor(0.5312), tensor(0.5156), tensor(0.5000), tensor(0.5000), tensor(0.5156), tensor(0.5000), tensor(0.5000), tensor(0.5156), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5156), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5469), tensor(0.5781), tensor(0.4844), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5625), tensor(0.5469), tensor(0.5781), tensor(0.5625), tensor(0.5938), tensor(0.5781)]]
test_loss_algo: [[4.826113700866699, 3.5712838172912598, 2.734816551208496, 2.631180763244629, 2.3575546741485596, 2.325777769088745, 2.3312954902648926, 2.1422080993652344, 2.1681454181671143, 2.0353267192840576, 2.033547878265381, 1.9407068490982056, 1.8953689336776733, 1.82086980342865, 1.7828224897384644, 1.7964158058166504, 1.7505759000778198, 1.783532977104187, 1.7559232711791992, 1.7997212409973145, 1.7354331016540527, 1.8081941604614258, 1.7540042400360107, 1.7634689807891846, 1.7610249519348145, 1.7308405637741089, 1.7136456966400146, 1.6993435621261597, 1.7041252851486206, 1.6952067613601685, 1.6937335729599, 1.6901639699935913, 1.68053138256073, 1.6823917627334595, 1.6856435537338257, 1.6892873048782349, 1.7006101608276367, 1.6687690019607544, 1.671473741531372, 1.679102897644043, 1.667540192604065, 1.684074878692627, 1.6333750486373901, 1.6599446535110474, 1.6248126029968262, 1.6379766464233398, 1.6322274208068848, 1.592365026473999, 1.6230210065841675, 1.5916152000427246], [4.536052703857422, 2.3980705738067627, 2.2523369789123535, 2.0672214031219482, 1.9792593717575073, 1.9134949445724487, 1.849575161933899, 1.7848938703536987, 1.7264529466629028, 1.7306535243988037, 1.720293641090393, 1.6948354244232178, 1.711350679397583, 1.6934171915054321, 1.670214056968689, 1.6751949787139893, 1.6304570436477661, 1.6202961206436157, 1.6202588081359863, 1.5407938957214355, 1.5658822059631348, 1.5824456214904785, 1.543264389038086, 1.5867974758148193, 1.557469129562378, 1.5709503889083862, 1.5250027179718018, 1.5243579149246216, 1.5329740047454834, 1.5227445363998413, 1.5025533437728882, 1.4840015172958374, 1.5068045854568481, 1.492671251296997, 1.442152500152588, 1.4467148780822754, 1.4753395318984985, 1.4635345935821533, 1.4471255540847778, 1.466339111328125, 1.4755772352218628, 1.4500207901000977, 1.445799469947815, 1.4319124221801758, 1.4186853170394897, 1.4362844228744507, 1.4371163845062256, 1.417258620262146, 1.4127204418182373, 1.4086544513702393], [4.537514686584473, 2.40942120552063, 2.198362112045288, 2.043233633041382, 1.9286984205245972, 1.889589548110962, 1.8455252647399902, 1.8045886754989624, 1.7972934246063232, 1.7522852420806885, 1.7339048385620117, 1.6691402196884155, 1.6468913555145264, 1.65956711769104, 1.6770193576812744, 1.681095004081726, 1.6697657108306885, 1.6202346086502075, 1.6796149015426636, 1.6253994703292847, 1.6107810735702515, 1.538401484489441, 1.5764539241790771, 1.572341799736023, 1.5852242708206177, 1.5276389122009277, 1.5704715251922607, 1.5594115257263184, 1.5952165126800537, 1.5415772199630737, 1.5219662189483643, 1.5010427236557007, 1.5610084533691406, 1.5386487245559692, 1.5017282962799072, 1.4939111471176147, 1.5141382217407227, 1.5254526138305664, 1.4986023902893066, 1.5197645425796509, 1.519037127494812, 1.5252406597137451, 1.5633599758148193, 1.4941436052322388, 1.548941731452942, 1.5004286766052246, 1.4814808368682861, 1.5235337018966675, 1.4899358749389648, 1.5400153398513794]]
global_train_loss_algo: [[], [], []]
