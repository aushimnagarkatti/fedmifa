import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = False


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2.5)],
    1: [1/np.power(10,2)],
    2: [1/np.power(10,2)],
    3: [0.01]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[2.298438487052917, 2.2265926933288576, 1.1115352666232319, 0.8271017294004558, 0.8346769353747368, 0.73162997180596, 0.6827447206899524, 0.8741764172911644, 0.6694598498940468, 0.4081033090595156, 0.7565724039077758, 0.6358267503976822, 0.5494093345850707, 0.563079024925828, 0.7913212306797505, 0.41888608490116896, 0.5540776341874152, 0.5708790896832944, 0.2548360870312899, 0.47848834024742254], [2.2224162960052487, 0.9069564971327783, 0.5501063215360046, 0.7194293198385276, 0.8993788680434227, 0.724407829446718, 0.7547649746760726, 0.8247268117964269, 0.7612934964895249, 0.4549818550911732, 0.8202092990279197, 0.7141912169754505, 0.601220036521554, 0.6308493747562169, 0.8536754482984543, 0.4855152214551344, 0.6056369596812874, 0.714282839372754, 0.33465639512985945, 0.5262905764719472], [2.2231094896793366, 0.7194700315594672, 0.5018133426271378, 0.6380503024021164, 0.6367393835261465, 0.42133563382085415, 0.4822376810014248, 0.6154240015521645, 0.5998396104574203, 0.28716599092818795, 0.6656873048469425, 0.4501824242249131, 0.43421042289584877, 0.38200103851035233, 0.5794813164323569, 0.28698987254174424, 0.37451195109111723, 0.43122701555257664, 0.18470859458204358, 0.33236276386538527], [2.222598315477371, 0.8114127048850059, 0.5207223778404295, 0.7061529831960798, 0.8271139685809613, 0.5193488827720284, 0.6566085501387715, 0.8071925441175699, 0.7057735482603311, 0.39462938656099145, 1.0204006053507328, 0.6713525830954313, 0.5677689874544739, 0.6915799115598202, 0.9250821307301521, 0.5153329148143531, 0.5466340086050332, 0.7117429834604263, 0.34751917034387586, 0.6098752710642292]]
Acc algo: [[tensor(0.0938), tensor(0.1094), tensor(0.1719), tensor(0.2500), tensor(0.3438), tensor(0.3281), tensor(0.3594), tensor(0.3281), tensor(0.3281), tensor(0.3438), tensor(0.3438), tensor(0.3750), tensor(0.3906), tensor(0.4375), tensor(0.5156), tensor(0.4688), tensor(0.4219), tensor(0.5156), tensor(0.5000), tensor(0.5156)], [tensor(0.0938), tensor(0.0625), tensor(0.2812), tensor(0.2656), tensor(0.3750), tensor(0.3594), tensor(0.3125), tensor(0.3125), tensor(0.3281), tensor(0.3281), tensor(0.2969), tensor(0.3281), tensor(0.3750), tensor(0.4062), tensor(0.3594), tensor(0.2969), tensor(0.4375), tensor(0.3750), tensor(0.3750), tensor(0.3906)], [tensor(0.1094), tensor(0.1406), tensor(0.2344), tensor(0.1406), tensor(0.2188), tensor(0.1094), tensor(0.3750), tensor(0.2344), tensor(0.2656), tensor(0.4062), tensor(0.4844), tensor(0.3281), tensor(0.5312), tensor(0.5469), tensor(0.3906), tensor(0.4688), tensor(0.5156), tensor(0.2812), tensor(0.4844), tensor(0.4375)], [tensor(0.1094), tensor(0.1406), tensor(0.2344), tensor(0.1719), tensor(0.1875), tensor(0.1562), tensor(0.4219), tensor(0.2969), tensor(0.2031), tensor(0.4062), tensor(0.4219), tensor(0.3906), tensor(0.4844), tensor(0.5312), tensor(0.4375), tensor(0.4688), tensor(0.4219), tensor(0.3281), tensor(0.5469), tensor(0.5312)]]
test_loss_algo: [[2.299321174621582, 2.2815194129943848, 3.241586685180664, 2.1989731788635254, 2.1767804622650146, 1.9456167221069336, 1.8856921195983887, 1.8234938383102417, 1.8215688467025757, 1.7581266164779663, 1.7340856790542603, 1.6984846591949463, 1.6384423971176147, 1.5782909393310547, 1.5278517007827759, 1.4969964027404785, 1.476322889328003, 1.4460783004760742, 1.4372133016586304, 1.4520268440246582], [2.299588680267334, 2.3461191654205322, 2.082920789718628, 2.040287971496582, 1.9726841449737549, 1.9027392864227295, 1.8984001874923706, 1.8795262575149536, 1.8455156087875366, 1.8051985502243042, 1.801416277885437, 1.745778203010559, 1.7099637985229492, 1.7359055280685425, 1.6595031023025513, 1.6914259195327759, 1.6000703573226929, 1.6143274307250977, 1.5887662172317505, 1.6072845458984375], [2.30301833152771, 2.0654616355895996, 2.206658124923706, 2.273552417755127, 2.0963945388793945, 3.554344654083252, 1.741518259048462, 2.2354164123535156, 2.0827436447143555, 1.7393484115600586, 1.6775968074798584, 1.9829987287521362, 1.5405609607696533, 1.2969248294830322, 1.9789116382598877, 1.4834476709365845, 1.6119623184204102, 2.003124713897705, 1.6419364213943481, 1.5591200590133667], [2.30342698097229, 2.0828304290771484, 2.126689910888672, 2.050952434539795, 1.909531593322754, 2.2937283515930176, 1.667529582977295, 1.8898369073867798, 1.9348113536834717, 1.672303557395935, 1.5266839265823364, 1.7864124774932861, 1.498173475265503, 1.3260304927825928, 1.7156094312667847, 1.4873543977737427, 1.6628198623657227, 1.784868836402893, 1.3242762088775635, 1.344516396522522]]
global_train_loss_algo: [[2.2941896634943344, 2.312263373828605, 2.881731783032722, 2.2333722681645543, 2.2040520656444227, 2.030789838727478, 1.985377649364569, 1.9453244802287168, 1.8914243391407726, 1.8329124924776805, 1.7670688368475345, 1.754187897648043, 1.6846810032034774, 1.6203000109518886, 1.6072692714078958, 1.607872589012546, 1.578940248093032, 1.500357698296647, 1.4779236719126592, 1.4246459885326492], [2.2950855130734653, 2.3239121467561064, 2.1036629442058867, 2.0484492251330324, 2.038413359076166, 1.971290023430534, 1.92637082088329, 1.9067975654626441, 1.841742759317998, 1.8588581882474366, 1.7574632888864679, 1.7139856646128018, 1.69339380971611, 1.6549360521918977, 1.666404972600815, 1.6527660315298973, 1.6399265555164697, 1.613928795775489, 1.648379556358318, 1.541756797629549], [2.2967906031767122, 2.3205979608208933, 2.0121685874736523, 2.1353671273307118, 1.8172697940141038, 2.7185774754990093, 1.775205802277226, 1.8146939327954636, 1.6631082383263143, 2.052040686387845, 1.6373186295904467, 1.8838321395847193, 1.3827340970258883, 1.3338079931181106, 2.049194988082437, 1.2428904408993928, 1.5895186304436315, 1.5181708299290493, 1.9132478104527955, 1.3003658469375747], [2.2963724547944717, 2.210575587609235, 1.9915948135163777, 2.0400227796086265, 1.6610537035690853, 2.011693925046555, 1.7404392413471057, 1.6122758931211194, 1.6189614221872881, 1.8299386155269945, 1.6800605398614694, 1.692290371183849, 1.5281216668350923, 1.5393146256656598, 1.757279370142066, 1.475268436819696, 1.7284510737794745, 1.478582829618088, 1.5688627801282937, 1.3747811573545645]]
