import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare' #emnist'#cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.05],#[1/np.power(10,2.5)],
    1: [0.05],#[1/np.power(10,2)],
    2: [0.05]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.5, #factor to reduce lr in scheduler
    1:0.5,
    2:0.5
    }

sch_freq = 300 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[4.293493001615387, 2.8088494262305224, 2.681035958768317, 2.4550293779403227, 2.391775986931102, 2.264597713115837, 2.1543780599135856, 2.0780922690186143, 2.1124777080916712, 2.003513933215712, 1.968270009561079, 2.136699334852623, 1.9540772578176735, 1.918610687398291, 1.8048623391219547, 1.8358226224354337, 1.8151217464332117, 1.87526899778615, 1.8256210421153178, 1.8514273035024775, 1.7654488033502524, 1.9379446928501127, 1.7575397443716931, 1.9036129115326699, 1.7666943049126718, 1.8680272058693432, 1.8010815822302315, 1.8044140329286649, 1.735247797136105, 1.8465263467346489, 1.7931729121316908, 1.7411376390576792, 1.7613322650534766, 1.7509772783452298, 1.7816585373634446, 1.7136792324317824, 1.7776249609987058, 1.736968145408032, 1.8498705378380076, 1.8711233435349583, 1.7452454671875555, 1.8173485283321806, 1.7559690182747623, 1.7954297233107706, 1.7861101448719316, 1.6883967868522276, 1.7654258918381172, 1.736505435503752, 1.769633547957061, 1.772845677940457], [4.290107253623078, 2.3267643071493094, 2.258854114319358, 2.0741260390999012, 1.9968131484425506, 1.8835645218890587, 1.8261889072114772, 1.8316869390724915, 1.8081967048662235, 1.710436096364187, 1.7038281628000302, 1.8412653390814218, 1.7003180681228636, 1.68691574063652, 1.562157924880197, 1.5938645776446794, 1.591175424975459, 1.6389630139452624, 1.6008457385052701, 1.6248784719066887, 1.5771829659054173, 1.7178253802891934, 1.549408816351967, 1.6938830369876618, 1.5643852204013773, 1.6890935249369368, 1.5765075007621594, 1.587875408720723, 1.5193807599623022, 1.6595140336967362, 1.5867869494721307, 1.5382613135007197, 1.536406335717156, 1.561606583027266, 1.5833763881920988, 1.512215606320988, 1.5815466057002834, 1.5393746072182737, 1.5638259928785978, 1.6768706778114677, 1.5550313479430422, 1.6323853380944993, 1.5694776399480612, 1.6143101639451292, 1.5957637731949768, 1.5029415811486235, 1.6045151045823207, 1.553883916223334, 1.5635346331582594, 1.5833713835627485], [4.28997321152273, 2.311092450176549, 2.250160192945134, 2.074241137945332, 2.001667121245954, 1.8916856253035816, 1.8037659559033135, 1.8237527028341436, 1.8201608857254903, 1.701576258331856, 1.7122434073057717, 1.8463227921899779, 1.6978905417649552, 1.6772626150917005, 1.5636251227091917, 1.6351102469842307, 1.590479808901432, 1.6373934612132097, 1.6051152546772536, 1.6298035140449194, 1.5664481276798932, 1.7044065568085873, 1.5467511337984683, 1.6974232713127069, 1.5734287684199935, 1.6859400491361263, 1.5767274240023266, 1.5985461535280325, 1.5218769316070682, 1.6509798024066231, 1.5750981825416868, 1.5392809112956058, 1.5697825365123295, 1.5625602697391563, 1.5707481960475707, 1.5156165897617437, 1.5954455155251788, 1.541730788063573, 1.5741253543510065, 1.679820540817375, 1.552293656528032, 1.6390579167471995, 1.5727579163681233, 1.59375854335491, 1.5919138493779657, 1.5058322098732777, 1.5681897522099488, 1.553671757727909, 1.5616631954397655, 1.5918052210647697]]
Acc algo: [[tensor(0.), tensor(0.1875), tensor(0.2031), tensor(0.2812), tensor(0.2500), tensor(0.2969), tensor(0.3125), tensor(0.3281), tensor(0.3438), tensor(0.4375), tensor(0.4219), tensor(0.3750), tensor(0.4062), tensor(0.4375), tensor(0.4844), tensor(0.4688), tensor(0.4531), tensor(0.4219), tensor(0.4531), tensor(0.4531), tensor(0.4688), tensor(0.4688), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.5000), tensor(0.4844), tensor(0.4844), tensor(0.4688), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.5156), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.5156)], [tensor(0.1875), tensor(0.2500), tensor(0.3281), tensor(0.4531), tensor(0.4531), tensor(0.4062), tensor(0.4688), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5312), tensor(0.5156), tensor(0.5781), tensor(0.5469), tensor(0.5781), tensor(0.5625), tensor(0.5781), tensor(0.5938), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5938), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.6094), tensor(0.5781), tensor(0.5781), tensor(0.5781), tensor(0.5781)], [tensor(0.1875), tensor(0.2656), tensor(0.3438), tensor(0.4375), tensor(0.4531), tensor(0.4531), tensor(0.4844), tensor(0.4219), tensor(0.4531), tensor(0.5000), tensor(0.4844), tensor(0.4688), tensor(0.4844), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5156), tensor(0.5625), tensor(0.5938), tensor(0.5156), tensor(0.5625), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5938), tensor(0.5938), tensor(0.5938), tensor(0.5938), tensor(0.5781), tensor(0.5625), tensor(0.5938), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5938), tensor(0.5781), tensor(0.5938), tensor(0.5938), tensor(0.5938), tensor(0.5781), tensor(0.6094), tensor(0.5938), tensor(0.5938), tensor(0.5781), tensor(0.5625), tensor(0.5625)]]
test_loss_algo: [[4.859468936920166, 3.573383092880249, 2.7286624908447266, 2.760676860809326, 2.4526867866516113, 2.2850067615509033, 2.223238706588745, 2.1937148571014404, 2.0683329105377197, 2.03550386428833, 1.994894027709961, 1.9385931491851807, 1.9186327457427979, 1.840503454208374, 1.8271920680999756, 1.8303875923156738, 1.7961950302124023, 1.8355344533920288, 1.7296205759048462, 1.7805124521255493, 1.737179160118103, 1.7361880540847778, 1.7374078035354614, 1.725965976715088, 1.7371023893356323, 1.7082529067993164, 1.722475528717041, 1.6972516775131226, 1.7114462852478027, 1.6999247074127197, 1.7007620334625244, 1.7107901573181152, 1.6791082620620728, 1.6957025527954102, 1.6818983554840088, 1.697055697441101, 1.6933695077896118, 1.6882425546646118, 1.6907122135162354, 1.6943179368972778, 1.6830798387527466, 1.6942453384399414, 1.6803451776504517, 1.6870590448379517, 1.689367413520813, 1.6707695722579956, 1.690328598022461, 1.6722984313964844, 1.6786537170410156, 1.6785966157913208], [3.9251961708068848, 2.4541127681732178, 2.242934226989746, 2.0235424041748047, 1.880238652229309, 1.856488823890686, 1.7859750986099243, 1.781936526298523, 1.6936346292495728, 1.6643574237823486, 1.655809998512268, 1.6379828453063965, 1.6140336990356445, 1.5825746059417725, 1.5769627094268799, 1.5757122039794922, 1.5433712005615234, 1.5452544689178467, 1.5293703079223633, 1.5634726285934448, 1.5452361106872559, 1.5354958772659302, 1.5353933572769165, 1.521658182144165, 1.5128597021102905, 1.5270185470581055, 1.5278886556625366, 1.5221970081329346, 1.5059425830841064, 1.512873888015747, 1.523395299911499, 1.5250853300094604, 1.5016858577728271, 1.5133066177368164, 1.5151879787445068, 1.5178965330123901, 1.5124231576919556, 1.5092463493347168, 1.5013620853424072, 1.5111610889434814, 1.5024605989456177, 1.5057941675186157, 1.50758957862854, 1.5095728635787964, 1.5030336380004883, 1.5053956508636475, 1.50893235206604, 1.4918773174285889, 1.5002832412719727, 1.5011900663375854], [3.9203546047210693, 2.438288927078247, 2.2125790119171143, 2.022019624710083, 1.9232293367385864, 1.8777838945388794, 1.7958343029022217, 1.821833848953247, 1.7400307655334473, 1.7395235300064087, 1.7163290977478027, 1.6497491598129272, 1.6714239120483398, 1.6246225833892822, 1.5890787839889526, 1.6074638366699219, 1.5664496421813965, 1.548390507698059, 1.5624195337295532, 1.5709071159362793, 1.5369389057159424, 1.5636622905731201, 1.5476019382476807, 1.5322792530059814, 1.5450268983840942, 1.5261833667755127, 1.5151879787445068, 1.5105576515197754, 1.4776146411895752, 1.4772313833236694, 1.4667613506317139, 1.5025793313980103, 1.5416045188903809, 1.5166844129562378, 1.5098272562026978, 1.4915295839309692, 1.4976798295974731, 1.4917664527893066, 1.4961671829223633, 1.4790180921554565, 1.5134539604187012, 1.4882937669754028, 1.4911466836929321, 1.480550765991211, 1.4786800146102905, 1.4913371801376343, 1.4925644397735596, 1.499126672744751, 1.5123498439788818, 1.5205652713775635]]
global_train_loss_algo: [[], [], []]
