import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare' #emnist'#cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.05],#[1/np.power(10,2.5)],
    1: [0.05],#[1/np.power(10,2)],
    2: [0.05]#[1/np.power(10,2)]
    }

lrfactor = {
    0:1
    , #factor to reduce lr in scheduler
    1:1,
    2:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[4.368665449550346, 2.8619310410452075, 2.565001747071446, 2.1955271295271226, 2.1558875233224213, 2.2123554620922086, 1.9221767497342253, 1.8879529154482555, 1.8506678260903335, 1.7878555981447586, 1.8077163185228602, 1.669482384268235, 1.7944474338078578, 1.711122786962846, 1.7212809322183094, 1.598591342752867, 1.7894605938906871, 1.624040070534758, 1.786740632884639, 1.5282332816662687, 1.480791994440779, 1.627392115375017, 1.5828198632416766, 1.5771290904602044, 1.757461296267555, 1.5664336139774477, 1.5757602157865533, 1.712009102730115, 1.5093938133696594, 1.5301142612381757, 1.4824253631656972, 1.4994756352673098, 1.4910712349385566, 1.4995262248977108, 1.5613211979572408, 1.3596956693675915, 1.5489030132098418, 1.5627371227811995, 1.4793482778780442, 1.5378693547176343, 1.578348956805969, 1.5007554469139843, 1.4727054874558956, 1.4918910482027417, 1.4126586937706689, 1.4649401030125822, 1.5048224727573982, 1.4514667110057977, 1.477310578436341, 1.40949646621968], [4.369783757204997, 2.213032520245027, 2.0410811434811227, 1.7986393871042463, 1.6973030897530124, 1.7762410919120157, 1.579239253741021, 1.5087809192671857, 1.5164364314607337, 1.4453843598755651, 1.4992234329533554, 1.3583635460387646, 1.471717108532024, 1.4130969991132527, 1.3707544580622326, 1.2869904641760022, 1.4090021133572992, 1.3105055569151447, 1.4264125609561062, 1.2119895480801077, 1.194671931377737, 1.3391786450557523, 1.2757918503145866, 1.2640330408661207, 1.3409508502091876, 1.270840678306666, 1.2832174716630393, 1.341770025202152, 1.2396926488108295, 1.2009783384323178, 1.209345600313058, 1.1813881498738477, 1.1558558330120772, 1.2346928082054358, 1.2795057065423314, 1.069359770563391, 1.2168553012600867, 1.2409668036417663, 1.1405823675788433, 1.2090247034741939, 1.2442922107287608, 1.1867832724575735, 1.1762751603940966, 1.1969416488768414, 1.103240620252189, 1.2122191760980232, 1.2507126016320564, 1.1516639278856098, 1.193515671351604, 1.132206968966957], [4.368864805987513, 2.187161677574621, 2.0228810715521157, 1.7712837301946824, 1.6624475442587445, 1.7668596738689384, 1.5793719831504174, 1.6060082845908348, 1.519910164374171, 1.465112510021618, 1.534120412314933, 1.3848219735435412, 1.4755718519664023, 1.41431673772266, 1.3998607552524596, 1.3123203190698187, 1.4672193772786168, 1.3357099958247043, 1.4665918507205629, 1.2418365059889553, 1.2075112749644774, 1.336584251176772, 1.2907983399650464, 1.307090094125756, 1.2481472297026486, 1.3129947950763117, 1.3378259925487475, 1.3517372940849735, 1.2613491811243869, 1.2300304634254986, 1.2305967892683785, 1.2238057401995908, 1.208419748134649, 1.273058902045113, 1.332985561940795, 1.1701095713811582, 1.3491245012662092, 1.3079459099718833, 1.17928443690395, 1.2693667915466786, 1.254609069706517, 1.2418027613803069, 1.2094120824104728, 1.255150698254704, 1.168637531051187, 1.256635097598862, 1.286927984706336, 1.2147479895775135, 1.2355448507286453, 1.1755429327201112]]
Acc algo: [[tensor(0.), tensor(0.1875), tensor(0.2344), tensor(0.2656), tensor(0.3594), tensor(0.3281), tensor(0.4062), tensor(0.4688), tensor(0.5156), tensor(0.4531), tensor(0.4844), tensor(0.5156), tensor(0.4844), tensor(0.5156), tensor(0.5156), tensor(0.5156), tensor(0.5469), tensor(0.5156), tensor(0.5469), tensor(0.5312), tensor(0.5312), tensor(0.5312), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5469), tensor(0.5625), tensor(0.5312), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.5625), tensor(0.5469), tensor(0.5156), tensor(0.5469), tensor(0.5469), tensor(0.5469)], [tensor(0.1875), tensor(0.3594), tensor(0.3750), tensor(0.4531), tensor(0.5000), tensor(0.5469), tensor(0.5312), tensor(0.5000), tensor(0.5000), tensor(0.5781), tensor(0.5625), tensor(0.6094), tensor(0.6094), tensor(0.6094), tensor(0.5781), tensor(0.5938), tensor(0.5781), tensor(0.5781), tensor(0.6094), tensor(0.5625), tensor(0.5781), tensor(0.5938), tensor(0.5781), tensor(0.6094), tensor(0.5312), tensor(0.5312), tensor(0.5938), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5625), tensor(0.5938), tensor(0.6094), tensor(0.5625), tensor(0.5625), tensor(0.5781), tensor(0.6094), tensor(0.5781), tensor(0.5781), tensor(0.5625), tensor(0.6094), tensor(0.5781), tensor(0.5625), tensor(0.5938), tensor(0.5938), tensor(0.5938), tensor(0.5781), tensor(0.5938), tensor(0.5938), tensor(0.5625)], [tensor(0.1875), tensor(0.3281), tensor(0.4375), tensor(0.4688), tensor(0.5000), tensor(0.5156), tensor(0.5156), tensor(0.5625), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5469), tensor(0.5312), tensor(0.5000), tensor(0.5312), tensor(0.5000), tensor(0.5312), tensor(0.5156), tensor(0.5469), tensor(0.5938), tensor(0.5469), tensor(0.5625), tensor(0.5625), tensor(0.5469), tensor(0.5938), tensor(0.5312), tensor(0.5625), tensor(0.5469), tensor(0.5312), tensor(0.5469), tensor(0.5156), tensor(0.5312), tensor(0.5781), tensor(0.5781), tensor(0.5312), tensor(0.5469), tensor(0.5781), tensor(0.5625), tensor(0.5625), tensor(0.6250), tensor(0.5625), tensor(0.5781), tensor(0.5781), tensor(0.5156), tensor(0.5156), tensor(0.5625), tensor(0.5469), tensor(0.5312), tensor(0.5625), tensor(0.5312)]]
test_loss_algo: [[4.862006187438965, 3.4517617225646973, 2.5658786296844482, 2.4009830951690674, 2.188778877258301, 2.172126293182373, 1.9102810621261597, 1.9265891313552856, 1.7941988706588745, 1.7801026105880737, 1.7512916326522827, 1.6791096925735474, 1.710524082183838, 1.6221301555633545, 1.6356838941574097, 1.6192963123321533, 1.602489709854126, 1.6169962882995605, 1.554289698600769, 1.591328501701355, 1.573029637336731, 1.5677814483642578, 1.5542126893997192, 1.572792649269104, 1.5216232538223267, 1.5062270164489746, 1.5171703100204468, 1.4798487424850464, 1.4975935220718384, 1.4884151220321655, 1.4909436702728271, 1.4923532009124756, 1.48763108253479, 1.465510368347168, 1.5095129013061523, 1.4661582708358765, 1.499205231666565, 1.5044379234313965, 1.4559731483459473, 1.46549391746521, 1.4920413494110107, 1.4803097248077393, 1.477800965309143, 1.4797427654266357, 1.444361925125122, 1.4737703800201416, 1.4645053148269653, 1.4212620258331299, 1.4859784841537476, 1.4184424877166748], [3.976609945297241, 2.2714104652404785, 1.8780547380447388, 1.7105162143707275, 1.7423423528671265, 1.5968445539474487, 1.5992405414581299, 1.6780296564102173, 1.6422122716903687, 1.5707290172576904, 1.5655138492584229, 1.5720653533935547, 1.547806978225708, 1.5576505661010742, 1.5545223951339722, 1.5647883415222168, 1.5054963827133179, 1.5374854803085327, 1.5317096710205078, 1.5203946828842163, 1.4796360731124878, 1.5056424140930176, 1.506699562072754, 1.5196247100830078, 1.5956355333328247, 1.5187592506408691, 1.4791858196258545, 1.5787051916122437, 1.4766733646392822, 1.4683350324630737, 1.4795650243759155, 1.45943021774292, 1.473880410194397, 1.444480538368225, 1.4553781747817993, 1.4779527187347412, 1.4543721675872803, 1.4729745388031006, 1.4617557525634766, 1.4444466829299927, 1.4507980346679688, 1.4814856052398682, 1.4853308200836182, 1.4756205081939697, 1.501120686531067, 1.4328855276107788, 1.4304678440093994, 1.4515316486358643, 1.457382321357727, 1.4478530883789062], [3.971500873565674, 2.153717279434204, 1.8300622701644897, 1.6917649507522583, 1.6955832242965698, 1.6157892942428589, 1.6279100179672241, 1.6087474822998047, 1.6484506130218506, 1.5314579010009766, 1.5030111074447632, 1.536146879196167, 1.4620914459228516, 1.5033384561538696, 1.4501912593841553, 1.4974501132965088, 1.4995566606521606, 1.549761414527893, 1.4776102304458618, 1.5804126262664795, 1.5398657321929932, 1.5470219850540161, 1.4947800636291504, 1.4056569337844849, 1.4351890087127686, 1.501943826675415, 1.4157580137252808, 1.471519947052002, 1.5151175260543823, 1.503185749053955, 1.5526188611984253, 1.5919950008392334, 1.5254522562026978, 1.472153902053833, 1.4590305089950562, 1.5228674411773682, 1.4576992988586426, 1.4529101848602295, 1.3816896677017212, 1.3058773279190063, 1.4525208473205566, 1.366564154624939, 1.438769817352295, 1.396721363067627, 1.4470995664596558, 1.541369915008545, 1.4085336923599243, 1.469841718673706, 1.3987294435501099, 1.462897539138794]]
global_train_loss_algo: [[], [], []]
