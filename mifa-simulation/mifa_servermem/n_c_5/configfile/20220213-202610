import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = False


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 0
clust_mech = 'dynamic' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2)],
    1: [1/np.power(10,1.5)],
    2: [1/np.power(10,2)],
    3: [1/np.power(10,1.5)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            #0: "MIFA",
            #1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[2.250751085281372, 0.7041204327344894, 0.843026927113533, 0.6706820695102215, 0.7573753488063811, 0.7471895229816437, 0.4504629254015162, 0.33703911085845906, 0.7218388774991036, 0.5969877326861024, 0.4949642467871308, 0.6316957427561284, 0.5521288545429706, 0.5137579495459794, 0.5989594328776002, 0.5083810987696051, 0.5717410999163985, 0.3998961128643714, 0.513037212737836, 0.4754184277099557, 0.4078601766750217, 0.422429705359973, 0.35960595564916736, 0.31309003720991313, 0.48471868110820643, 0.5302720402926206, 0.4172096315189265, 0.40131339626852425, 0.4636336610559374, 0.32601083098328676, 0.48760337233543394, 0.33283920786809174, 0.5837496754527092, 0.44465079380199307, 0.3621380366606172, 0.37859608441591264, 0.46164754341647496, 0.3082488780049607, 0.43742357637733226, 0.2626944122381974, 0.25373608673573467, 0.33371183116687464, 0.35082581707276406, 0.2637986796116456, 0.27121165937627667, 0.23764643274014813, 0.3173657682957128, 0.3614428969111759, 0.4129623216018081, 0.3641674773022533], [1.8495809948444368, 0.6837616325193083, 0.8365601301193237, 0.7606801307946445, 0.8163154952228069, 0.9196038834750653, 0.5361779014505738, 0.4756571361422539, 0.9172443658113479, 0.8070056497305631, 0.5877474111318588, 0.7062877007573843, 0.8029598587751389, 0.6260520819574594, 0.7517408649623395, 0.5472290050797165, 0.7219449038803578, 0.4663285771757365, 0.5371131577715278, 0.618573066163808, 0.5377109454572201, 0.4261083805561066, 0.5336238700896502, 0.34238877850119026, 0.6783235140144825, 0.637631980329752, 0.45461647694930446, 0.44505536288954317, 0.5050897791422904, 0.4202699733339251, 0.6304884392023087, 0.4498406645469368, 0.7085084059834481, 0.5636453464627266, 0.39225236106198286, 0.4877762825414539, 0.5534589861077256, 0.35024903768207877, 0.43972324319183825, 0.5517451128549874, 0.320662046817597, 0.3747960846591741, 0.44280341882258656, 0.3865511571057141, 0.29686138550285246, 0.2755247105285525, 0.3975805309880525, 0.5544556538836333, 0.5527973429730627, 0.45459311969578264]]
acc_algo = [[0.109375 0.09375  0.171875 0.28125  0.21875  0.234375 0.125    0.375
  0.390625 0.265625 0.3125   0.34375  0.421875 0.34375  0.3125   0.46875
  0.3125   0.359375 0.25     0.4375   0.390625 0.4375   0.296875 0.296875
  0.421875 0.46875  0.46875  0.421875 0.4375   0.484375 0.453125 0.4375
  0.453125 0.3125   0.4375   0.515625 0.265625 0.328125 0.4375   0.53125
  0.359375 0.453125 0.46875  0.53125  0.421875 0.515625 0.5      0.453125
  0.515625 0.4375  ]
 [0.09375  0.046875 0.125    0.34375  0.265625 0.3125   0.234375 0.4375
  0.359375 0.421875 0.375    0.390625 0.453125 0.484375 0.421875 0.484375
  0.40625  0.34375  0.40625  0.5      0.453125 0.46875  0.40625  0.453125
  0.421875 0.4375   0.484375 0.53125  0.46875  0.375    0.515625 0.375
  0.4375   0.390625 0.40625  0.609375 0.390625 0.46875  0.453125 0.546875
  0.453125 0.46875  0.515625 0.578125 0.5      0.609375 0.5625   0.4375
  0.578125 0.4375  ]]
test_loss_algo= [[2.285350799560547, 2.5994327068328857, 2.1590070724487305, 2.1145012378692627, 2.104405641555786, 2.1094415187835693, 2.8284380435943604, 2.071051836013794, 1.9500751495361328, 1.814741849899292, 2.096893787384033, 1.7498787641525269, 1.5805878639221191, 1.8682973384857178, 1.8188259601593018, 1.671040415763855, 1.8153889179229736, 1.8840863704681396, 1.9711754322052002, 1.7391334772109985, 1.7195791006088257, 1.6327288150787354, 2.107832431793213, 2.065335273742676, 1.7828764915466309, 1.4859954118728638, 1.5998194217681885, 1.5714876651763916, 1.771209955215454, 1.528662919998169, 1.3552350997924805, 1.8528424501419067, 1.4241122007369995, 1.740312099456787, 1.7239285707473755, 1.5104176998138428, 1.9534951448440552, 1.8727589845657349, 1.7568303346633911, 1.4859504699707031, 1.857931137084961, 1.7058340311050415, 2.003357410430908, 1.4872928857803345, 1.683064341545105, 1.3676979541778564, 1.4869904518127441, 1.6321215629577637, 1.3920817375183105, 1.7109272480010986], [2.609408378601074, 2.4347496032714844, 2.29837965965271, 1.8501907587051392, 1.9089947938919067, 1.8032513856887817, 2.1210927963256836, 1.6442530155181885, 1.8226392269134521, 1.5433300733566284, 1.757430911064148, 1.8072304725646973, 1.5316314697265625, 1.6589343547821045, 1.4707365036010742, 1.5193309783935547, 1.7460734844207764, 1.7763258218765259, 1.814003348350525, 1.5358293056488037, 1.5734416246414185, 1.6449898481369019, 1.786826252937317, 1.8485273122787476, 1.64457106590271, 1.5616194009780884, 1.4491695165634155, 1.4193198680877686, 1.6791026592254639, 1.58096182346344, 1.3705215454101562, 1.8328421115875244, 1.456339955329895, 1.6320805549621582, 1.630100965499878, 1.4489173889160156, 1.6866217851638794, 1.5589134693145752, 1.518283486366272, 1.263941764831543, 1.6093542575836182, 1.6572072505950928, 1.4630894660949707, 1.2320696115493774, 1.5795550346374512, 1.4387201070785522, 1.262986183166504, 1.4469691514968872, 1.1851937770843506, 1.546431303024292]]
global_train_loss_algo = [[2.313127050619296, 2.718198701548759, 2.3873546733270827, 2.4626120966108864, 2.1007465794873053, 2.185421536950504, 2.4055621977657307, 2.4672679367577635, 2.109319025598219, 1.9217471972755764, 2.141037308041702, 1.786066850585401, 1.9673936746614364, 2.0314957787618613, 2.084376709845365, 1.7893683648170413, 1.6995721892322726, 2.112284838086199, 1.8593177542357189, 2.0781999527645842, 2.1126727002965824, 2.089250533172237, 1.6666597996831245, 2.427712882571208, 1.6378433446750007, 1.5719170869158967, 1.6506810612080958, 1.6186595676500168, 2.0210934549646304, 1.6294488609599336, 1.4443097698414111, 1.6846655895338034, 1.4311492180885257, 2.081161778906117, 1.4696635843237953, 1.4366107608961023, 1.551239703164991, 1.9763728070746907, 1.5247630307741482, 1.6413531164683954, 1.7426458467607913, 1.7250330195097667, 2.0303831248332167, 1.5257286187023154, 1.904719739466372, 1.6443034367793052, 1.5075269330797902, 1.5828681426584874, 1.4741359590874303, 1.7369005465141647], [2.4084554736876425, 2.402089850981827, 2.300442163596678, 2.0279615957413792, 1.836868904130843, 1.9115093611085507, 1.8539090273935166, 1.8513423628209498, 1.9286987606216879, 1.8321602541162534, 1.766756489148835, 1.7736740908049562, 1.6151360575195468, 1.6101894558543135, 1.5632760248830557, 1.4785257596189103, 1.6545952160645019, 2.019210838448361, 1.5771292001390091, 1.7939104143615878, 1.8231555519201565, 1.8247647991265787, 1.487707942343124, 2.2465131654763772, 1.5435032707346066, 1.6041428750128393, 1.435643153727207, 1.5333436593375243, 1.74092260208886, 1.7079876446358078, 1.5218495926283815, 1.8345090065465863, 1.6594612705128273, 1.826515315896105, 1.4971025252281247, 1.5385879246170258, 1.5175873548783305, 1.5830313479504012, 1.5009528162229397, 1.4426662075854932, 1.619252022117605, 2.167339678310677, 1.6256351297163902, 1.5118630267774966, 1.6975096871176034, 1.7789865540116645, 1.4783535072260805, 1.224442282143761, 1.5658732361500831, 1.538248826628146]]
