import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1500 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'shakespeare_lstm' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'shakespeare'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2)],
    1: [1/np.power(10,1.5)],
    2: [1/np.power(10,1.5)],
    3: [1/np.power(10,1.5)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:0.5,
    2:0.5,
    3:0.5
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[4.286243175814462, 2.930396880841567, 2.4303657343345555, 2.391191677995835, 2.227160694814843, 2.169519340988213, 2.0761625354477005, 2.080366828012463, 1.9625075933792413, 1.97565348872707, 1.9461428980032605, 1.9989428074107027, 1.8783095086618133, 1.9012064520671803, 1.8754969713591827, 1.7863897679114182, 1.8596745763101112, 1.8747157780660928, 1.8404127561999304, 1.9032426805018108, 1.7832094311707496, 1.7351486086845398, 1.7747814385758507, 1.8005213450241704, 1.7235259851074538, 1.7508205702640374, 1.7416832992592313, 1.7174522667782646, 1.7153959138050467, 1.712381975446019], [4.197938950586254, 2.7692949640434557, 2.4988900938849254, 2.483187436123354, 2.3581825100952676, 2.33802052113071, 2.2821309820972626, 2.3246955319048466, 2.2449885299374963, 2.300539090945916, 2.2672023458575445, 2.3042173970395874, 2.2037868305078794, 2.2660909332353762, 2.2138506006894785, 2.1712490801374855, 2.211221920343744, 2.2418549910450176, 2.2434881654954424, 2.281448997812139, 2.211607243090946, 2.2176748728619686, 2.1674191827464986, 2.1953945064049263, 2.196197369270915, 2.199977907198581, 2.264224805682928, 2.1735841374018836, 2.171566692211006, 2.206059681925865], [4.199640227042115, 2.344163584785446, 2.014385108618309, 2.012271763522856, 1.893348093299401, 1.8821223592212868, 1.8165763644389077, 1.8563741093708976, 1.7749860808773152, 1.8756567146185166, 1.8035995465876564, 1.848916057915399, 1.7441314103920358, 1.8743296501945366, 1.7546463424989294, 1.7116446724819496, 1.7870537547565675, 1.7963852335482813, 1.7780081448554994, 1.8570758442726312, 1.7582280887536659, 1.7073474653429455, 1.7407019026853419, 1.7951720845367702, 1.7321702335994509, 1.730005256067141, 1.8026845475105613, 1.7532015055361245, 1.7485642188054875, 1.7430869481875575], [4.198819604465124, 2.356259880074255, 2.053947977873895, 2.064522126924314, 1.9108288809471237, 1.9473081319801355, 1.8811417779694586, 1.911256699851418, 1.8066593077649138, 1.781607687444638, 1.7959232988376463, 1.919343078317064, 1.7927288625587194, 1.8182524354157543, 1.8297333548169312, 1.7599029112585942, 1.8089112820175661, 1.8509364622658744, 1.8159507344750796, 1.8894880455215226, 1.7560955214364113, 1.7274373463392259, 1.753486014240318, 1.8269621927760973, 1.730088555329014, 1.783558457289272, 1.7209544115152144, 1.7694428003307372, 1.778952881533013, 1.775483103199487]]
acc_algo = [[tensor(0.0156), tensor(0.2969), tensor(0.3125), tensor(0.2812), tensor(0.3750), tensor(0.3438), tensor(0.4375), tensor(0.4375), tensor(0.4219), tensor(0.4531), tensor(0.4375), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4688), tensor(0.4531), tensor(0.4688), tensor(0.4688), tensor(0.4688), tensor(0.4688), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.5156), tensor(0.5000)], [tensor(0.1562), tensor(0.2344), tensor(0.2656), tensor(0.2969), tensor(0.3125), tensor(0.3281), tensor(0.3281), tensor(0.3125), tensor(0.3594), tensor(0.3594), tensor(0.3438), tensor(0.3594), tensor(0.3750), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594), tensor(0.3594)], [tensor(0.1875), tensor(0.3281), tensor(0.3438), tensor(0.4688), tensor(0.4531), tensor(0.4688), tensor(0.4688), tensor(0.4375), tensor(0.5000), tensor(0.4844), tensor(0.4688), tensor(0.4688), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4688), tensor(0.4844), tensor(0.4688), tensor(0.4688), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844), tensor(0.4844)], [tensor(0.1875), tensor(0.3281), tensor(0.4219), tensor(0.4375), tensor(0.4219), tensor(0.4219), tensor(0.4375), tensor(0.4219), tensor(0.4531), tensor(0.4375), tensor(0.4531), tensor(0.4688), tensor(0.4688), tensor(0.4688), tensor(0.4844), tensor(0.5156), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.5000), tensor(0.5156), tensor(0.5156), tensor(0.5000), tensor(0.5000)]]
test_loss_algo= [[4.822512626647949, 2.823087215423584, 2.511751651763916, 2.399254560470581, 2.173642158508301, 2.0814707279205322, 1.9803847074508667, 1.9498270750045776, 1.9287647008895874, 1.8733065128326416, 1.8529603481292725, 1.8259309530258179, 1.8183468580245972, 1.8171966075897217, 1.8045101165771484, 1.7959083318710327, 1.7685060501098633, 1.7541704177856445, 1.7538543939590454, 1.7420785427093506, 1.733562707901001, 1.7297958135604858, 1.723642349243164, 1.740310788154602, 1.7283605337142944, 1.7080481052398682, 1.712976336479187, 1.714592456817627, 1.7156214714050293, 1.7089612483978271], [4.579687595367432, 2.719216823577881, 2.6029560565948486, 2.4636521339416504, 2.3749332427978516, 2.331902027130127, 2.30180025100708, 2.2694692611694336, 2.225141763687134, 2.2365975379943848, 2.212618589401245, 2.1954731941223145, 2.188694953918457, 2.1821513175964355, 2.17191743850708, 2.167548418045044, 2.15337872505188, 2.152899980545044, 2.1497642993927, 2.153470754623413, 2.1393558979034424, 2.153163433074951, 2.153059959411621, 2.159406900405884, 2.160236120223999, 2.1610021591186523, 2.164914608001709, 2.1687958240509033, 2.1707873344421387, 2.18088698387146], [4.102962017059326, 2.183919668197632, 2.0007450580596924, 1.8530426025390625, 1.7678982019424438, 1.7289447784423828, 1.73031747341156, 1.7061549425125122, 1.712780237197876, 1.6904547214508057, 1.698217749595642, 1.681588053703308, 1.7009559869766235, 1.67496657371521, 1.6961318254470825, 1.702756404876709, 1.7014302015304565, 1.6780707836151123, 1.6887261867523193, 1.6995923519134521, 1.7086691856384277, 1.6958211660385132, 1.6901347637176514, 1.6927562952041626, 1.6981749534606934, 1.7019245624542236, 1.6896746158599854, 1.6975308656692505, 1.6851155757904053, 1.6909747123718262], [4.105473518371582, 2.195810317993164, 1.9865025281906128, 1.8874192237854004, 1.8361382484436035, 1.8003228902816772, 1.7874959707260132, 1.7616735696792603, 1.7587096691131592, 1.7446742057800293, 1.7449275255203247, 1.7345393896102905, 1.718125820159912, 1.7227492332458496, 1.7230041027069092, 1.7189630270004272, 1.7140370607376099, 1.7083791494369507, 1.7092280387878418, 1.7092843055725098, 1.7067606449127197, 1.7024669647216797, 1.7029709815979004, 1.7022068500518799, 1.7029465436935425, 1.7040655612945557, 1.7007180452346802, 1.7021780014038086, 1.702632188796997, 1.7015169858932495]]
global_train_loss_algo = [[], [], [], []]
