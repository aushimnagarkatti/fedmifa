import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 36 #55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.1],
    1: [0.1],
    2: [0.1], #0.05
    3: [0.1],
    4: [0.1]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1,
    4:1
    }

sch_freq = 100 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg",
            3: "scaffold",
            4: "UMIFA static"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[1.6488088396191596, 0.7489833681632444, 0.9756886017322539, 0.8659667214751245, 0.8937351003289222, 0.9290982973575591, 0.5085828544048127, 0.35286122815334237, 0.7638542172312737, 0.5252134403306992, 0.4609957717359065, 0.5599188185669483, 0.5597985466476529, 0.45540197721216824, 0.4439848321070895, 0.38330138390883806, 0.3760068290866911, 0.24754355621058494, 0.36689521756488835, 0.3316846372256987, 0.31665043894434347, 0.24077320197597146, 0.25366227512946354, 0.23246311580063775, 0.33675239618867636, 0.27744653779082, 0.23441091394983232, 0.24365137563530878, 0.2505666452733567, 0.22156622551287, 0.2484884288068861, 0.2199902249884326, 0.28930156864225864, 0.2607746298937127, 0.20076814120227934, 0.1494568104861537, 0.2886280638666358, 0.17788587757720054, 0.18053876758087423, 0.15538590502575972, 0.206447824943898, 0.15350869234163836, 0.19101652801502494, 0.12765476449858398, 0.17486765175010077, 0.1602794124186039, 0.15962453459505924, 0.15871212960453704, 0.17995134288550618, 0.13515131333726457], [1.6614369589090345, 0.6873895793088013, 0.8402907414734363, 0.6375747473724187, 0.6586551447398961, 0.7631247019767761, 0.38354829948613767, 0.25445010153547626, 0.5833489353582263, 0.3616297271265648, 0.32954521557316185, 0.39980442481581124, 0.39540765519719573, 0.3880713699758053, 0.34107388438656927, 0.21699124912731352, 0.3848127427254804, 0.2147112360520987, 0.26432577847735955, 0.3494224598375149, 0.3197980506997556, 0.16542739235563203, 0.13157172714127227, 0.12306805336709659, 0.29885314110899347, 0.2704624985106056, 0.15686299565633818, 0.17906232552079018, 0.2725574201310519, 0.1984456716021123, 0.16936183488927778, 0.18322443073266184, 0.22463651966769244, 0.2010478970815893, 0.1396410267605097, 0.12276664295233788, 0.2463150304016017, 0.08791277109849034, 0.2255889709945768, 0.3996955409484326, 0.11960348756274927, 0.1999806555482428, 0.11231551023898645, 0.11221901586221066, 0.15575975162414127, 0.11735562861111248, 0.13342608183855192, 0.1575055291755416, 0.11086830083237147, 0.1655096653678993], [1.667287935614586, 0.7037402648926945, 0.753368980512023, 0.5718594623915851, 0.6960939536988735, 0.7631372381001711, 0.3917835572262266, 0.23924486557400088, 0.560713291876018, 0.35315652384888385, 0.35199760207440706, 0.4327054857928306, 0.3390982085745781, 0.3182284625980537, 0.30863569683162495, 0.24069058775436133, 0.2459740951890126, 0.29927450950839557, 0.37826542290509685, 0.3962471766504586, 0.25501134713864304, 0.1610925484757172, 0.1526545134256594, 0.2111761045562161, 0.2573187521984801, 0.21459251381689684, 0.1490835174784661, 0.1811394917021971, 0.2556239805952646, 0.17272270478541032, 0.1976886225212365, 0.18519151444546877, 0.1960738824401051, 0.17748835945501926, 0.13577235100790858, 0.17228062064037658, 0.19002331458546906, 0.11748775476240551, 0.15749661300331352, 0.08589443328033668, 0.08026632274966687, 0.10666104062278466, 0.10677045408752746, 0.12641747856861912, 0.12886981834977634, 0.22685691523394783, 0.14416598671814426, 0.15212837918719743, 0.11943768384415307, 0.18335465784417465], [1.6810480719804766, 0.7174163252738072, 0.9327607037127018, 0.8242122501134872, 0.8298653584718704, 0.8728501310944559, 0.5303561180236284, 0.38612758535935426, 0.9289642409980298, 0.5681201326474546, 0.4943617010489107, 0.5768950255028904, 0.6624091152101755, 0.5114419701509177, 0.6317412623763085, 0.44465036658570173, 0.7323105900362135, 0.4321127990074455, 0.5673721312731506, 0.5149792038882152, 0.5093773158825934, 0.4063816495239735, 0.44669671077281237, 0.22667777487076818, 0.5639510764926672, 0.6734450433030725, 0.44659713380795435, 0.3737872212627554, 0.41508911934681236, 0.36310067201033236, 0.5424076768383383, 0.4257062016800046, 0.5931593068689108, 0.48481770280748604, 0.40570547175593674, 0.44658122729510075, 0.5585021826860611, 0.28126516106072813, 0.42742592740803964, 0.442213740143925, 0.33718978128163146, 0.3830076136998832, 0.39396505562122913, 0.3133960146084428, 0.3096336112800054, 0.2254537269542925, 0.34558869617991145, 0.44153211594559255, 0.4646003231301439, 0.3849534165859222], [1.6532642287015915, 0.6471399475692305, 0.8742503874376416, 0.5871662352234125, 0.6176664578914643, 0.7344943072646857, 0.3947471879328077, 0.26609827968131866, 0.6769899437576532, 0.417998097455129, 0.2793172605708242, 0.4756994765158743, 0.5571145942434669, 0.3975471097929403, 0.3839892734307796, 0.3887151143152733, 0.3781945094797993, 0.252950133663253, 0.34156225916623956, 0.28600118745467623, 0.33687844459898775, 0.2508558286633343, 0.15538392611546442, 0.20328792067630275, 0.32558624184923246, 0.24389472586335614, 0.23715945765739885, 0.15434351384348702, 0.23295429041725585, 0.19290775524917989, 0.2881841553631238, 0.19588372177910063, 0.29388878256082535, 0.18813450608402488, 0.15817639722336024, 0.211147985359421, 0.21868662219072577, 0.16637862253803176, 0.21477783739741424, 0.12638199991339205, 0.18620507744493525, 0.15561546042372357, 0.24830607820185832, 0.14361115815583617, 0.15752566167018814, 0.08937800185958622, 0.2095163553569, 0.22189958177510563, 0.15778095844842027, 0.14800279696355573]]
acc_algo = [[tensor(0.1000), tensor(0.0997), tensor(0.1043), tensor(0.1504), tensor(0.2209), tensor(0.2542), tensor(0.2681), tensor(0.2684), tensor(0.3502), tensor(0.3732), tensor(0.3818), tensor(0.4067), tensor(0.4138), tensor(0.4307), tensor(0.4481), tensor(0.4538), tensor(0.4487), tensor(0.4635), tensor(0.4570), tensor(0.4659), tensor(0.4621), tensor(0.4552), tensor(0.4696), tensor(0.4765), tensor(0.4749), tensor(0.4713), tensor(0.4719), tensor(0.4739), tensor(0.4793), tensor(0.4833), tensor(0.4728), tensor(0.4762), tensor(0.4724), tensor(0.4751), tensor(0.4788), tensor(0.4855), tensor(0.4829), tensor(0.4733), tensor(0.4784), tensor(0.4800), tensor(0.4774), tensor(0.4839), tensor(0.4822), tensor(0.4757), tensor(0.4851), tensor(0.4917), tensor(0.4791), tensor(0.4916), tensor(0.4868), tensor(0.4792)], [tensor(0.1000), tensor(0.1003), tensor(0.1614), tensor(0.2801), tensor(0.3325), tensor(0.3891), tensor(0.4058), tensor(0.4424), tensor(0.3759), tensor(0.4537), tensor(0.4574), tensor(0.4677), tensor(0.4441), tensor(0.4617), tensor(0.4687), tensor(0.4673), tensor(0.4686), tensor(0.4707), tensor(0.4705), tensor(0.4768), tensor(0.4793), tensor(0.4777), tensor(0.4881), tensor(0.4785), tensor(0.4870), tensor(0.4836), tensor(0.4786), tensor(0.4932), tensor(0.4886), tensor(0.4866), tensor(0.4877), tensor(0.5003), tensor(0.5005), tensor(0.4972), tensor(0.4948), tensor(0.4794), tensor(0.4908), tensor(0.4901), tensor(0.5017), tensor(0.5023), tensor(0.5065), tensor(0.5040), tensor(0.4968), tensor(0.4977), tensor(0.4972), tensor(0.5121), tensor(0.5028), tensor(0.5003), tensor(0.5081), tensor(0.5102)], [tensor(0.1000), tensor(0.1216), tensor(0.2060), tensor(0.2388), tensor(0.3381), tensor(0.3204), tensor(0.2368), tensor(0.3642), tensor(0.3831), tensor(0.4166), tensor(0.3887), tensor(0.4228), tensor(0.4318), tensor(0.4206), tensor(0.4372), tensor(0.4660), tensor(0.4256), tensor(0.4190), tensor(0.4182), tensor(0.4193), tensor(0.4534), tensor(0.4582), tensor(0.4568), tensor(0.4010), tensor(0.4678), tensor(0.4890), tensor(0.4657), tensor(0.4882), tensor(0.4818), tensor(0.4651), tensor(0.5005), tensor(0.4446), tensor(0.4944), tensor(0.4418), tensor(0.4689), tensor(0.5019), tensor(0.4670), tensor(0.4402), tensor(0.4531), tensor(0.4769), tensor(0.4547), tensor(0.4582), tensor(0.4556), tensor(0.5053), tensor(0.4887), tensor(0.4958), tensor(0.4731), tensor(0.4819), tensor(0.4965), tensor(0.4831)], [tensor(0.1000), tensor(0.1271), tensor(0.1896), tensor(0.2699), tensor(0.3309), tensor(0.3280), tensor(0.3326), tensor(0.3671), tensor(0.3862), tensor(0.4049), tensor(0.3677), tensor(0.4187), tensor(0.4332), tensor(0.4202), tensor(0.4090), tensor(0.4340), tensor(0.4212), tensor(0.3784), tensor(0.4227), tensor(0.4477), tensor(0.4303), tensor(0.4530), tensor(0.3847), tensor(0.3472), tensor(0.4504), tensor(0.4616), tensor(0.3495), tensor(0.4471), tensor(0.4270), tensor(0.4759), tensor(0.4444), tensor(0.4183), tensor(0.4697), tensor(0.3479), tensor(0.4339), tensor(0.4359), tensor(0.3945), tensor(0.3522), tensor(0.4191), tensor(0.4456), tensor(0.4381), tensor(0.4068), tensor(0.4252), tensor(0.4631), tensor(0.3938), tensor(0.4391), tensor(0.4574), tensor(0.4211), tensor(0.4639), tensor(0.4248)], [tensor(0.1000), tensor(0.1996), tensor(0.2440), tensor(0.3494), tensor(0.3931), tensor(0.3983), tensor(0.3760), tensor(0.4394), tensor(0.4204), tensor(0.4430), tensor(0.4576), tensor(0.4629), tensor(0.4531), tensor(0.4661), tensor(0.4795), tensor(0.4591), tensor(0.4714), tensor(0.4726), tensor(0.4746), tensor(0.4862), tensor(0.4726), tensor(0.4798), tensor(0.4808), tensor(0.4737), tensor(0.4870), tensor(0.4834), tensor(0.4657), tensor(0.4913), tensor(0.4835), tensor(0.4868), tensor(0.4924), tensor(0.4885), tensor(0.4903), tensor(0.4982), tensor(0.4783), tensor(0.4870), tensor(0.4948), tensor(0.4907), tensor(0.4922), tensor(0.5012), tensor(0.4949), tensor(0.4949), tensor(0.4945), tensor(0.4900), tensor(0.5005), tensor(0.4959), tensor(0.4983), tensor(0.4998), tensor(0.4967), tensor(0.4975)]]
test_loss_algo= [[2.3041593906985725, 2.398965357215541, 2.3674561886271093, 2.290297593280768, 2.048230963148129, 2.039897614223942, 1.9614994047553675, 1.904810282834776, 1.7615063425841604, 1.710632516320344, 1.7028163603156994, 1.656595000795498, 1.6301185295080682, 1.5906513223222867, 1.558539985091823, 1.563169953170096, 1.5798698815570515, 1.5634281559354941, 1.5538814925843742, 1.5814054338795365, 1.5646461992506768, 1.5850073615456843, 1.5990959288208348, 1.5761615903514206, 1.5890190605145351, 1.6353299572209643, 1.6220481570359249, 1.6067287086681197, 1.6178003746992464, 1.6144249522761933, 1.6648048580072488, 1.6584999014617532, 1.6721574705877122, 1.6591239377951166, 1.6660996694473704, 1.6726614517770755, 1.6966244954212455, 1.7270464175825666, 1.6875576547756317, 1.701499890369974, 1.706776505063294, 1.679592813655829, 1.6802495831896544, 1.715680324347915, 1.682993459853397, 1.696318099453191, 1.704991312543298, 1.6956656571406468, 1.7213839960705704, 1.7612757014620835], [2.3041593906985725, 2.34539476321761, 2.2740252344471634, 1.9553829469498556, 1.755121795235166, 1.645785833619962, 1.6389482856556108, 1.5428497320527483, 1.7506497118883073, 1.5447039346026767, 1.5197545631675964, 1.5164635690154544, 1.6792210234198601, 1.5447022003732669, 1.5617072893555757, 1.550661723325207, 1.5403188429061014, 1.523219113896607, 1.5921682468645133, 1.583617049417678, 1.606664949161991, 1.6221924321666645, 1.5692673868434444, 1.5978371535137201, 1.6135142737892783, 1.6369774364362097, 1.6457269639725898, 1.6154184212350542, 1.68796223743706, 1.627415563650192, 1.6767662342186946, 1.6787447955957644, 1.667799057854209, 1.663729646782966, 1.6611571494181445, 1.7436463870819967, 1.6926885478815454, 1.7817149420452725, 1.6780054022552102, 1.6549368793038046, 1.7381318420361562, 1.6448646985041868, 1.7291448359276838, 1.6731902383694983, 1.7256886511091973, 1.6327333708477627, 1.6470248418249143, 1.7011299687586012, 1.7231290401167172, 1.7046795749360588], [2.3041593906985725, 2.3664591373152035, 2.305847089761382, 2.122162914579841, 1.7484704833121816, 1.9543505087020292, 2.5380528945072443, 1.938704510403287, 1.8309148451325241, 1.6454316461162202, 1.9150181591131126, 1.7044718417392415, 1.6388869232432857, 1.7146879806640043, 1.700752615169355, 1.6258476182913324, 1.7625694973453594, 1.7915384997228148, 1.8181126960523568, 1.9522496180929196, 1.815411905574191, 1.8205368541608191, 1.7193920976796728, 2.21435537459744, 1.6873971823674099, 1.6850858512957385, 1.7468252531282462, 1.66287311095341, 1.7704086079718961, 1.7303262645271933, 1.6152567043425932, 2.0712181223425894, 1.6656309616793492, 1.9657978806526037, 1.8467650565372151, 1.6622648053108506, 1.7979509325543785, 2.0115776251835427, 1.9640547302877827, 1.9287082905981952, 1.9469056790042076, 2.007290378497664, 2.179059099999203, 1.7736311361288568, 1.8870823550376163, 1.7110716985289458, 1.8615681320239024, 1.797448027665448, 1.805570130894898, 1.8650911864201734], [2.3041593906985725, 2.3891813177971324, 2.2178329138239476, 1.9760662286904207, 1.7567840852555197, 1.8657041605870435, 1.8110096986126747, 1.8665619731708696, 1.7593147101675628, 1.7749008972933338, 2.033182606575595, 1.7460611800479282, 1.6552214789542423, 1.6537609730556513, 1.7967190097092063, 1.6141555438375776, 1.6370133548785166, 1.8666739296761288, 1.689406278027091, 1.5741118124336193, 1.8252407836306626, 1.7061781040422477, 1.8300678502222536, 2.4123056610678413, 1.6454011049999553, 1.5908226534059853, 2.175526317517469, 1.6152214442089106, 1.7271509079416847, 1.521651626772182, 1.7440407921554177, 1.8067886290276887, 1.565859706538498, 2.314752442062281, 1.7458895437277047, 1.7075823310074534, 1.7722939184516857, 2.3052025456337413, 1.9829748663932654, 1.8312791836489537, 1.782157151562393, 1.9418920703754303, 1.9376681936774285, 1.7277261185797916, 1.9705965989714216, 2.20175210837346, 1.7079169180742495, 1.8027009576748891, 1.7131557920176512, 1.8708481492510267], [2.3041593906985725, 2.146823286250898, 1.9789914758342086, 1.7406544055149054, 1.6368923574496226, 1.6426358427971033, 1.7270676049457234, 1.5542141844512551, 1.6114843316898224, 1.5476894720344787, 1.5327594029675624, 1.5398112489918994, 1.531859709958362, 1.5143834777698395, 1.507994331372012, 1.5578237179737942, 1.6050840912351183, 1.5670464805736664, 1.537293210910384, 1.5564251340878237, 1.5631305723433282, 1.5858194516722564, 1.6346517262185456, 1.5701072762726218, 1.578370883206653, 1.6109631828441742, 1.6510536617534175, 1.5942779635168185, 1.6516062917223402, 1.6165593429735512, 1.6469519609098981, 1.6126868937425554, 1.6133025322750116, 1.6183046983305815, 1.6981023633556, 1.5933526458254286, 1.6653822941385257, 1.6066022374827391, 1.6118960167951644, 1.7055137362449793, 1.6628708011785132, 1.601596305324773, 1.6722585342492267, 1.6492622506086994, 1.6535722905663168, 1.7160216600272307, 1.645824825687773, 1.6888875179230027, 1.6475213879992248, 1.6633412116652082]]
global_train_loss_algo = [[], [], [], [], []]
