import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 3000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = True


#Cluster
K= 5        #try for 5,10,20
cluster = 0

#Model
model_type = 'cnnmnist' #'r'
dataset = 'emnist'#cifar10'

#Plotting
plot_every_n = 50



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [0.2],#[1/np.power(10,2.5)],
    1: [0.08],#[1/np.power(10,2)],
    2: [0.08]#[1/np.power(10,2)]
    }

lrfactor = {
    0:0.8
    , #factor to reduce lr in scheduler
    1:0.8,
    2:0.8
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            0: "MIFA",
            1: "UMIFA",
            2: "FedAvg"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


Loss algo: [[3.6779395985603336, 3.7052617928187055, 3.2477623670895897, 2.8421196257273356, 2.502503660043081, 2.224040749470393, 2.071810266971588, 1.9648226346174877, 1.8298293912410735, 1.5167088627815246, 1.5433000450134275, 1.5058106764157613, 1.6508949271837872, 1.4744385625918706, 1.4016439379056296, 1.3636887521743775, 1.0918542750676472, 0.9888376464446385, 1.2555819376309714, 1.185965201854706, 1.200777409474055, 1.4586281146208446, 1.3303701767126719, 1.164472676873207, 1.338610447247823, 1.1245361264546712, 1.0692639861504236, 1.1429611976941427, 1.101023518840472, 1.171663936972618, 1.380364965995153, 0.9707916000684103, 1.2832500429948173, 1.0841459159056346, 0.8911080654462179, 1.1080866907040279, 1.1040146321058273, 1.1561767047842344, 0.7867203189929327, 1.0536934024095534, 1.0752875169912972, 0.9920645745595295, 1.1855902604262034, 0.7778580387433369, 0.9645342975854874, 1.1024034982919695, 0.9522655401627222, 1.2085361911654473, 0.7944155329465865, 0.936528332233429, 1.0616381945212683, 1.2684467596610387, 0.9382177249590555, 0.8233279938573638, 0.8277104132175446, 1.1782662134567896, 1.0599978970686597, 1.2971243249575297, 1.03075767993927, 1.0423495811223984], [3.740568347771963, 1.9256995126803715, 1.157995293219884, 1.0778094765345254, 0.9047997486193975, 0.8091004535555839, 0.9547661936283112, 0.939515768756469, 0.8523940916856129, 0.7759221104780833, 0.7341726625363031, 0.7003949252764383, 0.9287654381866256, 0.7932327265044053, 0.7409508954286574, 0.7580214794476827, 0.5997210456927617, 0.5013778075575829, 0.7444831589857737, 0.6673786893288294, 0.6541554105281829, 0.8236310537060103, 0.9660931135180096, 0.6494131177862486, 0.806791909635067, 0.5814284927646318, 0.6244033659497898, 0.6788220071792602, 0.6537306359012921, 0.7222491950839758, 0.8217782748738924, 0.5556137337684631, 0.8075623628695805, 0.7664163017272949, 0.5340284623702367, 0.6973794253667196, 0.6816927780210971, 0.7644316917856535, 0.4472980843981107, 0.6513724066813786, 0.7427202147642772, 0.5927822321653367, 0.788297473291556, 0.46884150077899295, 0.5757493024567764, 0.757044983456532, 0.5984849130113921, 0.820495054423809, 0.4771654800822338, 0.5986273902654649, 0.7028208891352017, 0.8123566905260085, 0.6498033781449, 0.5339067463816609, 0.5293289725383122, 0.7561111821532249, 0.8177780877749126, 0.8727168668111165, 0.6754016897082329, 0.7119413911898931], [3.7519528388977053, 1.9166887568632762, 1.1971653670867284, 1.0453619622389474, 0.9141323098341625, 0.7757075278957686, 0.9449100585778554, 0.7829913963551323, 0.8712622161706289, 0.7504114751021067, 0.7116165943741798, 0.5795501316594891, 0.9916681418319545, 0.8053877067565918, 0.7246077377398809, 0.7429134983023007, 0.5964168751239777, 0.5093823307255905, 0.631472988128662, 0.6525582063595453, 0.6590707322955132, 0.8557577206691105, 0.8470088855574527, 0.6327899618546168, 0.7765744556983312, 0.5610005316138268, 0.617932541469733, 0.628477824529012, 0.6239959201018015, 0.7143235881030561, 0.7333478434880576, 0.5244786392052967, 0.769818110227585, 0.6984776484966279, 0.5039470805724461, 0.6056755039095879, 0.6619816500941912, 0.7458958323001861, 0.4032746363679568, 0.6243625189860662, 0.6154791788781683, 0.5572105451424918, 0.7479474810510873, 0.43108105570077904, 0.5555672825872898, 0.6602070428927739, 0.6311112533013026, 0.7600490655303, 0.44812828610340755, 0.5746156025131544, 0.6389504855275154, 0.7389919637640318, 0.6080492004156113, 0.6321389554540318, 0.4519049962858359, 0.7244431812564531, 0.7166150006453197, 0.8245488401651382, 0.6347116866707802, 0.651167295018832]]
Acc algo: [[tensor(0.), tensor(0.0312), tensor(0.0312), tensor(0.0625), tensor(0.1406), tensor(0.1719), tensor(0.2344), tensor(0.2188), tensor(0.2969), tensor(0.2656), tensor(0.3438), tensor(0.2656), tensor(0.3594), tensor(0.3594), tensor(0.2969), tensor(0.3125), tensor(0.3125), tensor(0.3125), tensor(0.3906), tensor(0.3125), tensor(0.3750), tensor(0.3906), tensor(0.4062), tensor(0.4219), tensor(0.3750), tensor(0.4531), tensor(0.4531), tensor(0.4062), tensor(0.4531), tensor(0.4844), tensor(0.4531), tensor(0.5312), tensor(0.5312), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.4844), tensor(0.4062), tensor(0.4688), tensor(0.4531), tensor(0.5156), tensor(0.5312), tensor(0.4062), tensor(0.4844), tensor(0.5000), tensor(0.5000), tensor(0.5312), tensor(0.5000), tensor(0.4531), tensor(0.4531), tensor(0.4531), tensor(0.4219), tensor(0.4219), tensor(0.4531), tensor(0.4531), tensor(0.4375), tensor(0.5000), tensor(0.5312), tensor(0.4688), tensor(0.4375)], [tensor(0.0156), tensor(0.1875), tensor(0.4062), tensor(0.3750), tensor(0.4219), tensor(0.4375), tensor(0.3594), tensor(0.4531), tensor(0.4688), tensor(0.3594), tensor(0.4844), tensor(0.5156), tensor(0.4844), tensor(0.3906), tensor(0.5312), tensor(0.5469), tensor(0.5156), tensor(0.5781), tensor(0.4531), tensor(0.6094), tensor(0.5156), tensor(0.5781), tensor(0.5312), tensor(0.5938), tensor(0.6250), tensor(0.5000), tensor(0.5000), tensor(0.6094), tensor(0.5781), tensor(0.4688), tensor(0.5156), tensor(0.5469), tensor(0.4844), tensor(0.4688), tensor(0.5625), tensor(0.6094), tensor(0.5781), tensor(0.5938), tensor(0.5938), tensor(0.5156), tensor(0.6094), tensor(0.5000), tensor(0.5625), tensor(0.5781), tensor(0.5625), tensor(0.5781), tensor(0.5938), tensor(0.6094), tensor(0.5469), tensor(0.5938), tensor(0.5312), tensor(0.5781), tensor(0.6094), tensor(0.5938), tensor(0.5625), tensor(0.5469), tensor(0.5781), tensor(0.5781), tensor(0.5469), tensor(0.5781)], [tensor(0.0156), tensor(0.2031), tensor(0.3750), tensor(0.3281), tensor(0.4375), tensor(0.4375), tensor(0.4062), tensor(0.5156), tensor(0.4688), tensor(0.3906), tensor(0.5312), tensor(0.5156), tensor(0.4531), tensor(0.4688), tensor(0.5938), tensor(0.5625), tensor(0.4844), tensor(0.5781), tensor(0.5625), tensor(0.5469), tensor(0.6094), tensor(0.5469), tensor(0.5000), tensor(0.5469), tensor(0.5469), tensor(0.5156), tensor(0.5781), tensor(0.5469), tensor(0.6094), tensor(0.6094), tensor(0.5938), tensor(0.5625), tensor(0.6406), tensor(0.4844), tensor(0.6562), tensor(0.4531), tensor(0.5625), tensor(0.6562), tensor(0.5156), tensor(0.6406), tensor(0.6719), tensor(0.5781), tensor(0.6250), tensor(0.4531), tensor(0.5938), tensor(0.5938), tensor(0.4219), tensor(0.6875), tensor(0.5469), tensor(0.6562), tensor(0.6406), tensor(0.6406), tensor(0.6094), tensor(0.5938), tensor(0.5312), tensor(0.6562), tensor(0.6250), tensor(0.5781), tensor(0.6250), tensor(0.5000)]]
test_loss_algo: [[4.12573766708374, 4.055965900421143, 5.1085920333862305, 3.380969285964966, 2.79196834564209, 2.876394271850586, 2.4139797687530518, 2.616279363632202, 2.3874893188476562, 2.517693519592285, 2.335088014602661, 2.1971611976623535, 2.236647367477417, 1.9910837411880493, 2.1189379692077637, 1.9642839431762695, 1.8028327226638794, 1.8261914253234863, 1.7441747188568115, 1.7971583604812622, 1.7577446699142456, 1.7466628551483154, 1.7419307231903076, 1.5925278663635254, 1.6139434576034546, 1.5414347648620605, 1.6309771537780762, 1.553209900856018, 1.4200942516326904, 1.5229980945587158, 1.474493384361267, 1.3184139728546143, 1.406070590019226, 1.4995704889297485, 1.7626996040344238, 1.4972763061523438, 1.6223303079605103, 1.8400791883468628, 1.4947788715362549, 1.4233458042144775, 1.4923657178878784, 1.6163290739059448, 1.7918145656585693, 1.4542577266693115, 1.500296950340271, 1.6683703660964966, 1.510230541229248, 1.3569554090499878, 1.5636364221572876, 1.7117936611175537, 1.5360469818115234, 1.4504114389419556, 1.6828877925872803, 1.6154922246932983, 1.4895997047424316, 1.5919215679168701, 1.4078534841537476, 1.2576085329055786, 1.3573023080825806, 1.6346774101257324], [4.1686506271362305, 2.8752200603485107, 1.9307223558425903, 1.8122105598449707, 1.4689797163009644, 1.7400823831558228, 2.0137476921081543, 1.5332973003387451, 1.5074124336242676, 1.9708547592163086, 1.377521276473999, 1.5105408430099487, 1.5744435787200928, 1.6388319730758667, 1.1254613399505615, 1.2816729545593262, 1.6824824810028076, 1.383789300918579, 1.4622739553451538, 1.1221367120742798, 1.443007469177246, 1.201512336730957, 1.3893791437149048, 1.170886754989624, 1.0638413429260254, 1.5694644451141357, 1.2503230571746826, 1.1137341260910034, 1.124924659729004, 1.431469440460205, 1.2662042379379272, 1.3524445295333862, 1.2587405443191528, 1.5313656330108643, 1.0521897077560425, 1.0673292875289917, 1.1811316013336182, 1.0722286701202393, 1.1470425128936768, 1.263338565826416, 1.175675868988037, 1.339457392692566, 1.219921350479126, 1.2140703201293945, 1.1676304340362549, 1.2813618183135986, 1.0880733728408813, 1.1202669143676758, 1.1721023321151733, 1.1320788860321045, 1.1962668895721436, 1.0506813526153564, 1.0330908298492432, 1.1126692295074463, 1.1718472242355347, 1.3205206394195557, 1.0352790355682373, 1.122704267501831, 1.0642693042755127, 1.0996861457824707], [4.138529300689697, 2.571150779724121, 2.025369882583618, 1.8246495723724365, 1.3564571142196655, 1.666385531425476, 1.8755085468292236, 1.497291922569275, 1.7697219848632812, 2.014878988265991, 1.6145161390304565, 1.7723515033721924, 1.7256770133972168, 1.735525369644165, 1.222870945930481, 1.2691768407821655, 1.8145852088928223, 1.271426796913147, 1.2994025945663452, 1.3660011291503906, 1.1417255401611328, 1.2337112426757812, 1.495693325996399, 1.2877676486968994, 1.4373408555984497, 1.3259023427963257, 1.1629219055175781, 1.4612923860549927, 1.167628526687622, 1.1610971689224243, 1.147019863128662, 1.1891249418258667, 1.011642336845398, 1.538881540298462, 1.001882553100586, 1.5096116065979004, 1.4215790033340454, 1.082068920135498, 1.5065968036651611, 1.072184681892395, 1.0735617876052856, 1.185994267463684, 1.0922545194625854, 1.4866653680801392, 1.0591949224472046, 1.1863700151443481, 1.7079845666885376, 1.0137174129486084, 1.2242645025253296, 1.014402151107788, 1.0580286979675293, 1.0542442798614502, 1.112926959991455, 1.2415155172348022, 1.4774421453475952, 0.9835411310195923, 1.1913180351257324, 1.0793784856796265, 1.0328280925750732, 1.4632203578948975]]
global_train_loss_algo: [[], [], []]
