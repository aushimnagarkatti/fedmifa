import lenet
import numpy as np

#Generic Hyp
batch_size= 64 #Neural Network batch size
n_rnds= 1000 #Global rounds
tau=5 #Local rounds --> not in use since we use local epochs
total_c= 250 #Total no of clients
no_of_c=[5] #participating clients
local_epochs = 5 
plot_local_train_loss = False


#Cluster
K= 55        #try for 5,10,20. Number of clusters is predetermined for static
cluster = 1
clust_mech = 'static' #'static 

#Model
model_type = 'lenet' #shakespeare_lstm' #'shakespeare_lstm'#'cnnmnist'#'cnnmnist' #'r'
dataset = 'cifar10'

#Plotting
plot_every_n = 20



#Learning rate
algo_lr = {  #lr for each algo, length has to be the same for all algos
    0: [1/np.power(10,2)],
    1: [1/np.power(10,1.5)],
    2: [1/np.power(10,2)],
    3: [1/np.power(10,1.5)]
    }

lrfactor = {
    0:1, #factor to reduce lr in scheduler
    1:1,
    2:1,
    3:1
    }

sch_freq = 200 #scheduler every n rounds



#select algos to run
d_algo = {
            #0: "MIFA",
            1: "UMIFA",
            #2: "FedAvg",
            #3: "scaffold"
        }




#MIFA paper paradigm variables
#p_i min is for the paper's paradigm for client selection, each client is selected with a min prob of 0.2
pi_min = 0.2
sel_client_variablepi = False #variable no of clients selected each round according to pi assigned
enforce_cp_r1 = False #enforce all client part in 1st round


loss_algo = [[2.0752789711952206, 0.7238472003483912, 0.9431309607625007, 0.8345157957077027, 0.8599964457750321, 0.9120618653297425, 0.5167472682480001, 0.4390749305824283, 0.860334400832653, 0.847466899305582, 0.6504276821762323, 0.8138057176768779, 0.7564072942733764, 0.6981962180510163, 0.6940194140002132, 0.638797319829464, 0.8073299421370029, 0.4912016942612536, 0.6495150388608454, 0.6661091010679957, 0.5830921399779618, 0.5010585518507287, 0.5365843236818909, 0.40693048625718803, 0.6077263156045227, 0.6976501844450832, 0.48991215979214753, 0.5281745786871761, 0.5992816166207194, 0.4294075845048065, 0.5961373045295477, 0.4865252374485135, 0.7014797133207321, 0.5860563693381845, 0.4959890638949583, 0.5232046218588948, 0.548784383291495, 0.3503260070027318, 0.5497449113428592, 0.4720645135082305, 0.38547242521075537, 0.3495871965150582, 0.4370922912377864, 0.35987124327570197, 0.3899714270042023, 0.37834599281777626, 0.4242054551467299, 0.5563928272272459, 0.4858165960485349, 0.4826403060741723]]
acc_algo = [[0.0625   0.109375 0.140625 0.296875 0.265625 0.234375 0.359375 0.328125
  0.328125 0.390625 0.421875 0.390625 0.3125   0.40625  0.4375   0.375
  0.375    0.390625 0.453125 0.390625 0.421875 0.40625  0.375    0.453125
  0.4375   0.40625  0.375    0.4375   0.484375 0.484375 0.46875  0.5
  0.5      0.484375 0.4375   0.4375   0.53125  0.53125  0.546875 0.515625
  0.515625 0.453125 0.515625 0.5      0.5      0.5      0.53125  0.484375
  0.453125 0.46875 ]]
test_loss_algo= [[2.3041911125183105, 2.3792879581451416, 2.319857120513916, 2.0816330909729004, 2.0408763885498047, 1.9429433345794678, 1.8639976978302002, 1.8736512660980225, 1.9022293090820312, 1.8140826225280762, 1.8149009943008423, 1.7482813596725464, 1.7860674858093262, 1.7200990915298462, 1.6683309078216553, 1.7425261735916138, 1.7102822065353394, 1.6423704624176025, 1.6448537111282349, 1.6193766593933105, 1.5902959108352661, 1.555930495262146, 1.5763182640075684, 1.5480766296386719, 1.5629209280014038, 1.5517573356628418, 1.5443834066390991, 1.5070431232452393, 1.5152288675308228, 1.5111827850341797, 1.5701011419296265, 1.4715583324432373, 1.4437774419784546, 1.4918406009674072, 1.4594484567642212, 1.499413013458252, 1.4736744165420532, 1.3850306272506714, 1.4298198223114014, 1.4306819438934326, 1.4546000957489014, 1.4423120021820068, 1.4356852769851685, 1.4066014289855957, 1.4146838188171387, 1.4262441396713257, 1.3996565341949463, 1.3537993431091309, 1.3782312870025635, 1.3852604627609253]]
global_train_loss_algo = [[2.3068569432134214, 2.3046618264044643, 2.162924311045186, 2.0947389739858524, 2.1062307266323157, 2.1455236902017423, 1.991531876042066, 1.9691086286474067, 1.9881241265160348, 1.889057346774489, 1.8757370567077871, 1.7823126700223255, 1.702260232794925, 1.744750304600162, 1.7602592173134883, 1.6788409469682541, 1.6395474925370472, 1.624849726629379, 1.6165691665981128, 1.5947133535924165, 1.5641895589011405, 1.5684392462910899, 1.5560582431076129, 1.5558777643591546, 1.5547727053732519, 1.5117536463091135, 1.5478146724078967, 1.51667344875043, 1.5687216976109672, 1.5298654822742237, 1.5352890657646883, 1.5076113379824803, 1.4900897698634117, 1.476109565981209, 1.4766113075149028, 1.4644008687390087, 1.520210544319104, 1.5232518650686648, 1.5081545352326025, 1.4662096619301135, 1.4573013017244656, 1.4576979340494747, 1.4887842900307893, 1.4401199738387866, 1.358835378418798, 1.3487369942543146, 1.4141145847032748, 1.3944736766388348, 1.33922284399457, 1.363503291478852]]
